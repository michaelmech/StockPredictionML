{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": " EnsembleStockPredictor",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdwSt6oe6bi0",
        "outputId": "2e7bc87c-9894-4c1c-e694-637415da2374"
      },
      "source": [
        "!pip install blitz-bayesian-pytorch\n",
        "!pip install -e git+git://github.com/vonHacht/yfinance.git@master#egg=yfinance\n",
        "#!pip install optuna\n",
        "!pip install --upgrade ta\n",
        "#!pip install pandas_ta\n",
        "!pip install -U git+https://github.com/twopirllc/pandas-ta\n",
        "!pip install git+git://github.com/peerchemist/finta.git\n",
        "!pip install stumpy\n",
        "!pip install tslearn\n",
        "!pip install impyute\n",
        "!pip install -U scikit-learn\n",
        "!pip install fbprophet\n",
        " \n",
        " \n",
        "!pip install quandl\n",
        "!pip install pytrends\n",
        "!pip install fracdiff\n",
        "!pip install PyPortfolioOpt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting blitz-bayesian-pytorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/74/8dc8e93b70da6e247fc28afbd508ff98ffd1aa835cd863fbd727b9d25b12/blitz_bayesian_pytorch-0.2.7-py3-none-any.whl (47kB)\n",
            "\r\u001b[K     |██████▉                         | 10kB 19.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 20kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 30kB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 40kB 17.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from blitz-bayesian-pytorch) (7.1.2)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from blitz-bayesian-pytorch) (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from blitz-bayesian-pytorch) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from blitz-bayesian-pytorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from blitz-bayesian-pytorch) (0.22.2.post1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->blitz-bayesian-pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->blitz-bayesian-pytorch) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->blitz-bayesian-pytorch) (1.0.1)\n",
            "Installing collected packages: blitz-bayesian-pytorch\n",
            "Successfully installed blitz-bayesian-pytorch-0.2.7\n",
            "Obtaining yfinance from git+git://github.com/vonHacht/yfinance.git@master#egg=yfinance\n",
            "  Cloning git://github.com/vonHacht/yfinance.git (to revision master) to ./src/yfinance\n",
            "  Running command git clone -q git://github.com/vonHacht/yfinance.git /content/src/yfinance\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Installing collected packages: yfinance\n",
            "  Running setup.py develop for yfinance\n",
            "Successfully installed yfinance\n",
            "Collecting ta\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/22/a355ecf2d67da8150332d22ef65c3a1f79109528279bf5d40735b6f2bd72/ta-0.7.0.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from ta) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from ta) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ta) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->ta) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->ta) (1.15.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.7.0-cp37-none-any.whl size=28716 sha256=e27b4c13d04c7c0c6fbabc642c93c787a10622d2a797a6588afa08d2b6f18f7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/88/30/de9553fb54a474eb7480b937cdbb140bdda613d29cf4da7994\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.7.0\n",
            "Collecting git+https://github.com/twopirllc/pandas-ta\n",
            "  Cloning https://github.com/twopirllc/pandas-ta to /tmp/pip-req-build-dm_i4wts\n",
            "  Running command git clone -q https://github.com/twopirllc/pandas-ta /tmp/pip-req-build-dm_i4wts\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from pandas-ta==0.2.75b0) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->pandas-ta==0.2.75b0) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->pandas-ta==0.2.75b0) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pandas-ta==0.2.75b0) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->pandas-ta==0.2.75b0) (1.15.0)\n",
            "Building wheels for collected packages: pandas-ta\n",
            "  Building wheel for pandas-ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas-ta: filename=pandas_ta-0.2.75b0-cp37-none-any.whl size=197080 sha256=f124a72d0a9728331200b88de466c6afcf2bf84fafd2f0bb6f7b36fbc8dcc3c5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sd5jogjz/wheels/64/67/96/15e918c3b53b4a323b5bd037c7f08be5ef6908141c50f07c76\n",
            "Successfully built pandas-ta\n",
            "Installing collected packages: pandas-ta\n",
            "Successfully installed pandas-ta-0.2.75b0\n",
            "Collecting git+git://github.com/peerchemist/finta.git\n",
            "  Cloning git://github.com/peerchemist/finta.git to /tmp/pip-req-build-itap4u0e\n",
            "  Running command git clone -q git://github.com/peerchemist/finta.git /tmp/pip-req-build-itap4u0e\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from finta==1.3) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from finta==1.3) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->finta==1.3) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->finta==1.3) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->finta==1.3) (1.15.0)\n",
            "Building wheels for collected packages: finta\n",
            "  Building wheel for finta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for finta: filename=finta-1.3-cp37-none-any.whl size=29600 sha256=b3e723e1454b09947ace91174928c8af802a2b0a7cf2dbdbd66f5cdd478dccdc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5e4y5v_3/wheels/61/67/51/04957d540e6015563e7ca652e46a91c22ba8d4fb3158b6ce9d\n",
            "Successfully built finta\n",
            "Installing collected packages: finta\n",
            "Successfully installed finta-1.3\n",
            "Collecting stumpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/da/8d372a1af518930ecb3ad9acc627115450149b613ba1b9b51b4d3721218e/stumpy-1.8.0-py3-none-any.whl (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from stumpy) (1.19.5)\n",
            "Requirement already satisfied: numba>=0.48 in /usr/local/lib/python3.7/dist-packages (from stumpy) (0.51.2)\n",
            "Collecting scipy>=1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e8/43ffca541d2f208d516296950b25fe1084b35c2881f4d444c1346ca75815/scipy-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (27.4MB)\n",
            "\u001b[K     |████████████████████████████████| 27.4MB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.48->stumpy) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.48->stumpy) (56.1.0)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy, stumpy\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "Successfully installed scipy-1.6.3 stumpy-1.8.0\n",
            "Collecting tslearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/98/9a67e2869a8b1416eb6e6fd5e69c56f86869980403b18f30bdb5783ade9d/tslearn-0.5.0.5-cp37-cp37m-manylinux2010_x86_64.whl (790kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 13.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from tslearn) (0.51.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from tslearn) (1.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tslearn) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from tslearn) (1.6.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from tslearn) (0.22.2.post1)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from tslearn) (0.29.23)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->tslearn) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->tslearn) (56.1.0)\n",
            "Installing collected packages: tslearn\n",
            "Successfully installed tslearn-0.5.0.5\n",
            "Collecting impyute\n",
            "  Downloading https://files.pythonhosted.org/packages/37/28/86829f67c9affb847facaab94687761d3555539ec675f7577778c5b2680a/impyute-0.0.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from impyute) (1.6.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from impyute) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from impyute) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->impyute) (1.0.1)\n",
            "Installing collected packages: impyute\n",
            "Successfully installed impyute-0.0.8\n",
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 66.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.6.3)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.2 threadpoolctl-2.1.0\n",
            "Requirement already satisfied: fbprophet in /usr/local/lib/python3.7/dist-packages (0.7.1)\n",
            "Requirement already satisfied: Cython>=0.22 in /usr/local/lib/python3.7/dist-packages (from fbprophet) (0.29.23)\n",
            "Requirement already satisfied: cmdstanpy==0.9.5 in /usr/local/lib/python3.7/dist-packages (from fbprophet) (0.9.5)\n",
            "Requirement already satisfied: pystan>=2.14 in /usr/local/lib/python3.7/dist-packages (from fbprophet) (2.19.1.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from fbprophet) (1.19.5)\n",
            "Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from fbprophet) (1.1.5)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from fbprophet) (3.2.2)\n",
            "Requirement already satisfied: LunarCalendar>=0.0.9 in /usr/local/lib/python3.7/dist-packages (from fbprophet) (0.0.9)\n",
            "Requirement already satisfied: convertdate>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from fbprophet) (2.3.2)\n",
            "Requirement already satisfied: holidays>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from fbprophet) (0.10.5.2)\n",
            "Requirement already satisfied: setuptools-git>=1.2 in /usr/local/lib/python3.7/dist-packages (from fbprophet) (1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from fbprophet) (2.8.1)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.7/dist-packages (from fbprophet) (4.41.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.4->fbprophet) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->fbprophet) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->fbprophet) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->fbprophet) (0.10.0)\n",
            "Requirement already satisfied: ephem>=3.7.5.3 in /usr/local/lib/python3.7/dist-packages (from LunarCalendar>=0.0.9->fbprophet) (3.7.7.1)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.7/dist-packages (from convertdate>=2.1.2->fbprophet) (0.5.11)\n",
            "Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->fbprophet) (0.2.1)\n",
            "Requirement already satisfied: hijri-converter in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->fbprophet) (2.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->fbprophet) (1.15.0)\n",
            "Collecting quandl\n",
            "  Downloading https://files.pythonhosted.org/packages/8b/2b/feefb36015beaecc5c0f9f2533e815b409621d9fa7b50b2aac621796f828/Quandl-3.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from quandl) (1.19.5)\n",
            "Collecting inflection>=0.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/59/91/aa6bde563e0085a02a435aa99b49ef75b0a4b062635e606dab23ce18d720/inflection-0.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from quandl) (2.8.1)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from quandl) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from quandl) (1.15.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from quandl) (8.7.0)\n",
            "Requirement already satisfied: pandas>=0.14 in /usr/local/lib/python3.7/dist-packages (from quandl) (1.1.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.14->quandl) (2018.9)\n",
            "Installing collected packages: inflection, quandl\n",
            "Successfully installed inflection-0.5.1 quandl-3.6.1\n",
            "Collecting pytrends\n",
            "  Downloading https://files.pythonhosted.org/packages/96/53/a4a74c33bfdbe1740183e00769377352072e64182913562daf9f5e4f1938/pytrends-4.7.3-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytrends) (2.23.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pytrends) (4.2.6)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.7/dist-packages (from pytrends) (1.1.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (2.10)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25->pytrends) (1.15.0)\n",
            "Installing collected packages: pytrends\n",
            "Successfully installed pytrends-4.7.3\n",
            "Collecting fracdiff\n",
            "  Downloading https://files.pythonhosted.org/packages/51/a3/4358063358e75ccda935d6c45031100b20eb55784492af3eaba57184418e/fracdiff-0.4.2-py3-none-any.whl\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from fracdiff) (1.19.5)\n",
            "Collecting statsmodels<0.13.0,>=0.12.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/69/8eef30a6237c54f3c0b524140e2975f4b1eea3489b45eb3339574fc8acee/statsmodels-0.12.2-cp37-cp37m-manylinux1_x86_64.whl (9.5MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5MB 14.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy<2.0.0,>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from fracdiff) (1.6.3)\n",
            "Requirement already satisfied: scikit-learn<0.25.0,>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from fracdiff) (0.24.2)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels<0.13.0,>=0.12.0->fracdiff) (0.5.1)\n",
            "Requirement already satisfied: pandas>=0.21 in /usr/local/lib/python3.7/dist-packages (from statsmodels<0.13.0,>=0.12.0->fracdiff) (1.1.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.24.0->fracdiff) (2.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.24.0->fracdiff) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5->statsmodels<0.13.0,>=0.12.0->fracdiff) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21->statsmodels<0.13.0,>=0.12.0->fracdiff) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21->statsmodels<0.13.0,>=0.12.0->fracdiff) (2018.9)\n",
            "Installing collected packages: statsmodels, fracdiff\n",
            "  Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "Successfully installed fracdiff-0.4.2 statsmodels-0.12.2\n",
            "Collecting PyPortfolioOpt\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/55/7d39d78d554ee33a7317e345caf01339da11406c28f18bc48794fe967935/PyPortfolioOpt-1.4.1-py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from PyPortfolioOpt) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.7/dist-packages (from PyPortfolioOpt) (1.1.5)\n",
            "Collecting cvxpy<2.0.0,>=1.1.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/47/fd1e818b8da30ef18695a0fbf9b66611ab18506f0a44fc69480a75f4db1b/cvxpy-1.1.12.tar.gz (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 22.1MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy<2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from PyPortfolioOpt) (1.6.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->PyPortfolioOpt) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->PyPortfolioOpt) (2.8.1)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from cvxpy<2.0.0,>=1.1.10->PyPortfolioOpt) (0.6.2.post0)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.7/dist-packages (from cvxpy<2.0.0,>=1.1.10->PyPortfolioOpt) (2.0.7.post1)\n",
            "Requirement already satisfied: scs>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from cvxpy<2.0.0,>=1.1.10->PyPortfolioOpt) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.19->PyPortfolioOpt) (1.15.0)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.7/dist-packages (from osqp>=0.4.1->cvxpy<2.0.0,>=1.1.10->PyPortfolioOpt) (0.1.5.post0)\n",
            "Building wheels for collected packages: cvxpy\n",
            "  Building wheel for cvxpy (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cvxpy: filename=cvxpy-1.1.12-cp37-cp37m-linux_x86_64.whl size=2731623 sha256=1d0071760d1044448a97bc072483126c93c54a3cf4d5c80a96ab4862f69a19bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/62/55/1da181c05c710c5d99bd560edebec3bd6a61cb69acef9dc00e\n",
            "Successfully built cvxpy\n",
            "Installing collected packages: cvxpy, PyPortfolioOpt\n",
            "  Found existing installation: cvxpy 1.0.31\n",
            "    Uninstalling cvxpy-1.0.31:\n",
            "      Successfully uninstalled cvxpy-1.0.31\n",
            "Successfully installed PyPortfolioOpt-1.4.1 cvxpy-1.1.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxlZX0zk60I3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import quandl\n",
        "import blitz\n",
        "from blitz.modules import BayesianLSTM\n",
        "from blitz.modules import BayesianLinear\n",
        "from blitz.utils import variational_estimator\n",
        "import yfinance as yf\n",
        "import pandas_datareader as pdr\n",
        "import ta\n",
        "from pandas import DataFrame, Series\n",
        "from finta import TA\n",
        "from scipy.stats import boxcox\n",
        "import stumpy\n",
        "import tslearn\n",
        "from sklearn import metrics\n",
        "import math\n",
        "from google.colab import files\n",
        "import csv\n",
        "import inspect\n",
        "import impyute\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from fracdiff import FracdiffStat,fdiff\n",
        "from tslearn.utils import to_time_series_dataset\n",
        "from tslearn.svm import TimeSeriesSVC\n",
        "from tslearn.neural_network import TimeSeriesMLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.metrics import f1_score as f1\n",
        "from sklearn.linear_model import SGDClassifier as SGC\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from fbprophet import Prophet\n",
        "from scipy import signal\n",
        "import pytrends\n",
        "from pytrends import dailydata\n",
        "from datetime import date, timedelta\n",
        "from functools import partial\n",
        "from time import sleep\n",
        "from pytrends.exceptions import ResponseError\n",
        "from pytrends.request import TrendReq\n",
        "from sklearn.ensemble import AdaBoostClassifier,BaggingClassifier\n",
        "from datetime import datetime\n",
        "from sklearn.inspection import permutation_importance as pimp\n",
        "import pypfopt\n",
        "from sklearn.neighbors import BallTree\n",
        "from sklearn.neighbors import DistanceMetric\n",
        "from scipy.stats import mode\n",
        " \n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        " \n",
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5898gPPzvXxr"
      },
      "source": [
        "####Pick Device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM1qYoDLvW86"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOfzfNU7wuYL"
      },
      "source": [
        "def Mult_Ticker_GTrend():\n",
        "  string=input()\n",
        "  ticker_list=string.split(',')\n",
        "  collector=[]\n",
        "    \n",
        "  for ticker in ticker_list:\n",
        "    ticker_df=GTrends_Daily(ticker + ' stock')\n",
        "    collector.append(ticker_df)\n",
        "  \n",
        "  tickers_df=pd.concat(collector,axis=1)\n",
        "  \n",
        "\n",
        "  return tickers_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmfXw9--LjLJ"
      },
      "source": [
        "def DropDate(klepto_df):\n",
        "  klepto_df.index=pd.to_datetime(klepto_df['date'])\n",
        "  \n",
        "  klepto_df=klepto_df.drop('date',axis=1)\n",
        "  \n",
        "  return klepto_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYaL2B0JvwMy"
      },
      "source": [
        "def Add_Gtrends():\n",
        "  string=input()\n",
        "  term_list=string.split(',')\n",
        "  to_add_list=[GTrends_Daily(term) for term in term_list]\n",
        "  main_gtrends_df=UpdateUniGTrends()\n",
        "  to_add_list.append(main_gtrends_df)\n",
        "\n",
        "  main_gtrends_df=pd.concat(to_add_list,axis=1)\n",
        "\n",
        "  main_gtrends_df.dropna().to_csv('drive/My Drive/Nick Fury/External Datasets/gtrends.csv/gtrends.csv')\n",
        "  main_gtrends_df=UpdateUniGTrends()\n",
        "  \n",
        "  return main_gtrends_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5kn_DovCq7E"
      },
      "source": [
        "def UpdateTickerGTrends(ticker):\n",
        "  ticker_gtrends_df=pd.read_csv('drive/My Drive/Nick Fury/External Datasets/gtrends.csv/{}.csv'.format(ticker))\n",
        "\n",
        "  ticker_gtrends_df.index=ticker_gtrends_df.date\n",
        "  ticker_gtrends_df=ticker_gtrends_df.iloc[:,1:]\n",
        "  ticker_gtrends_df.index=pd.to_datetime(ticker_gtrends_df.index)\n",
        "\n",
        "  timeframe='now 7-d'\n",
        "\n",
        "  keyword=ticker+' stock'\n",
        "\n",
        "  pytrends_object = pytrends.request.TrendReq(hl='en-US', tz=360)\n",
        "\n",
        "  pytrends_object.build_payload([keyword],timeframe=timeframe)\n",
        "\n",
        "  single_gtrends_df=pytrends_object.interest_over_time()[keyword]\n",
        "\n",
        "  single_gtrends_df=single_gtrends_df.resample('1D').mean()\n",
        "\n",
        "  ticker_gtrends_df=pd.concat([ticker_gtrends_df,pd.DataFrame(single_gtrends_df)])\n",
        "\n",
        "  ticker_gtrends_df = ticker_gtrends_df[~ticker_gtrends_df.index.duplicated(keep='first')]\n",
        "\n",
        "  ticker_gtrends_df.to_csv('drive/My Drive/Nick Fury/External Datasets/gtrends.csv/{}.csv'.format(ticker))\n",
        "\n",
        "  return ticker_gtrends_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFmD8BUm5wg8"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VsS_BHjWZWB"
      },
      "source": [
        "####External Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5LAKgzoVGl5"
      },
      "source": [
        "def GTrends_Daily(search_term):\n",
        "  daily_data=pytrends.dailydata.get_daily_data(search_term,start_year=2010,start_mon=1,stop_year=datetime.now().date().year,stop_mon=datetime.now().date().month)\n",
        "\n",
        "  data=daily_data[search_term+'_unscaled']\n",
        "  data=data.rename(search_term)\n",
        "\n",
        "  return data\n",
        "\n",
        "def Add_Weekend(main_gtrends_df):\n",
        "  weekend_df=main_gtrends_df[main_gtrends_df.index.dayofweek>4]\n",
        "  weekend_df.index=pd.to_datetime(weekend_df.index)\n",
        "\n",
        "  collector=list()\n",
        "  for idx in range(len(weekend_df)-1):\n",
        "    if weekend_df.iloc[idx+1].name.date()-weekend_df.iloc[idx].name.date()==timedelta(days=1):\n",
        "      collector.append((weekend_df.iloc[idx+1]+weekend_df.iloc[idx]))\n",
        "\n",
        "  average_df=pd.concat(collector,axis=1).T\n",
        "  average_df.index=weekend_df.index[::2]\n",
        "  average_df.columns=average_df.columns+'_wknd'\n",
        "\n",
        "  average_df['week']=0\n",
        "  for idx in range(len(average_df)):\n",
        "    average_df['week'].iloc[idx]=int(str(average_df.iloc[idx].name.isocalendar()[0])+str(average_df.iloc[idx].name.isocalendar()[1]))\n",
        "  \n",
        "  for idx in range(len(average_df)-1):\n",
        "    average_df['week'].iloc[idx]=average_df['week'].iloc[idx+1]\n",
        "\n",
        "  if str(average_df['week'].iloc[-1])[-2:]!='52':\n",
        "    average_df['week'].iloc[-1]=average_df['week'].iloc[-1]+1\n",
        "\n",
        "  else:\n",
        "    average_df['week'].iloc[-1]= str(int(str(average_df['week'].iloc[-1])[:4])+1)+'1'\n",
        "\n",
        "  average_df=average_df.reset_index()\n",
        "  average_df.index=average_df.week\n",
        "  average_df=average_df.drop('week',axis=1)\n",
        "\n",
        "  main_gtrends_df['week']=0\n",
        "  for idx in range(len(main_gtrends_df)):\n",
        "    main_gtrends_df['week'].iloc[idx]=int(str(main_gtrends_df.iloc[idx].name.isocalendar()[0])+str(main_gtrends_df.iloc[idx].name.isocalendar()[1]))\n",
        "  \n",
        "  for idx in range(len(main_gtrends_df)-1):\n",
        "    if main_gtrends_df.iloc[idx].name.dayofweek>=4:\n",
        "      if str(main_gtrends_df['week'].iloc[idx])[-2:]!='52':\n",
        "        main_gtrends_df['week'].iloc[idx]=main_gtrends_df['week'].iloc[idx]+1\n",
        "\n",
        "      else:\n",
        "        main_gtrends_df['week'].iloc[idx]= int(str(int(str(main_gtrends_df['week'].iloc[idx])[:4])+1)+'1')\n",
        "    \n",
        "  main_gtrends_df=main_gtrends_df.reset_index()\n",
        "\n",
        "  main_gtrends_df.index=main_gtrends_df.week\n",
        "  main_gtrends_df=main_gtrends_df.drop('week',axis=1)\n",
        "\n",
        "  main_gtrends_df=pd.merge_asof(main_gtrends_df.sort_index(),average_df.sort_index(),right_index=True,left_index=True)\n",
        "\n",
        "  main_gtrends_df=main_gtrends_df.rename(columns={'date_x':'date'})\n",
        "  main_gtrends_df.index=main_gtrends_df['date']\n",
        "  main_gtrends_df=main_gtrends_df.drop(['date_y','date'],axis=1)\n",
        "  main_gtrends_df=main_gtrends_df.sort_index()\n",
        "\n",
        "  return main_gtrends_df\n",
        "\n",
        "def UpdateUniGTrends():\n",
        "  main_gtrends_df=pd.read_csv('drive/My Drive/Nick Fury/External Datasets/gtrends.csv/gtrends.csv')\n",
        "\n",
        "  df_date = datetime.strptime(main_gtrends_df['date'].iloc[-1], \"%Y-%m-%d\").date()\n",
        "  current_date = datetime.today().date()\n",
        "  \n",
        "  if True in main_gtrends_df.columns.str.contains('date'):\n",
        "    main_gtrends_df.index=main_gtrends_df.date\n",
        "    main_gtrends_df=main_gtrends_df.iloc[:,1:]\n",
        "    main_gtrends_df.index=pd.to_datetime(main_gtrends_df.index)\n",
        "\n",
        "  if current_date > df_date:\n",
        "    \n",
        "    if True in main_gtrends_df.columns.str.contains('date'):\n",
        "      main_gtrends_df.index=main_gtrends_df.date\n",
        "      main_gtrends_df=main_gtrends_df.iloc[:,1:]\n",
        "      main_gtrends_df.index=pd.to_datetime(main_gtrends_df.index)\n",
        "\n",
        "    collector=[]\n",
        "    keywords_list=main_gtrends_df.columns.tolist()\n",
        "    timeframe='now 7-d'\n",
        "\n",
        "    pytrends_object = pytrends.request.TrendReq(hl='en-US', tz=360)\n",
        "\n",
        "    for keyword in keywords_list:\n",
        "      \n",
        "      pytrends_object.build_payload([keyword],timeframe=timeframe)\n",
        "\n",
        "      single_gtrends_df=pytrends_object.interest_over_time()[keyword]\n",
        "\n",
        "      single_gtrends_df=single_gtrends_df.resample('1D').mean()\n",
        "\n",
        "      collector.append(single_gtrends_df)\n",
        "\n",
        "    update_gtrends_df=pd.concat(collector,axis=1)\n",
        "\n",
        "    main_gtrends_df=pd.concat([main_gtrends_df,update_gtrends_df])\n",
        "    main_gtrends_df = main_gtrends_df[~main_gtrends_df.index.duplicated(keep='first')]\n",
        "\n",
        "\n",
        "    main_gtrends_df.to_csv('drive/My Drive/Nick Fury/External Datasets/gtrends.csv/gtrends.csv')\n",
        "\n",
        "  return main_gtrends_df\n",
        "\n",
        "def get_returns(ticker):\n",
        "  data=yf.Ticker(ticker).history(period='max')\n",
        "  data=data.drop(['Dividends','Stock Splits'],axis=1)\n",
        "\n",
        "  data[ticker+'_returns']=data.Close.pct_change(periods=1)*100\n",
        "  \n",
        "  fracdev=FracdiffStat()\n",
        "  data[ticker+'_diff']=fracdev.fit_transform(data.Close.values.reshape(data.Close.shape+(1,)))\n",
        "\n",
        "  return data[ticker+'_diff']#data[[ticker+'_returns',ticker+'_diff']] \n",
        "\n",
        "def Prophesize(data_column):\n",
        "\n",
        "  proph_data=data_column.reset_index()\n",
        "  proph_data.columns=['ds','y']\n",
        "\n",
        "  prophet = Prophet(seasonality_mode='multiplicative')\n",
        "    \n",
        "  prophet.fit(proph_data)\n",
        "\n",
        "  tenet = pd.DataFrame(np.nan,columns=['y'],index=data_column.index)\n",
        "  tenet['ds']=data_column.index\n",
        "\n",
        "  tenet['y'].iloc[0]=proph_data['y'].iloc[0]\n",
        "\n",
        "  t=[1+t*.000545 for t in range(len(tenet))]\n",
        "  tenet['t']=t\n",
        "\n",
        "  scaler=MinMaxScaler()\n",
        "  scaled=scaler.fit_transform(data_column.values.reshape(data_column.values.shape + (1,)))\n",
        "  tenet['y_scaled']=np.nan\n",
        "  tenet['y_scaled'].iloc[0]=scaled[0]\n",
        "\n",
        "  prophecy=prophet.predict(tenet)\n",
        "\n",
        "  return prophecy\n",
        "\n",
        "def t2v_prof(data_wo_prof):\n",
        "\n",
        "  ###time2vec\n",
        "\n",
        "  t2v_wo_prices=pd.concat([pd.DataFrame(t2v_func(data_wo_prof[col]),columns=[col+'_linear',col+'_periodic']) for col in data_wo_prof.drop(['high_returns','low_returns', 'open_returns', 'close_returns'],axis=1)],axis=1)\n",
        "  t2v_wo_prices.index=data_wo_prof.index\n",
        "\n",
        "  t2v_w_prices=pd.DataFrame(t2v_func(data_wo_prof.loc[:,['high_returns','low_returns', 'open_returns', 'close_returns']]),index=data_wo_prof.index,columns=['prices_lin','prices_per'])\n",
        "\n",
        " ###matrix profile\n",
        "  NoVol_df=data_wo_prof[[col for col in data_wo_prof if (col[-6:]!='volume')]]\n",
        "\n",
        "  m=3\n",
        "\n",
        "  NoVolProf,_= stumpy.mstump(NoVol_df.T,m=m,discords=True)\n",
        "  \n",
        "  NoVolProf_df=pd.DataFrame(NoVolProf,index=NoVol_df.index,columns=NoVol_df.columns[m-1:]+'_prof')\n",
        "\n",
        "  ###semantic segmentation\n",
        "  returns_prof=stumpy.stump(data_wo_prof['close_returns'],m=m)\n",
        "\n",
        "  price_prof=stumpy.stump(data_wo_prof['close'],m=m)\n",
        "\n",
        "  diff_prof=stumpy.stump(data_wo_prof['close_diff'],m=m)\n",
        "\n",
        "  returns_cac, regime_locations = stumpy.fluss(returns_prof[:, 1], L=3, n_regimes=2, excl_factor=20)\n",
        "\n",
        "  price_cac, regime_locations = stumpy.fluss(price_prof[:, 1], L=3, n_regimes=2, excl_factor=20)\n",
        "\n",
        "  diff_cac, regime_locations = stumpy.fluss(diff_prof[:, 1], L=3, n_regimes=2, excl_factor=20)\n",
        "\n",
        "  cac=np.vstack((returns_cac,price_cac,diff_cac)).T\n",
        "\n",
        "  cac_df=pd.DataFrame(cac,index=data_wo_prof['close_returns'].index[m-1:],columns=['returns_cac','price_cac','diff_cac'])\n",
        "\n",
        "  ###combining everything together\n",
        "\n",
        "  data=pd.concat([data_wo_prof,NoVolProf_df,t2v_wo_prices,t2v_w_prices,cac_df],axis=1)\n",
        "    \n",
        "  data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "  data=data.dropna()\n",
        "  \n",
        "  ###outliers\n",
        "  outliers=IsolationForest().fit_predict(np.reshape(data['close_returns'].values,(data['close_returns'].shape[0],-1)))\n",
        "  data['outliers']=outliers\n",
        "  \n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUzgHMnTWvUH"
      },
      "source": [
        "####Main Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PN3HCOmgdun"
      },
      "source": [
        "def Get_Data(main_ticker):\n",
        "  \n",
        "  ### sector and asset data + vix\n",
        "  index_list=['vox','vcr','vdc','vde','vfh','vht','vis','vgt','vaw','vnq','vpu','dbp','vglt','vgit','vgsh','eem','uso','pdbc','gld','vwo','lqd']\n",
        "  returns_list=[get_returns(index) for index in index_list]\n",
        "\n",
        "  returns_df=pd.concat(returns_list,axis=1)       #different sectors and asset allocations \n",
        "\n",
        "  vix=yf.Ticker('^vix').history(period='max')['Close']\n",
        "  vix_df=pd.DataFrame(boxcox(vix)[0],columns=['vix'],index=vix.index) #vix\n",
        "\n",
        "  fracdev=FracdiffStat()\n",
        "\n",
        "  vix_diff=fracdev.fit_transform(vix.values.reshape(vix.shape+(1,)))\n",
        "  vix_diff_df=pd.DataFrame(vix_diff,index=vix.index,columns=['vix_diff'])\n",
        "\n",
        "  returns_df=pd.merge_asof(returns_df,vix_df,left_index=True,right_index=True)\n",
        "  returns_df=pd.merge_asof(returns_df,vix_diff_df,left_index=True,right_index=True)\n",
        "\n",
        "  ###individual stock data\n",
        "  prices=yf.Ticker(main_ticker).history(period='max').dropna()\n",
        "  prices.columns=[col.lower() for col in prices.columns]\n",
        "  data=prices.pct_change(periods=1)*100\n",
        "  data=data.drop(['dividends','stock splits'],axis=1)\n",
        "  \n",
        "  data.columns=[column.lower()+'_returns' for column in data.columns]\n",
        "  data['volume']=boxcox(yf.Ticker(main_ticker).history(period='max')['Volume'].dropna()+0.0000000001)[0]\n",
        "  data=data.drop(['volume_returns'],axis=1)\n",
        "  data['mfi']=ta.volume.money_flow_index(high=prices['high'],low=prices['low'],close=prices['close'],volume=prices['volume'])\n",
        "  data['cci']=ta.trend.cci(high=prices['high'],low=prices['low'],close=prices['close'],window=14)\n",
        "  data['high_bb2']=ta.volatility.bollinger_hband(close=prices['close'],window=14,window_dev=2)\n",
        "  data['low_bb2']=ta.volatility.bollinger_lband(close=prices['close'],window=14,window_dev=2)\n",
        "  data['ema200']=ta.trend.ema_indicator(close=prices['close'],window=200)\n",
        "  data['ema50']=ta.trend.ema_indicator(close=prices['close'],window=50)\n",
        "  data['ema20']=ta.trend.ema_indicator(close=prices['close'],window=20)\n",
        "  data['ema50']=ta.trend.ema_indicator(close=prices['close'],window=10)\n",
        "  data['rsi']=ta.momentum.rsi(close=prices['close'],window=14)\n",
        "  data['close_diff']=fracdev.fit_transform(prices.close.dropna().values.reshape(prices.close.dropna().values.shape+(1,)))\n",
        "  \n",
        "  individual_data=data\n",
        "\n",
        "  ###quandl macroeconomic data\n",
        "  quandl.ApiConfig.api_key = '**************'\n",
        "\n",
        "  yale_list=['YALE/US_CONF_INDEX_VAL_INDIV','YALE/US_CONF_INDEX_VAL_INST','YALE/US_CONF_INDEX_CRASH_INDIV','YALE/US_CONF_INDEX_CRASH_INST']\n",
        "\n",
        "  yale_df_list=[quandl.get(code, start_date='2010-1-1', end_date=str(datetime.today().date()))['Index Value'] for code in yale_list] \n",
        "\n",
        "  yale_df=pd.DataFrame(yale_df_list,index=yale_list).T  #yale confidence data\n",
        "\n",
        "  employ_df=quandl.get('FRED/NROUST', start_date='2010-1-1', end_date=str(datetime.today().date())) #short term unemployment]\n",
        "  product_df=quandl.get('FRED/GDPPOT', start_date='2010-1-1', end_date=str(datetime.today().date())) #nominal gross domestic product\n",
        "\n",
        "  fed_df=pd.concat([product_df,employ_df],axis=1)\n",
        "  fed_df.columns=['FRED/NROUST','FRED/GDPPOT'] #federal reserve data\n",
        "\n",
        "  #zillow real estate_data\n",
        "  region_ids=['9840', '9980', '9859', '9602', '99886', '99109']\n",
        "\n",
        "  estate_list=[]\n",
        "  for id in region_ids:\n",
        "    single_region=quandl.get_table('ZILLOW/DATA', indicator_id='ZALL', region_id=id)\n",
        "    single_region.index=single_region.date\n",
        "    single_region=single_region.drop(['region_id','indicator_id','date'],axis=1)\n",
        "    estate_list.append(single_region)\n",
        "\n",
        "  estate_df=pd.concat(estate_list,axis=1)\n",
        "  estate_df.columns=region_ids\n",
        "\n",
        "  #aluminum data\n",
        "  alu_df= quandl.get('WORLDAL/PALPROD', start_date='2010-1-1', end_date=str(datetime.today().date()))[['Total','Daily Average']]\n",
        "  alu_df.columns=alu_df.columns+'_alu'\n",
        "\n",
        "  #social security data\n",
        "  ssn_df=quandl.get('SOCSEC/ALL', start_date='2010-1-1', end_date=str(datetime.today().date()))\n",
        "  ssn_df.columns=ssn_df.columns+'_ssn'\n",
        "\n",
        "  #inflation\n",
        "  infl_df=quandl.get('RATEINF/CPI_USA', start_date='2010-1-1', end_date=str(datetime.today().date()))\n",
        "  infl_df.columns=['basket_price']\n",
        "  infl_df['inflation_Q']=infl_df['basket_price'].pct_change()\n",
        "\n",
        "  #sunspots\n",
        "  suns_df=quandl.get('SIDC/SUNSPOTS_M', start_date='2010-1-1', end_date=str(datetime.today().date()))['Number of Observations']\n",
        "  suns_df.columns=['n_sunspots']\n",
        "\n",
        "  #metals \n",
        "  metals_list=['JOHNMATT/RUTH','JOHNMATT/IRID','JOHNMATT/RHOD','JOHNMATT/PALL','JOHNMATT/PLAT']\n",
        "\n",
        "  single_metals=[quandl.get(metal, start_date='2010-1-1', end_date=str(datetime.today().date())).iloc[:,0] for metal in metals_list]\n",
        "\n",
        "  metals_df=pd.concat(single_metals,axis=1)\n",
        "  metals_df.columns=[metal[-4:] for metal in metals_list]\n",
        "  metals_df=metals_df.fillna(method='ffill')\n",
        "\n",
        "  for column in metals_df.columns:\n",
        "    metals_df[column+'_diff']=fracdev.fit_transform(metals_df[column].values.reshape(metals_df[column].shape+(1,)))\n",
        "\n",
        "  #shortint\n",
        "  short_df=quandl.get('FINRA/FNSQ_'+main_ticker.upper(), start_date='2010-1-1', end_date=str(datetime.today().date()))['ShortVolume']\n",
        "  short_df.columns=['short_int']\n",
        "\n",
        "  ##weekly sentiment\n",
        " \n",
        "  sentiment_df=quandl.get('AAII/AAII_SENTIMENT-AAII-Investor-Sentiment-Data')\n",
        "\n",
        "  sentiment_df=sentiment_df.dropna()\n",
        "  sentiment_df=sentiment_df[['Bullish','Neutral','Bearish']].iloc[1:,:]\n",
        "\n",
        "  ###pytrends\n",
        "  ticker_gtrends_df=UpdateTickerGTrends(main_ticker)\n",
        "  ticker_gtrends_df=Add_Weekend(ticker_gtrends_df)\n",
        "  main_gtrends_df=UpdateUniGTrends()\n",
        "  main_gtrends_df=Add_Weekend(main_gtrends_df)\n",
        "  \n",
        "  #impute nans\n",
        "  df_list=[short_df,metals_df,suns_df,infl_df,ssn_df,alu_df,estate_df,sentiment_df,fed_df,yale_df]\n",
        "  data_wo_prof=pd.concat([returns_df,individual_data,prices.close,ticker_gtrends_df],axis=1)\n",
        "\n",
        "  #imputed_df=impyute.imputation.cs.em(data_wo_prof.values,loops=500)\n",
        "  #imputed_df=pd.DataFrame(imputed_df)\n",
        "  #imputed_df.columns=data_wo_prof.columns\n",
        "  #imputed_df.index=data_wo_prof.index\n",
        "  for df in df_list:\n",
        "    data_wo_prof=pd.merge_asof(data_wo_prof,df,right_index=True,left_index=True).dropna()\n",
        "  \n",
        "  data_wo_prof=data_wo_prof.loc[:,(data_wo_prof != data_wo_prof.iloc[0]).any()] \n",
        "  data_wo_prof = data_wo_prof.loc[:,data_wo_prof.apply(pd.Series.nunique) != 1]\n",
        "\n",
        "  data=t2v_prof(data_wo_prof).dropna()\n",
        "\n",
        "  data=pd.concat([data,main_gtrends_df],axis=1)\n",
        "  \n",
        "  \n",
        "  data['label']=0\n",
        "  data['label'].iloc[np.where(data['close_returns']>0)]=1\n",
        "  data['label'].iloc[np.where(data['close_returns']<-0)]=-1\n",
        "  #data['label'].iloc[np.where((data['close_returns']<0) & (data['close_returns']>-0.3))]=0\n",
        "  data['label'].iloc[np.where(data['close_returns']==0)]=0\n",
        "\n",
        "  data=data.drop(list(data.index[np.where(data['label']==0)[0]])) \n",
        "\n",
        "  scaler=MinMaxScaler((-1,1))\n",
        "\n",
        "  scaled_data=scaler.fit_transform(data)\n",
        "  scaled_df=pd.DataFrame(scaled_data,index=data.index,columns=data.columns)\n",
        "  scaled_df['label']=data['label']\n",
        "\n",
        "  return scaled_df,data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGwq3UA3WX-w"
      },
      "source": [
        "####Time2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66wFP47wWdjS"
      },
      "source": [
        "class Time2Vec(nn.Module):\n",
        "  def __init__(self, activation,hiddem_dim,in_features):\n",
        "      super().__init__()\n",
        "      self.in_features=in_features\n",
        "      if activation == \"sin\":\n",
        "          self.l1 = SineActivation(self.in_features, hiddem_dim)\n",
        "      elif activation == \"cos\":\n",
        "          self.l1 = CosineActivation(self.in_features, hiddem_dim)\n",
        "      \n",
        "      self.fc1 = nn.Linear(hiddem_dim, 2)\n",
        "  \n",
        "  def forward(self, x):\n",
        "      #x = x.unsqueeze(1)\n",
        "      x = self.l1(x)\n",
        "      x = self.fc1(x)\n",
        "      return x\n",
        "\n",
        "def t2v(tau, f, out_features, w, b, w0, b0, arg=None):\n",
        "    if arg:\n",
        "        v1 = f(torch.matmul(tau, w) + b, arg)\n",
        "    else:\n",
        "        \n",
        "        v1 = f(torch.matmul(tau, w) + b)\n",
        "    v2 = torch.matmul(tau, w0) + b0\n",
        "   \n",
        "    return torch.cat([v1, v2], 1)\n",
        "\n",
        "class SineActivation(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(SineActivation, self).__init__()\n",
        "        self.out_features = out_features\n",
        "        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
        "        self.b0 = nn.parameter.Parameter(torch.randn(1, 1))\n",
        "        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
        "        self.b = nn.parameter.Parameter(torch.randn(1, out_features-1))\n",
        "        self.f = torch.sin\n",
        "\n",
        "    def forward(self, tau):\n",
        "        return t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)\n",
        "\n",
        "def t2v_func(data):\n",
        "  if len(data.shape) >1:\n",
        "    time2vec=Time2Vec(activation='sin',hiddem_dim=100,in_features=data.shape[-1])\n",
        "    t2v_data=time2vec(torch.Tensor(data.values))\n",
        "\n",
        "  else:\n",
        "    data=np.reshape(data.values,(data.shape[0],1))\n",
        "    time2vec=Time2Vec(activation='sin',hiddem_dim=100,in_features=data.shape[-1])\n",
        "    t2v_data=time2vec(torch.Tensor(data))\n",
        "  \n",
        "  if min(t2v_data[:,-1])<=0:\n",
        "    t2v_data[:,-1]=torch.Tensor(boxcox(((t2v_data[:,-1]+abs(min(t2v_data[:,-1]))+0.00000001)+0.000001).detach())[0])\n",
        "  \n",
        "  if min(t2v_data[:,0])<=0:\n",
        "    t2v_data[:,0]=torch.Tensor(boxcox(((t2v_data[:,0]+abs(min(t2v_data[:,0]))+0.00000001)+0.000001).detach())[0])\n",
        "  \n",
        "  return t2v_data.detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffapg1lvVWaV"
      },
      "source": [
        "###Manual Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjJBmOVTWAXb"
      },
      "source": [
        "####Bayesian-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njf0AMGtTHJP"
      },
      "source": [
        "#defining the neural net\n",
        "@variational_estimator\n",
        "class net(nn.Module):\n",
        "    def __init__(self,hidden_size,dropout,train_inputs,single_label=False):#batch_first=True\n",
        "        super(net,self).__init__()\n",
        "        self.hidden_size=hidden_size\n",
        "\n",
        "        self.input_size=train_inputs.shape[-1] #number of features\n",
        "        \n",
        "        self.output_size= self.input_size #should = input_size, but change to 1 if we're interested in only 1 feature\n",
        "\n",
        "        if single_label==True:\n",
        "          self.output_size=1\n",
        "\n",
        "        \"\"\"self.lstm = torch.nn.LSTM(\n",
        "            input_size=self.input_size,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bias=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True,\n",
        "        )\"\"\"\n",
        "\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "        self.lstm=BayesianLSTM(self.input_size,self.hidden_size)\n",
        "        self.linear=BayesianLinear(self.hidden_size,self.output_size)\n",
        "        #self.linear=nn.Linear(self.hidden_size,self.output_size)\n",
        "  \n",
        "    def forward(self, inputs):\n",
        "        \n",
        "        output, (hidden,cell) =self.lstm(inputs)\n",
        "\n",
        "        output=self.dropout(output)\n",
        "      \n",
        "        output=self.linear(output)\n",
        "        \n",
        "        output=torch.tanh(output)[:,-1,-1]\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgAWsO-IWM82"
      },
      "source": [
        "####Global KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DTdszElWMR5"
      },
      "source": [
        "class GlobalWeightedKNN:\n",
        "  \"\"\"\n",
        "  A k-NN classifier with feature weights\n",
        "\n",
        "  Returns: predictions of k-NN.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "      self.X_train = None\n",
        "      self.y_train = None\n",
        "      self.k = None\n",
        "      self.weights = None\n",
        "      self.tree = None\n",
        "      self.predictions = list()\n",
        "\n",
        "  def fit(self, X_train, y_train, k, weights):        \n",
        "      self.X_train = X_train\n",
        "      self.y_train = y_train\n",
        "      self.k = k\n",
        "      self.weights = weights\n",
        "      self.tree = BallTree(X_train,metric=DistanceMetric.get_metric('wminkowski', p=2, w=weights))\n",
        "\n",
        "\n",
        "  def predict(self, testing_data):\n",
        "      \"\"\"\n",
        "      Takes a 2d array of query cases.\n",
        "\n",
        "      Returns a list of predictions for k-NN classifier\n",
        "      \"\"\"\n",
        "      indexes = self.tree.query(testing_data, self.k, return_distance=False)\n",
        "      y_answers = self.y_train[indexes]\n",
        "      self.predictions = np.apply_along_axis(lambda x: mode(x)[0], 1, y_answers)\n",
        "      return self.predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt46bPaeW7-I"
      },
      "source": [
        "###Splitting Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0xDtW2HTDXD"
      },
      "source": [
        "def Tensify(data,train_ratio,sample_len,close_ind,kpca=False,pca=False):\n",
        "    tensor_list=[]\n",
        "    \n",
        "    train_inputs, train_targets, test_inputs, test_targets = Extract_XY(data,train_ratio,sample_len,close_ind,pca,kpca)\n",
        "    \n",
        "    data_list=[train_inputs,train_targets,test_inputs,test_targets]\n",
        "    \n",
        "    for data in data_list:\n",
        "        tensor_list.append(torch.Tensor(data)) \n",
        "        \n",
        "    train_inputs=tensor_list[0]\n",
        "    train_targets=tensor_list[1]\n",
        "    test_inputs=tensor_list[2]\n",
        "    test_targets=tensor_list[3]\n",
        "    \n",
        "    return train_inputs.to(device),train_targets.to(device),test_inputs.to(device),test_targets.to(device)\n",
        "\n",
        "def Extract_XY(data,train_ratio,sample_len,close_ind,pca=False,kpca=False):\n",
        "  chunk_list=[]\n",
        "\n",
        "  num_targets=len(data)-sample_len\n",
        "\n",
        "  for timestep in range(num_targets):\n",
        "      end_index=timestep+sample_len+1 #targets are only '1' index ahead of inputs\n",
        "      chunk_list.append(data.iloc[timestep:end_index,:].values)\n",
        "\n",
        "  chunks=np.array(chunk_list)\n",
        "\n",
        "  to_pca=data.iloc[:round(len(data)*train_ratio)]\n",
        "\n",
        "  train_data=chunks[:round(train_ratio*data.shape[0])]\n",
        "  test_data=chunks[round(train_ratio*data.shape[0]):]\n",
        "\n",
        "  train_inputs=train_data[:,:-1,:]\n",
        "  #train_targets=train_data[:,1:,:]\n",
        "  \n",
        "  train_targets=train_data[:,-1,close_ind] #if we only want to get a single label on the last day\n",
        "  #train_targets=train_data[:,1:,-1]  #if we want to predict a single feature over next sample_len\n",
        "  #train_targets=train_data[:,-1,:] #if we want to predict just the last day\n",
        "  #train_targets=np.reshape(train_targets,(train_targets.shape[0],train_targets.shape[1],-1))\n",
        "  #train_targets=np.reshape(train_targets,(train_targets.shape[0],-1,train_targets.shape[1]))\n",
        "  \n",
        "  \n",
        "  test_inputs=test_data[:][:,:-1,:]\n",
        "  #test_targets=test_data[:,1:,:]\n",
        "  \n",
        "  test_targets=test_data[:,-1,close_ind]\n",
        "  #test_targets=test_data[:,1:,-1]\n",
        "  #test_targets=test_data[:,-1,:] \n",
        "  #test_targets=np.reshape(test_targets,(test_targets.shape[0],test_targets.shape[1],-1))\n",
        "  #test_targets=np.reshape(test_targets,(test_targets.shape[0],-1,test_targets.shape[1]))\n",
        "\n",
        "  if pca==True:\n",
        "    pca=PCA(n_components=0.99)\n",
        "    pca.fit(to_pca)\n",
        "    train_inputs=np.array([pca.transform(chunk) for chunk in train_inputs])\n",
        "    test_inputs=np.array([pca.transform(chunk) for chunk in test_inputs])\n",
        "  \n",
        "  if kpca==True:\n",
        "    pca=PCA(n_components=0.99)\n",
        "    pca.fit(to_pca)\n",
        "    pca_inputs=np.array([pca.transform(chunk) for chunk in train_inputs])\n",
        "    kpca=KernelPCA(n_components=pca_inputs.shape[-1]+5,kernel='cosine',fit_inverse_transform=True)\n",
        "    kpca.fit(to_pca)\n",
        "    train_inputs=np.array([kpca.transform(chunk) for chunk in train_inputs])\n",
        "    test_inputs=np.array([kpca.transform(chunk) for chunk in test_inputs])\n",
        "\n",
        "  return train_inputs,train_targets, test_inputs,test_targets\n",
        "\n",
        "def ShiftOneLabelSplit(data,train_ratio,column,val=False,include_label=True): #column\n",
        "\n",
        "  if val==True:\n",
        "    split_data=data[:round(0.8*len(data))]\n",
        "    \n",
        "    train_data=split_data[:round(0.65*len(split_data))]\n",
        "    val_data=split_data[round(0.35*len(split_data)):]\n",
        "    test_data=data[round(0.8*len(data)):]\n",
        "    \n",
        "    column_ind=data.columns.get_loc(column)\n",
        "    \n",
        "    train_inputs=(train_data.iloc[:-1].values)\n",
        "    train_labels=(train_data.iloc[1:,column_ind].values)\n",
        "    val_inputs=(val_data.iloc[:-1].values)\n",
        "    val_labels=(val_data.iloc[1:,column_ind].values)\n",
        "    test_inputs=(test_data.iloc[:-1].values)\n",
        "    test_labels=(test_data.iloc[1:,column_ind].values)\n",
        "\n",
        "    if include_label==False:\n",
        "      train_inputs=train_inputs[:,:-1]\n",
        "      val_inputs=val_inputs[:,:-1]\n",
        "      test_inputs=test_inputs[:,:-1]\n",
        "\n",
        "    return train_inputs,train_labels,val_inputs,val_labels,test_inputs,test_labels\n",
        "  \n",
        "  else: \n",
        "    train_data=data[:round(0.8*len(data))]\n",
        "    test_data=data[round(0.8*len(data)):]\n",
        "\n",
        "    column_ind=data.columns.get_loc(column)\n",
        "    \n",
        "    train_inputs=(train_data.iloc[:-1].values)\n",
        "    train_labels=(train_data.iloc[1:,column_ind].values)\n",
        "    test_inputs=(test_data.iloc[:-1].values)\n",
        "    test_labels=(test_data.iloc[1:,column_ind].values)\n",
        "\n",
        "    if include_label==False:\n",
        "      train_inputs=train_inputs[:,:-1]\n",
        "      test_inputs=test_inputs[:,:-1]\n",
        "\n",
        "  return train_inputs,train_labels,test_inputs,test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd7GdYHPXCBv"
      },
      "source": [
        "###Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li800lmeF-k2"
      },
      "source": [
        "def feat_permute(clf,train_inputs,train_labels,inputs):\n",
        "\n",
        "  clf.fit(train_inputs,train_labels)\n",
        "  results=pimp(clf,train_inputs,train_labels,scoring='f1_macro',n_repeats=10)\n",
        "  perms=pd.Series(abs(results['importances_mean']),index=inputs.columns)\n",
        " \n",
        "  return perms\n",
        "\n",
        "\n",
        "def feat_importance(train_inputs,train_labels,inputs):\n",
        "\n",
        "  randf=RandomForestClassifier()\n",
        "  randf_perm=feat_permute(randf,train_inputs,train_labels,inputs)\n",
        "\n",
        "  sgc=SGC(loss='log',penalty='l1')\n",
        "  sgc_perm=feat_permute(sgc,train_inputs,train_labels,inputs)\n",
        "\n",
        "  nnet=TimeSeriesMLPClassifier(activation='relu',max_iter=8000,alpha=0.0001)\n",
        "  nnet_perm=feat_permute(nnet,train_inputs,train_labels,inputs)\n",
        " \n",
        "  logreg=LogisticRegression(multi_class='multinomial',penalty='l1',solver='saga',max_iter=300)\n",
        "  logreg_perm=feat_permute(logreg,train_inputs,train_labels,inputs)\n",
        "\n",
        "  gbc=GradientBoostingClassifier()\n",
        "  gbc_perm=feat_permute(gbc,train_inputs,train_labels,inputs)\n",
        "\n",
        "  ada=AdaBoostClassifier()\n",
        "  ada_perm=feat_permute(ada,train_inputs,train_labels,inputs)\n",
        "\n",
        "  gnb=GaussianNB()\n",
        "  gnb_perm=feat_permute(gnb,train_inputs,train_labels,inputs)\n",
        "\n",
        "  bag=BaggingClassifier()\n",
        "  bag_perm=feat_permute(bag,train_inputs,train_labels,inputs)\n",
        "  \n",
        "  params_lightGB = {\n",
        "    'task': 'train',\n",
        "    'application':'binary',\n",
        "    'num_class':1,\n",
        "    'boosting': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'metric_freq':50,\n",
        "    'is_training_metric':False,\n",
        "    'max_depth':4,\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.01,\n",
        "    'feature_fraction': 1.0,\n",
        "    'bagging_fraction': 1.0,\n",
        "    'bagging_freq': 0,\n",
        "    'bagging_seed': 2018,\n",
        "    'verbose': 0,\n",
        "    'num_threads':16\n",
        "    }\n",
        "\n",
        "  lgb_train=lgb.Dataset(train_inputs[:round(0.8*len(train_inputs))],train_labels[:round(0.8*len(train_inputs))])\n",
        "  lgb_val=lgb.Dataset(train_inputs[round(0.8*len(train_inputs)):],train_labels[round(0.8*len(train_inputs)):])\n",
        "  lgbm = lgb.train(params_lightGB, lgb_train,\n",
        "                      num_boost_round=2000,\n",
        "                      valid_sets= lgb_val,\n",
        "                      early_stopping_rounds=200,\n",
        "                      verbose_eval=False)\n",
        "\n",
        "  lgb_feat=pd.Series(lgbm.feature_importance(),index=data.columns,name='lgb')\n",
        "\n",
        "  feat_importance=pd.concat([logreg_perm,sgc_perm,ada_perm,gbc_perm,gnb_perm,bag_perm,nnet_perm,randf_perm,lgb_feat],axis=1)\n",
        "  scaler=MinMaxScaler((-1,1))\n",
        "  scaled_feats=scaler.fit_transform(feat_importance)\n",
        "\n",
        "  scaled_feats_df=pd.DataFrame(scaled_feats,columns=feat_importance.columns,index=feat_importance.index)\n",
        "  sorted_feats=(abs(scaled_feats_df)).sum(1).sort_values()\n",
        "\n",
        "  return sorted_feats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjaZqXPeXLvs"
      },
      "source": [
        "###Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT1AvyINO_Sf"
      },
      "source": [
        "def StackPred(train_inputs,train_labels,inputs,kpca=False,pca=False):\n",
        "  orig_inputs=inputs.copy()\n",
        "\n",
        "  if kpca==True:\n",
        "    pca=PCA(n_components=0.9999)\n",
        "    pca_inputs=pca.fit_transform(train_inputs)\n",
        "    kpca=KernelPCA(n_components=pca_inputs.shape[-1]+5,kernel='cosine',fit_inverse_transform=True)\n",
        "    train_inputs=kpca.fit_transform(train_inputs)\n",
        "    print(train_inputs.shape)\n",
        "    inputs=kpca.transform(inputs)\n",
        "      \n",
        "  if pca ==True:\n",
        "    pca=PCA(n_components=0.9999)\n",
        "    train_inputs=pca.fit_transform(train_inputs)\n",
        "    print(train_inputs.shape)\n",
        "    inputs=pca.transform(inputs)\n",
        "\n",
        "  gknn=GlobalWeightedKNN()\n",
        "  gknn.fit(train_inputs,train_labels,k=1,weights=np.random.rand(train_inputs.shape[-1]))\n",
        "  knn_preds=pd.Series(np.squeeze(gknn.predict(inputs),axis=1),name='knn')\n",
        "\n",
        "  nnet=TimeSeriesMLPClassifier(activation='relu',max_iter=8000,alpha=0.0001)\n",
        "  nnet.fit(train_inputs,train_labels)\n",
        "  nn_preds=pd.Series(nnet.predict(inputs),name='nn')\n",
        "  nn_probs=pd.Series(nnet.predict_proba(inputs)[:,-1],name='nn')\n",
        "\n",
        "  svm=TimeSeriesSVC(kernel='rbf',probability=True)\n",
        "  svm.fit(train_inputs,train_labels)\n",
        "  svm_preds=pd.Series(svm.predict(inputs),name='svm')\n",
        "  svm_probs=pd.Series(svm.predict_proba(inputs)[:,-1],name='svm')\n",
        "\n",
        "  randf=RandomForestClassifier()\n",
        "  randf.fit(train_inputs,train_labels)\n",
        "  randf_preds=pd.Series(randf.predict(inputs),name='randf')\n",
        "  randf_probs=pd.Series(randf.predict_proba(inputs)[:,-1],name='randf')\n",
        "\n",
        "  sgc=SGC(loss='log',penalty='l1')\n",
        "  sgc.fit(train_inputs,train_labels)\n",
        "  sgc_preds=pd.Series(sgc.predict(inputs),name='sgc')\n",
        "  sgc_probs=pd.Series(sgc.predict_proba(inputs)[:,-1],name='sgc')\n",
        "\n",
        "  logreg=LogisticRegression(multi_class='multinomial',penalty='l1',solver='saga',max_iter=300)\n",
        "  logreg.fit(train_inputs,train_labels)\n",
        "  logreg_preds=pd.Series(logreg.predict(inputs),name='logreg')\n",
        "  logreg_probs=pd.Series(logreg.predict_proba(inputs)[:,-1],name='logreg')\n",
        "  \n",
        "  gnb=GaussianNB()\n",
        "  gnb.fit(train_inputs,train_labels)\n",
        "  gnb_preds=pd.Series(gnb.predict(inputs),name='gnb')\n",
        "  gnb_probs=pd.Series(gnb.predict_proba(inputs)[:,-1],name='gnb')\n",
        "\n",
        "  gbc=GradientBoostingClassifier()\n",
        "  gbc.fit(train_inputs,train_labels)\n",
        "  gbc_preds=pd.Series(gbc.predict(inputs),name='gbc')\n",
        "  gbc_probs=pd.Series(gbc.predict_proba(inputs)[:,-1],name='gbc')\n",
        "\n",
        "  ada=AdaBoostClassifier()\n",
        "  ada.fit(train_inputs,train_labels)\n",
        "  ada_preds=pd.Series(ada.predict(inputs),name='ada')\n",
        "  ada_probs=pd.Series(ada.predict_proba(inputs)[:,-1],name='ada')\n",
        "\n",
        "  bag=BaggingClassifier()\n",
        "  bag.fit(train_inputs,train_labels)\n",
        "  bag_preds=pd.Series(bag.predict(inputs),name='bag')\n",
        "  bag_probs=pd.Series(bag.predict_proba(inputs)[:,-1],name='bag')\n",
        "  \n",
        "  scaler=MinMaxScaler((-1,1))\n",
        "  params_lightGB = {\n",
        "    'task': 'train',\n",
        "    'application':'binary',\n",
        "    'num_class':1,\n",
        "    'boosting': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'metric_freq':50,\n",
        "    'is_training_metric':False,\n",
        "    'max_depth':4,\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.01,\n",
        "    'feature_fraction': 1.0,\n",
        "    'bagging_fraction': 1.0,\n",
        "    'bagging_freq': 0,\n",
        "    'bagging_seed': 2018,\n",
        "    'verbose': 0,\n",
        "    'num_threads':16\n",
        "    }\n",
        "  \n",
        "  lgb_train=lgb.Dataset(train_inputs[:round(0.8*len(train_inputs))],train_labels[:round(0.8*len(train_inputs))])\n",
        "  lgb_val=lgb.Dataset(train_inputs[round(0.8*len(train_inputs)):],train_labels[round(0.8*len(train_inputs)):])\n",
        "  lgbm = lgb.train(params_lightGB, lgb_train,\n",
        "                      num_boost_round=2000,\n",
        "                      valid_sets= lgb_val,\n",
        "                      early_stopping_rounds=200,\n",
        "                      verbose_eval=False)\n",
        "  lgb_preds=pd.Series(lgbm.predict(inputs),name='lgb')\n",
        "\n",
        "  scaled_lgb_probs=pd.DataFrame(scaler.fit_transform(lgb_preds.values.reshape(lgb_preds.shape+(1,))),columns=['lgb'])\n",
        "  scaled_lgb_preds=pd.Series([1 if x>0 else -1 if x<0 else 0 for x in scaled_lgb_probs.values],name='lgb')\n",
        "\n",
        "  prophecy=Prophesize(orig_inputs['close_returns'])\n",
        "  detrended=signal.detrend(prophecy['yhat'])\n",
        "  detrended_series=pd.Series(detrended,name='prophet')\n",
        "  prophet_preds=pd.Series(np.array([1 if x>0 else -1 if x<0 else 0 for x in detrended]),name='prophet')\n",
        "  \n",
        "  sample_len=17\n",
        "  close_ind=orig_inputs.columns.get_loc('label')\n",
        "  \n",
        "  train_inputs,train_targets,test_inputs,test_targets=Tensify(orig_inputs,0.7,sample_len,kpca=kpca,pca=pca,close_ind=close_ind)\n",
        "\n",
        "  model=net(100,0.1,train_inputs,single_label=True).to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(\n",
        "      lr=0.001,\n",
        "      amsgrad=True,\n",
        "      eps=1e-7,\n",
        "      params=model.parameters(),\n",
        "      weight_decay=0.001\n",
        "  )\n",
        "\n",
        "  loss_func=nn.L1Loss()\n",
        "  num_epochs=300\n",
        "\n",
        "  train_losses=[]\n",
        "  loss_sum=0\n",
        "  for epoch in range(num_epochs):\n",
        "    avg_loss,output=train_step(train_inputs,train_targets,model,loss_func,optimizer)\n",
        "    \n",
        "    loss_sum+=avg_loss\n",
        "    if (epoch % 5) == 4: \n",
        "        train_losses.append(loss_sum/5)\n",
        "        loss_sum=0\n",
        "\n",
        "  scaled_inputs=torch.cat((train_inputs,test_inputs))\n",
        "  scaled_targets=torch.cat((train_targets,test_targets))\n",
        "  \n",
        "  evals, _=eval_step(scaled_inputs,scaled_targets,model,loss_func)\n",
        "  pad = pd.Series(np.nan,index=list(range(sample_len)),name='lstm')\n",
        "  lstm_preds=pd.Series(np.array([1 if x>0 else -1 if x<0 else 0 for x in evals]),name='lstm')\n",
        "  lstm_probs=pd.Series(evals.cpu().detach().numpy(),name='lstm')\n",
        "  padded_preds=pd.concat([pad,lstm_preds],ignore_index=True)\n",
        "  padded_probs=pd.concat([pad,lstm_probs],ignore_index=True)\n",
        "\n",
        "  pred_df=pd.concat([svm_preds,randf_preds,nn_preds,sgc_preds,knn_preds,gbc_preds,logreg_preds,gnb_preds,ada_preds,bag_preds,padded_preds,prophet_preds,scaled_lgb_preds],axis=1) \n",
        "  proba_df=pd.concat([svm_probs,knn_preds,randf_probs,nn_probs,sgc_probs,gbc_probs,gnb_probs,logreg_probs,ada_probs,bag_probs,padded_probs,detrended_series,scaled_lgb_probs],axis=1)  \n",
        "\n",
        "  #pred_df=pd.concat([padded_preds,gnb_preds,sgc_preds,logreg_preds,ada_preds,prophet_preds,scaled_lgb_preds,nn_preds],axis=1)\n",
        "  #proba_df=pd.concat([padded_probs,gnb_probs,sgc_probs,logreg_probs,ada_probs,detrended_series,scaled_lgb_probs,nn_probs],axis=1)\n",
        "\n",
        "  proba_df.index=orig_inputs.index\n",
        "  pred_df.index=orig_inputs.index\n",
        "\n",
        "  proba_df['label']=orig_inputs['label']\n",
        "  pred_df['label']=orig_inputs['label']\n",
        "\n",
        "  return pred_df.dropna(),proba_df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob78sikIbxEa"
      },
      "source": [
        "####Train, Evaluate, and Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS9Yx6j1-n_a"
      },
      "source": [
        "def train_step(inputs, targets,model,loss_func,optimizer):\n",
        "    \n",
        "    model.zero_grad()\n",
        "    output=model(inputs)\n",
        "    \n",
        "    loss = loss_func(output,targets)\n",
        "    \n",
        "    loss.backward()#(retain_graph=True)\n",
        "    optimizer.step()\n",
        "    \n",
        "    avg_loss=loss\n",
        "    return avg_loss,output\n",
        " \n",
        "def eval_step(inputs,targets,model,loss_func):\n",
        "    model.eval()\n",
        "    evals= model(inputs)\n",
        "    loss=loss_func(evals,targets)\n",
        "    avg_loss=loss\n",
        "    return evals, avg_loss\n",
        " \n",
        "def confidence_test(confidence,evals,test_targets):\n",
        " \n",
        "  evals_list=[]\n",
        "  target_list=[]\n",
        " \n",
        "  for i in range(len(evals)):\n",
        "    if evals[i]>confidence:\n",
        "      evals_list.append(1)\n",
        "      target_list.append(test_targets[i].item())\n",
        " \n",
        "    elif evals[i]<-confidence:\n",
        "      evals_list.append(-1)\n",
        "      target_list.append(test_targets[i].item())\n",
        "    \n",
        "    else:\n",
        "      continue\n",
        " \n",
        "  evals_arr=np.array(evals_list)\n",
        "  target_arr=np.array(target_list)\n",
        " \n",
        "  f1_score=f1(evals_arr,target_arr,average='macro')\n",
        "  \n",
        "  return evals_arr,target_arr,f1_score\n",
        " \n",
        "def Train_and_Evaluate(ensemble_df,train_ratio,sample_len):\n",
        "  train_inputs,train_targets,test_inputs,test_targets=Tensify(ensemble_df,train_ratio,sample_len,ensemble_df.columns.get_loc('label'))\n",
        "  \n",
        "  model=net(100,0.2,train_inputs,single_label=True).to(device)\n",
        "  \n",
        "  optimizer = torch.optim.Adam(\n",
        "      lr=0.001,\n",
        "      amsgrad=True,\n",
        "      params=model.parameters(),\n",
        "      weight_decay=0.001\n",
        "  )\n",
        " \n",
        "  loss_func= nn.L1Loss()\n",
        "  \n",
        "  num_epochs=150\n",
        "  train_losses=[]\n",
        "  loss_sum=0\n",
        "  for epoch in range(num_epochs):\n",
        "    avg_loss,output=train_step(train_inputs,train_targets,model,loss_func,optimizer)\n",
        "    \n",
        "    loss_sum+=avg_loss\n",
        "    if (epoch % 5) == 4: \n",
        "        train_losses.append(loss_sum/5)\n",
        "        loss_sum=0\n",
        " \n",
        "  plt.plot(train_losses)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training Loss')\n",
        " \n",
        "  evals, eval_loss=eval_step(test_inputs,test_targets,model,loss_func)\n",
        " \n",
        "  trans_evals=np.array([1 if x>0 else -1 if x<0 else 0 for x in evals])\n",
        "  trans_preds=np.array([1 if x>0 else -1 if x<0 else 0 for x in output])\n",
        "  print('Training f1_score:',f1(train_targets.cpu().numpy(),trans_preds,average='macro'),'for',ticker)\n",
        "  print('Testing f1_score', f1(test_targets.cpu().numpy(),trans_evals,average='macro'),'for',ticker)\n",
        " \n",
        "  return model,evals,test_targets\n",
        " \n",
        "def Predict(ensemble_df,model,evals,test_targets,sample_len):\n",
        "\n",
        "  predict_input=torch.Tensor(ensemble_df.iloc[-sample_len:,:].values).unsqueeze(0).to(device)\n",
        "  prediction=model(predict_input).item()\n",
        "  print('prediction',prediction)\n",
        "\n",
        "  evals_arr,target_arr,f1_score=confidence_test(prediction,evals,test_targets)\n",
        "  print('f1 score for confidence',f1_score)\n",
        "\n",
        "  seq_len=10\n",
        "  f1_list=[]\n",
        "  for i in range(len(target_arr)):\n",
        "\n",
        "    f1_list.append(f1(target_arr[i:i+seq_len],evals_arr[i:i+seq_len],average='macro'))\n",
        "\n",
        "  print('last days f1',f1_list[-1])\n",
        "  \n",
        "  return f1_list,prediction,f1_score\n",
        " \n",
        "def To_Judge(pred_df,prob_df):\n",
        "    \n",
        "    sample_len=5\n",
        "    pred_model,pred_evals,pred_test_targets=Train_and_Evaluate(pred_df,0.7,sample_len)\n",
        "    prob_model,prob_evals,prob_test_targets=Train_and_Evaluate(prob_df,0.7,sample_len)\n",
        "    \n",
        "    pred_f1,pred_prediction,pred_f1_score=Predict(pred_df,pred_model,pred_evals,pred_test_targets,sample_len)\n",
        "    if (pred_f1_score>0.7):\n",
        "      if (pred_f1[-1] >=0.8):\n",
        "        return pred_prediction*pred_f1_score*pred_f1[-1]\n",
        "    \n",
        "    else:\n",
        "      prob_f1,prob_prediction,prob_f1_score=Predict(prob_df,prob_model,prob_evals,prob_test_targets,sample_len)\n",
        "\n",
        "      if (prob_f1_score >0.7):\n",
        "        if (prob_f1[-1] >=0.8):\n",
        "          return prob_prediction*prob_f1_score*prob_f1[-1]\n",
        " \n",
        "      else:\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXXdq6_XcVIC"
      },
      "source": [
        "####Bet Sizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N__eYwe3uRNt"
      },
      "source": [
        "def Prices(stocks_list):\n",
        "\n",
        "  prices_df=pd.DataFrame()\n",
        "  \n",
        "  for stock_symbol in stocks_list:\n",
        "     \n",
        "    data=pdr.get_data_yahoo(stock_symbol)\n",
        "    \n",
        "    prices_df[str(stock_symbol)]=data['Close']\n",
        "    \n",
        "    prices_df=prices_df.dropna(axis=0)\n",
        "\n",
        "  return prices_df\n",
        "\n",
        "def Blacklitter(prices_df,view_dict):\n",
        "  risk_model=pypfopt.risk_models.CovarianceShrinkage(prices_df)\n",
        "  cov_matrix=risk_model.oracle_approximating()\n",
        "\n",
        "  view_series=pd.Series(view_dict)\n",
        "\n",
        "  cov_matrix=pypfopt.risk_models.fix_nonpositive_semidefinite(cov_matrix,fix_method='spectral')\n",
        "\n",
        "  if np.linalg.det(cov_matrix)==0:\n",
        "    cov_matrix=cov_matrix+0.0000001\n",
        "  \n",
        "  bl=pypfopt.black_litterman.BlackLittermanModel(cov_matrix=cov_matrix,absolute_views=view_series)\n",
        "\n",
        "  weights=bl.bl_weights()\n",
        "  performance=bl.portfolio_performance()\n",
        "\n",
        "  return weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la2BxvxyKuZr"
      },
      "source": [
        "###Putting It All Together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgqW9YxzOymR"
      },
      "source": [
        "class StockPredictor:\n",
        "  def __init__(self):\n",
        "    string=input()\n",
        "    self.ticker_list=string.split(',')\n",
        "  \n",
        "  def Do_Everything(self):\n",
        "    views_dict={}\n",
        "    for main_ticker in self.ticker_list:\n",
        "      #try:\n",
        "        global ticker\n",
        "        ticker=main_ticker\n",
        "        print('--'*4,ticker,'--'*4)\n",
        "        scaled_df,data=Get_Data(ticker)\n",
        "        training_inputs,training_labels,test_inputs,test_labels=ShiftOneLabelSplit(scaled_df,0.7,'label',val=False)\n",
        "        pred_df,prob_df=StackPred(training_inputs,training_labels,scaled_df,pca=True)\n",
        "        \n",
        "        pca_prediction=To_Judge(pred_df,prob_df)\n",
        "    \n",
        "        if pca_prediction==None:\n",
        "          pred_df,prob_df=StackPred(training_inputs,training_labels,scaled_df)\n",
        "          reg_prediction=To_Judge(pred_df,prob_df)\n",
        "\n",
        "          if reg_prediction!=None:\n",
        "            views_dict[ticker]=reg_prediction\n",
        "            \n",
        "          else:\n",
        "            pred_df,prob_df=StackPred(training_inputs,training_labels,scaled_df,kpca=True)\n",
        "            kpca_prediction=To_Judge(pred_df,prob_df)\n",
        "            if kpca_prediction!=None:\n",
        "              views_dict[ticker]=kpca_prediction\n",
        "\n",
        "        else:\n",
        "          views_dict[ticker]=pca_prediction \n",
        "      \n",
        "      #except:\n",
        "        #pass\n",
        "    \n",
        "    prices_df=Prices(views_dict.keys())\n",
        "    weights=Blacklitter(prices_df,views_dict)\n",
        "\n",
        "    for key,value in weights.items():\n",
        "      data=pdr.get_data_yahoo(key)\n",
        "      price=data.Close[-1]\n",
        "      shares=value/price\n",
        "      print('Get',shares,'shares of',key)\n",
        "\n",
        "    return weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdqYkdeWbM3z"
      },
      "source": [
        "stockpred=StockPredictor()\n",
        "weights=stockpred.Do_Everything()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9mE7Xf4Hgx6"
      },
      "source": [
        "def TickerTrends_Daily(ticker):\n",
        "  keywords=['buy '+ticker+' stock','sell '+ticker+' stock',ticker+' earnings'] #ticker+' stock'\n",
        "\n",
        "  df=yf.Ticker(ticker).history(period='max')\n",
        "\n",
        "  collector=[]\n",
        "\n",
        "  if df.index[0].year >2010:\n",
        "    for search_term in keywords:\n",
        "      try:\n",
        "    \n",
        "        daily_data=pytrends.dailydata.get_daily_data(search_term,start_year=df.index[0].year,start_mon=df.index[0].month,stop_year=datetime.now().date().year,stop_mon=datetime.now().date().month)\n",
        "\n",
        "        data=daily_data[search_term+'_unscaled']\n",
        "        data=data.rename(search_term)\n",
        "        collector.append(data)\n",
        "\n",
        "      except:\n",
        "        try:\n",
        "          search_term=search_term[:-6]\n",
        "          daily_data=pytrends.dailydata.get_daily_data(search_term,start_year=df.index[0].year,start_mon=df.index[0].month,stop_year=datetime.now().date().year,stop_mon=datetime.now().date().month)\n",
        "\n",
        "          data=daily_data[search_term+'_unscaled']\n",
        "          data=data.rename(search_term)\n",
        "          collector.append(data)\n",
        "\n",
        "        except:\n",
        "          pass\n",
        "  \n",
        "  \n",
        "  else:\n",
        "    for search_term in keywords:\n",
        "      try:\n",
        "        daily_data=pytrends.dailydata.get_daily_data(search_term,start_year=2010,start_mon=1,stop_year=datetime.now().date().year,stop_mon=datetime.now().date().month)\n",
        "\n",
        "        data=daily_data[search_term+'_unscaled']\n",
        "        data=data.rename(search_term)\n",
        "        collector.append(data)\n",
        "\n",
        "      except:\n",
        "        try:\n",
        "          search_term=search_term[:-6]\n",
        "          daily_data=pytrends.dailydata.get_daily_data(search_term,start_year=2010,start_mon=1,stop_year=datetime.now().date().year,stop_mon=datetime.now().date().month)\n",
        "\n",
        "          data=daily_data[search_term+'_unscaled']\n",
        "          data=data.rename(search_term)\n",
        "          collector.append(data)\n",
        "\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "  ticker_data=pd.concat(collector,axis=1)\n",
        "    \n",
        "  return ticker_data\n",
        "\n",
        "def CombineTickerTrends(ticker):\n",
        "  ticker_gtrends_df=UpdateTickerGTrends(ticker)\n",
        "\n",
        "  other_df=TickerTrends_Daily(ticker)\n",
        "\n",
        "  ticker_gtrends_df=pd.concat([ticker_gtrends_df,other_df],axis=1).dropna()\n",
        "\n",
        "  ticker_gtrends_df.to_csv('drive/My Drive/Nick Fury/External Datasets/gtrends.csv/{}.csv'.format(ticker))\n",
        "\n",
        "  return ticker_gtrends_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wy7JeoMSqWU"
      },
      "source": [
        "def UpdateTickerGTrends(ticker):\n",
        "  ticker_gtrends_df=pd.read_csv('drive/My Drive/Nick Fury/External Datasets/gtrends.csv/{}.csv'.format(ticker)).dropna()\n",
        "\n",
        "  ticker_gtrends_df.index=ticker_gtrends_df.date\n",
        "  ticker_gtrends_df=ticker_gtrends_df.iloc[:,1:]\n",
        "  ticker_gtrends_df.index=pd.to_datetime(ticker_gtrends_df.index)\n",
        "\n",
        "  timeframe='now 7-d'\n",
        "\n",
        "  collector=[]\n",
        "\n",
        "  for keyword in ticker_gtrends_df.columns:\n",
        "\n",
        "    pytrends_object = pytrends.request.TrendReq(hl='en-US', tz=360)\n",
        "\n",
        "    pytrends_object.build_payload([keyword],timeframe=timeframe)\n",
        "\n",
        "    single_gtrends_df=pytrends_object.interest_over_time()[keyword]\n",
        "\n",
        "    single_gtrends_df=single_gtrends_df.resample('1D').mean()\n",
        "    \n",
        "    collector.append(single_gtrends_df)\n",
        "\n",
        "  multi_gtrends_df=pd.concat(collector,axis=1)\n",
        "\n",
        "  ticker_gtrends_df=pd.concat([ticker_gtrends_df,multi_gtrends_df])\n",
        "\n",
        "  ticker_gtrends_df = ticker_gtrends_df[~ticker_gtrends_df.index.duplicated(keep='first')]\n",
        "\n",
        "  ticker_gtrends_df = ticker_gtrends_df.loc[:,~ticker_gtrends_df.columns.duplicated()]\n",
        "\n",
        "  ticker_gtrends_df.to_csv('drive/My Drive/Nick Fury/External Datasets/gtrends.csv/{}.csv'.format(ticker))\n",
        "\n",
        "  return ticker_gtrends_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYYkd8yCI71E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
