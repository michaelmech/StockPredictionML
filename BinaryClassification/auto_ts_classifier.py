# -*- coding: utf-8 -*-
"""Auto_TS_Classifier

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DpGLa60SkIBGLR_A6O1zyRz09bjIWJYh
"""

!pip install -U yfinance
!pip install quandl

#!pip install --upgrade statsmodels
!pip install statsmodels==0.13.1
!pip install --upgrade scipy

#!pip install optuna
!pip install flaml  

#!pip install tpot
!pip install featexp

!pip install sktime
!pip install pytrends

!pip install --upgrade ta
!pip install pandas_ta
!pip install stumpy

import pandas as pd
import numpy as np
import pandas_datareader.data as pdr
import matplotlib.pyplot as plt
import stumpy

from sklearn import manifold
from sklearn.pipeline import Pipeline

from sklearn.preprocessing import binarize

import quandl
import json

import math

import tqdm
from tqdm import tqdm_notebook

from sklearn.ensemble import IsolationForest

from sklearn.decomposition import TruncatedSVD

import torch
import torch.nn as nn

import pickle

import yfinance as yf
yf.pdr_override()

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures

import joblib
from joblib import Parallel,delayed

import datetime
#from fracdiff.sklearn import FracdiffStat

import sktime

from sktime.forecasting.model_selection import temporal_train_test_split as ttts

#from sklearn.feature_selection import VarianceThreshold
import csv
from google.colab import files

from google.colab import drive
drive.mount('drive',force_remount=True)

from statsmodels.tsa.stattools import adfuller

import pandas_ta
import ta

from flaml import AutoML

import featexp
from featexp import get_trend_stats

from scipy.signal import find_peaks

import lightgbm

from lightgbm import LGBMClassifier

import sklearn
from sklearn.metrics import roc_auc_score
from sklearn.metrics import f1_score
from sklearn.feature_selection import RFE

from sklearn import model_selection

import datetime

from datetime import timedelta

import sklearn
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import RFE
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier

import pytrends
from pytrends import dailydata

from scipy.stats import shapiro


import warnings
warnings.simplefilter("ignore")

###helper functions

def colon_print(word1,word2):
  print(word1,':',word2)

from sklearn.model_selection import StratifiedKFold, train_test_split

def shapiro_normality_test(data,nan_sub=-1):

  if nan_sub:
    data=data[data!=nan_sub]
  
  valid=[]
  for col in data:

    stat,p=shapiro(data[col].values)

    if p>0.05:
      valid.append(col)
    
  return valid

def insort(a, b, kind='mergesort'):
    # took mergesort as it seemed a tiny bit faster for my sorted large array try.
    c = np.concatenate((a, b)) # we still need to do this unfortunatly.
    c.sort(kind=kind)
    flag = np.ones(len(c), dtype=bool)
    np.not_equal(c[1:], c[:-1], out=flag[1:])
    return c[flag]

class StratifiedKFold3(StratifiedKFold):

  def split(self, X, y, groups=None):
    s = super().split(X, y, groups)
    for train_indxs, test_indxs in s:
      y_train = y[train_indxs]
      train_indxs, cv_indxs = train_test_split(train_indxs,stratify=y_train, test_size=(1 / (self.n_splits - 1)))
      yield train_indxs, cv_indxs, test_indxs

def split_data(df,targets=None,test_size=0.1):

  df=df.copy()
  
  if targets is not None:
    df['targets']=targets

  x=df.drop('targets',axis=1)
  
  y=df['targets']
  
  xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=test_size, random_state=69, stratify=y)

  train_val_data= pd.concat([xtrain,ytrain],axis=1)

  test_data=pd.concat([xtest,ytest],axis=1)

  return train_val_data,test_data

def ts_split_data(df,targets=None,test_size=0.1):
  df=df.copy()

  if targets is not None:
    df['targets']=targets

  x=df.drop('targets',axis=1)
  
  y=df['targets']

  ytrain, ytest, xtrain, xtest= ttts(y,x,test_size=test_size)

  train_val_data= pd.concat([xtrain,ytrain],axis=1)

  test_data=pd.concat([xtest,ytest],axis=1)

  return train_val_data,test_data

def upload_sentiment():

  uploaded=files.upload()

  name=list(uploaded.keys())[0]

  df=pd.read_excel(name)

  df=df.iloc[2:,:4]

  df['Date']=pd.to_datetime(df['Unnamed: 0'],errors='coerce') 

  df=df.dropna(subset=['Date'])
  df.index=df['Date']

  df=df.drop(['Date','Unnamed: 0'],axis=1)

  columns=['Bullish','Neutral','Bearish']
  df.columns=columns

  return df

def post_process(df,miss_threshold=0.95,corr_thresh=0.98):

  df=df.copy()

  df=df.replace({np.inf: np.nan, -np.inf:np.nan})

  if 'targets' in df:
    targets=df['targets']
    df=df.drop('targets',axis=1)

  df=df.iloc[:,~df.columns.duplicated()] #remove duplicated indices

  missing= pd.DataFrame(df.isnull().sum())

  missing['fraction']= missing[0]/df.shape[0]

  missing.sort_values('fraction', ascending = False, inplace = True)

  missing_cols = list(missing[missing['fraction'] > missing_threshold].index)

  df=df.drop(missing_cols,axis=1)

  unique_counts = pd.DataFrame(df.nunique()).sort_values(0, ascending = True)

  zero_variance_cols = list(unique_counts[unique_counts[0] == 1].index)

  df=df.drop(zero_variance_cols,axis=1)

  corr_matrix = feature_matrix.corr()==df.corr

  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))

  high_corr_cols = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]

  df=df.drop(high_corr_cols,axis=1)

  return df,targets

def split_sanity_check(lst_of_data=[],targets=None):
  
  if not isinstance(lst_of_data,list):
    raise Exception('Not a list')
  

  for i in range(len(lst_of_data)-2):
    print(i)
    lst_of_data[i].align([lst_of_data][i+1],join='inner',axis=1)

  
  if targets is not None:
    
    assert((pd.concat([lst_of_data]).shape[0])==len(targets))

def compute_vol(df: pd.DataFrame,
                span: int=100) -> pd.DataFrame:
    '''       
     Compute period volatility of returns as exponentially
     weighted    moving standard deviation:
     Args:        
          df (pd.DataFrame): Dataframe with price series in a single 
          column.        
          span (int): Span for exponential weighting.    
     Returns:        pd.DataFrame: Dataframe containing volatility 
          estimates.      
    '''
    df.fillna(method='ffill', inplace=True)
    r = df.pct_change()
    return r.ewm(span=span).std()

def tbl(
    df: pd.DataFrame, 
    t: int, 
    upper: float=None, 
    lower: float=None,
    devs: float=2.5,
    join: bool=False,
    span: int=100) -> pd.DataFrame:

    if t < 1:
      raise ValueError("Look ahead time invalid, t<1.")
    
    df.fillna(method='ffill', inplace=True)

    lims = np.array([upper, lower])    

    labels = pd.DataFrame(index=df.index, columns=['Label'])

    returns = df.pct_change()

    r = range(0, len(df)-1-t)
    
    for idx in r:
      s = returns.iloc[idx:idx+t]
      minimum = s.cumsum().values.min()
      maximum = s.cumsum().values.max()

      if not all(np.isfinite(s.cumsum().values)):
          labels['Label'].iloc[idx] = np.nan
          continue

      if any(lims == None):
          vol = compute_vol(df[:idx+t], span)
          
      if upper == None:
          u = vol.iloc[idx]*devs
      else:
          u = upper
          
      if lower == None:
          l = -vol.iloc[idx]*devs
      else:
          l = lower
      
      valid = np.isfinite(u) and np.isfinite(l)
      
      if not valid:
        labels['Label'].iloc[idx] = np.nan
        continue 
      
      if any(s.cumsum().values >= u):
        labels['Label'].iloc[idx] = 1
      elif any(s.cumsum().values <= l):
        labels['Label'].iloc[idx] = -1
      else:
        labels['Label'].iloc[idx] = 0
    
    if join:
      df = df.join(labels)
      return df        
    
    return labels

def upload_sentiment():

  uploaded=files.upload()

  name=list(uploaded.keys())[0]

  df=pd.read_excel(name)

  df=df.iloc[2:,:4]

  df['Date']=pd.to_datetime(df['Unnamed: 0'],errors='coerce') 

  df=df.dropna(subset=['Date'])
  df.index=df['Date']

  df=df.drop(['Date','Unnamed: 0'],axis=1)

  columns=['Bullish','Neutral','Bearish']
  df.columns=columns

  return df

def update_stuff():
  
  sentiment_df=get_sentiment()

  d2=current_date=datetime.datetime.now()
  d1=prev_date=sentiment_df.index[-1]
  
  monday1 = (d1 - timedelta(days = d1.weekday()))
  monday2 = (d2 - timedelta(days = d2.weekday()))

  week_diff=(monday2 - monday1).days / 7

  if week_diff>0:
    update_sentiment()

  update_one_fracdiff()

  update_gtrends()

  return sentiment_df

def update_sentiment():
  
  df=upload_sentiment()

  df.to_csv('/content/drive/MyDrive/BinaryStocks/csv/sentiment.csv')

def get_sentiment():
  df=pd.read_csv('/content/drive/MyDrive/BinaryStocks/csv/sentiment.csv')

  df.index=pd.to_datetime(df.Date)

  df=df.drop('Date',axis=1)

  return df

def update_gtrends():
  gtrends_df=pd.read_csv('/content/drive/MyDrive/BinaryStocks/csv/google_trends.csv')

  gtrends_df.index=gtrends_df.date

  gtrends_df.index=pd.to_datetime(gtrends_df.index)

  gtrends_df=gtrends_df.drop('date',axis=1)

  next_date=(gtrends_df.index[-1]+datetime.timedelta(1))

  current_date=datetime.datetime.now()

  current_month=current_date.month

  current_year=current_date.year

  month=next_date.month
  year=next_date.year

  gseries_lst=[]
  for term in gtrends_df:
    gtrend_series=pytrends.dailydata.get_daily_data(term,start_year=year,start_mon=month,stop_year=current_year,stop_mon=current_month)[term+'_unscaled']
    gseries_lst.append(gtrend_series)
  
  new_df=pd.concat(gseries_lst,axis=1)
  new_df.columns=gtrends_df.columns

  gtrends_df=pd.concat([gtrends_df,new_df])

  gtrends_df=gtrends_df[~gtrends_df.index.duplicated(keep='first')]

  gtrends_df.to_csv('/content/drive/MyDrive/BinaryStocks/csv/google_trends.csv')

  return gtrends_df

def update_one_fracdiff():
  
  index_list=['vox','vcr','vdc','vde','vfh','vht','vis','vgt','vaw','vnq','vpu','dbp','vglt','vgit','vgsh','eem','uso','gld','vwo','lqd','spy']
  close_lst=[pdr.get_data_yahoo(ticker)['Close'] for ticker in index_list]
  close_df=pd.concat(close_lst,axis=1)
  
  close_diffs=load_diffs()

  next_date=(close_diffs.index[-1]+datetime.timedelta(1))

  new_diffs=one_step_fracdiff(close_df,close_df.loc[next_date:])

  close_diffs=pd.concat([close_diffs,new_diffs])

  close_diffs.to_csv('/content/drive/MyDrive/BinaryStocks/csv/close_diff.csv')

  return close_diffs

def frac_diff(x, d=0.7):
    """
    Fractionally difference time series

    :param x: numeric vector or univariate time series
    :param d: number specifying the fractional difference order.
    :return: fractionally differenced series
    """
    if np.isnan(np.sum(x)):
        return None

    n = len(x)
    if n < 2:
        return None

    x = np.subtract(x, np.mean(x))

    # calculate weights
    weights = [0] * n
    weights[0] = -d
    for k in range(2, n):
        weights[k - 1] = weights[k - 2] * (k - 1 - d) / k

    # difference series
    ydiff = list(x)

    for i in range(0, n):
        dat = x[:i]
        w = weights[:i]
        ydiff[i] = x[i] + np.dot(w, dat[::-1])

    return np.array(ydiff)

def one_step_fracdiff(close_df,combined_df):
  cutoff_date=combined_df.index[0]

  cutoff_index=len(close_df.loc[:cutoff_date])

  diff_array=np.zeros((combined_df.shape[0],close_df.shape[-1]))

  for t in range(len(diff_array)):
    diff_array[t,:]=frac_diff(close_df.iloc[:cutoff_index+t+1].dropna().values)[-1,:]

  diff_df=pd.DataFrame(diff_array,columns=close_df.columns,index=combined_df.index)

  return diff_df

def load_diffs():

  close_diffs=pd.read_csv('/content/drive/MyDrive/BinaryStocks/csv/close_diff.csv')

  close_diffs.index=pd.to_datetime(close_diffs['Unnamed: 0'])
  close_diffs=close_diffs.drop('Unnamed: 0',axis=1)

  return close_diffs

#sentiment_df=update_stuff()

sentiment_df=get_sentiment()

"""###Data Collection"""

def yahooTA(ticker,freq):
  
  sixty=['1m','2m','5m','15m','30m','15m']
 
  if freq in sixty:
    data=pdr.get_data_yahoo(ticker,interval=freq,period='60d')
    data.index=data.index.tz_convert('US/Pacific')
  
  elif freq=='1h':
    data=pdr.get_data_yahoo(ticker,interval=freq,period='710d')
    data.index=data.index.tz_convert('US/Pacific')
  
  elif freq=='1d' or freq=='1wk':
    #data=pdr.get_data_yahoo(ticker,interval=freq)
    data=pdr.get_data_yahoo(ticker,interval=freq)
  
  elif freq=='1mo':
    data=pdr.get_data_yahoo(ticker,interval=freq,period='1200d')
   
  data=data.drop('Adj Close',axis=1)
  #data=data.reset_index()
  
  data['mfi2']=ta.volume.money_flow_index(high=data['High'],low=data['Low'],close=data['Close'],volume=data['Volume'],window=2)
  data['mfi10']=ta.volume.money_flow_index(high=data['High'],low=data['Low'],close=data['Close'],volume=data['Volume'],window=10)
  
  data['rsi10']=ta.momentum.rsi(close=data['Close'],window=10)
  data['rsi2']=ta.momentum.rsi(close=data['Close'],window=2)            
 
  data['hma10']=pandas_ta.hma(data['Close'],length=10)
  data['hma20']=pandas_ta.hma(data['Close'],length=20)
  data['hma50']=pandas_ta.hma(data['Close'],length=50)
  #data['hma150']=pandas_ta.hma(data['Close'],length=150)
  #data['ema200']=ta.trend.ema_indicator(data['Close'],window=200)
  data['ema10']=pandas_ta.ema(data['Close'],length=10)
  data['ema20']=pandas_ta.ema(data['Close'],length=20)
  data['ema50']=pandas_ta.ema(data['Close'],length=50)
 
  #data['natr']=pandas_ta.natr(data['High'],data['Low'],data['Close'],length=10)
 
  psar=pandas_ta.psar(data['High'],data['Low'],data['Close'],0.4,0.4,0.4)
    
  psar['PSARl_0.4_0.4'].fillna(psar['PSARs_0.4_0.4'],inplace=True)
  data['psar']=psar['PSARl_0.4_0.4']
 
  psar=pandas_ta.psar(data['High'],data['Low'],data['Close'],2,2,2)
 
  psar['PSARl_2.0_2.0'].fillna(psar['PSARs_2.0_2.0'],inplace=True)
  data['stop_loss']=psar['PSARl_2.0_2.0']
 
  data['hbb']=ta.volatility.bollinger_hband(data['Close'])
  data['lbb']=ta.volatility.bollinger_lband(data['Close'])
  
  data['close_returns']=data.Close.pct_change()
  data['price_roc']=data.Close.diff()
  data['vol_roc']=data.Volume.diff().ewm(span=10).mean()
 
  data['Volume'].iloc[:data['hma10'].isnull().sum()]=data['hma10'].iloc[:data['hma10'].isnull().sum()]
 
  data['vol_ema']=data['Volume'].ewm(span=10).mean()
 
  data.columns=[col.lower() for col in data.columns]
 
  data['cci10']=pandas_ta.cci(data['low'],data['high'],data['close'],length=10)
 
  data['qstick10']=pandas_ta.qstick(data['open'],data['close'],length=10)
  data['qstick2']=pandas_ta.qstick(data['open'],data['close'],length=2)
 
  return data.drop(['open','high','low'],axis=1)

def returns_only(combined_df):

  index_list=['vox','vcr','vdc','vde','vfh','vht','vis','vgt','vaw','vnq','vpu','dbp','vglt','vgit','vgsh','eem','uso','pdbc','gld','vwo','lqd']
  returns_df=get_returns(index_list)

  combined_df=pd.concat([combined_df,returns_df],axis=1)
  
  return combined_df

def one_step_matprof(series,name=None):
  window_size=3

  lst=[]

  for t in range(len(series)-window_size):

    lst.append(stumpy.stump(series.iloc[:t+window_size],m=window_size)[-1,0])

  profile=pd.DataFrame(lst,index=series.index[window_size:])

  profile.replace([np.inf, -np.inf], np.nan, inplace=True)

  if name is not None:
    profile.columns=[name+'_prof']

  return profile

def get_volume(ticker_list):
  volume_lst=[]
  for ticker in ticker_list:
    data=yf.Ticker(ticker).history(period='max').dropna()
    data=data.drop(['Dividends','Stock Splits'],axis=1)

    data.columns=[ticker+'_'+col for col in data.columns]
    
    volume_lst.append(data[ticker+'_'+'Volume'])
  
  volume_df=pd.concat(volume_lst,axis=1)
    
  return volume_df

def get_common(sentiment_df=sentiment_df,extract=False):
  if extract:
    #sentiment_df.index=sentiment_df.index+datetime.timedelta(1)

    gtrends_df=pd.read_csv('/content/drive/MyDrive/BinaryStocks/csv/google_trends.csv')

    gtrends_df.index=gtrends_df.date

    gtrends_df.index=pd.to_datetime(gtrends_df.index)

    gtrends_df=gtrends_df.drop('date',axis=1)

    gtrends_df.index=gtrends_df.index + datetime.timedelta(2)

    quandl.ApiConfig.api_key = '7-VJ9UUtktHtEh1iuCaS'

    yale_list=['YALE/US_CONF_INDEX_VAL_INDIV','YALE/US_CONF_INDEX_VAL_INST','YALE/US_CONF_INDEX_CRASH_INDIV','YALE/US_CONF_INDEX_CRASH_INST']

    yale_df_list=[quandl.get(code, start_date='2010-1-1', end_date=str(datetime.datetime.today().date()))['Index Value'] for code in yale_list] 

    yale_df=pd.DataFrame(yale_df_list,index=yale_list).T  #yale confidence data

    yale_df.index=yale_df.index+datetime.timedelta(61)

    employ_df=quandl.get('FRED/NROUST', start_date='2010-1-1', end_date=str(datetime.datetime.today().date())) #short term unemployment]
    product_df=quandl.get('FRED/GDPPOT', start_date='2010-1-1', end_date=str(datetime.datetime.today().date())) #nominal gross domestic product

    fed_df=pd.concat([product_df,employ_df],axis=1)
    fed_df.columns=['FRED/NROUST','FRED/GDPPOT'] #federal reserve data

    infl_df=quandl.get('RATEINF/CPI_USA', start_date='2010-1-1', end_date=str(datetime.datetime.today().date()))
    infl_df.columns=['basket_price']
    infl_df['inflation_Q']=infl_df['basket_price'].pct_change()

    short_df=quandl.get('FINRA/FNSQ_'+'spy'.upper(), start_date='2010-1-1', end_date=str(datetime.datetime.today().date()))['ShortVolume']
    short_df.columns=['short_int']

    spy_df=yahooTA('spy','1d')

    index_list=['vox','vcr','vdc','vde','vfh','vht','vis','vgt','vaw','vnq','vpu','dbp','vglt','vgit','vgsh','eem','uso','pdbc','gld','vwo','lqd']

    volume_df=get_volume(index_list)

    sentiment_df=sentiment_df.dropna()
    to_concat=[short_df,gtrends_df,spy_df,volume_df] #returns_df,daily
    to_merge=[sentiment_df,infl_df,fed_df,employ_df,product_df,yale_df]  #yale_df,wkly
    
    combined_df=pd.concat(to_concat,axis=1)

    for df in to_merge:
      combined_df=pd.merge_asof(combined_df,df,right_index=True,left_index=True)
  
  else:

    combined_df=pd.read_csv('/content/drive/MyDrive/BinaryStocks/csv/common.csv')

    combined_df.index=pd.to_datetime(combined_df['Unnamed: 0'])

    combined_df=combined_df.drop('Unnamed: 0',axis=1)
  
  return combined_df

def get_growth_value(sentiment_df=sentiment_df,timeframe='1wk',shift=True):

  common_df=get_common(sentiment_df=sentiment_df)

  growth_df=pdr.get_data_yahoo('vigax',interval=timeframe)['Close']
  value_df=pdr.get_data_yahoo('vviax',interval=timeframe)['Close']

  daily_growth=pdr.get_data_yahoo('vigax',interval='1d')['Close'].iloc[-1600:]

  daily_value=pdr.get_data_yahoo('vviax',interval='1d')['Close'].iloc[-1600:]

  growth_prof=one_step_matprof(daily_growth,'growth')

  value_prof=one_step_matprof(daily_value,'value')

  comb_df=pd.concat([growth_df.pct_change(),value_df.pct_change()],axis=1)

  comb_df.columns=['growth','value']

  greater=comb_df[comb_df['growth']>comb_df['value']].index

  lower=comb_df[comb_df['growth']<comb_df['value']].index

  comb_df['labels']=-1

  comb_df['labels'].loc[lower]=0

  comb_df['labels'].loc[greater]=1

  comb_df['labels']=comb_df['labels'].replace(to_replace=-1,method='bfill')

  vix_series=pdr.get_data_yahoo('^vix',interval='1d')['Close']

  vix_df=pd.DataFrame(vix_series)

  vix_df.columns=['vix_close']

  common_df=pd.concat([common_df,vix_df,growth_prof,value_prof],axis=1)

  if shift:
    comb_df['targets']=comb_df['labels'].shift(-1)
          
    if timeframe=='1wk':
    
      combined_df=pd.merge_asof(common_df,comb_df,right_index=True,left_index=True)

    else:
      combined_df=pd.concat([common_df,comb_df],axis=1)
  
  else:
    comb_df['targets']=comb_df['labels']

    if timeframe=='1wk':
      combined_df=pd.merge_asof(common_df,comb_df[['targets','labels']],right_index=True,left_index=True)

      combined_df['targets']=combined_df['targets'].shift(-2)
      
    else:
      combined_df=pd.concat([common_df,comb_df[['targets','labels']]],axis=1)
    
  combined_df=combined_df.astype(float)

  targets=combined_df['targets'].fillna(method='ffill') 

  combined_df=combined_df.drop(['labels','targets'],axis=1).fillna(-1)

  return combined_df.loc['2016':],targets.loc['2016':].astype(int)

def get_backtest(timeframe='1wk',strategy='hrp'):

  common_df=get_common(sentiment_df=sentiment_df)

  spy_close=pdr.get_data_yahoo('spy',interval=timeframe)['Close'].iloc[-1800:]

  spy_prof=one_step_matprof(pdr.get_data_yahoo('spy',interval='1d')['Close'].iloc[-1600:],'spy')

  vix_series=pdr.get_data_yahoo('^vix',interval='1d')['Close']

  vix_df=pd.DataFrame(vix_series)

  vix_df.columns=['vix_close']

  common_df=pd.concat([common_df,vix_df,spy_prof],axis=1)

  backtest_df=pd.read_csv('/content/drive/MyDrive/BinaryStocks/csv/algo_backtest.csv')

  backtest_df.index=pd.to_datetime(backtest_df['Date'])
  backtest_df=backtest_df.drop('Date',axis=1)

  others=['bl2','hrp','bl2_long','hrp_long','bl1','bl1_long']

  others.remove(strategy)

  backtest_df=backtest_df.drop(others,axis=1)
  
  if timeframe=='1wk':

    backtest_df=backtest_df.resample('W-Mon', label='left', closed = 'left').mean()

    backtest_df['targets']=0
    backtest_df['targets'][backtest_df[strategy]>0]=1

    combined_df=pd.merge_asof(common_df,backtest_df,right_index=True,left_index=True)
    
  
  else:

    backtest_df['targets']=0
    backtest_df['targets'][backtest_df[strategy]>0]=1

    combined_df=pd.concat([common_df,backtest_df],axis=1)

  combined_df=combined_df.astype(float)

  targets=combined_df['targets'].fillna(method='ffill') 

  combined_df=combined_df.drop(['targets']+[strategy],axis=1).fillna(-1)
  
  return combined_df.loc['2016':],targets.loc['2016':].astype(int)

def get_compare_backtest(timeframe='1wk',strategy1='hrp',strategy2='bl1'):

  common_df=get_common(sentiment_df=sentiment_df)

  spy_close=pdr.get_data_yahoo('spy',interval=timeframe)['Close'].iloc[-1800:]

  spy_prof=one_step_matprof(pdr.get_data_yahoo('spy',interval='1d')['Close'].iloc[-1600:],'spy')

  vix_series=pdr.get_data_yahoo('^vix',interval='1d')['Close']

  vix_df=pd.DataFrame(vix_series)

  vix_df.columns=['vix_close']

  common_df=pd.concat([common_df,vix_df,spy_prof],axis=1)

  backtest_df=pd.read_csv('/content/drive/MyDrive/BinaryStocks/csv/algo_backtest.csv')

  backtest_df.index=pd.to_datetime(backtest_df['Date'])
  backtest_df=backtest_df.drop('Date',axis=1)

  others=['bl2','hrp','bl2_long','hrp_long','bl1','bl1_long']

  others.remove(strategy1)
  others.remove(strategy2)

  backtest_df=backtest_df.drop(others,axis=1)
  
  if timeframe=='1wk':

    backtest_df=backtest_df.resample('W-Mon', label='left', closed = 'left').mean()

    backtest_df['targets']=0
    backtest_df['targets'][backtest_df[strategy1]>backtest_df[strategy2]]=1

    combined_df=pd.merge_asof(common_df,backtest_df,right_index=True,left_index=True)
    
  
  else:

    backtest_df['targets']=0
    backtest_df['targets'][backtest_df[strategy1]>backtest_df[strategy2]]=1

    combined_df=pd.concat([common_df,backtest_df],axis=1)

    

  combined_df=combined_df.astype(float)

  targets=combined_df['targets'].fillna(method='ffill') 

  combined_df=combined_df.drop(['targets']+[strategy1,strategy2],axis=1).fillna(-1)
  
  return combined_df.loc['2016':],targets.loc['2016':].astype(int)

def get_triple(timeframe='1d',negative=False,multi_class=False,n=5):

  common_df=get_common(sentiment_df=sentiment_df)
  
  spy_close=pdr.get_data_yahoo('spy',interval=timeframe)['Close'].iloc[-1800:]

  spy_prof=one_step_matprof(pdr.get_data_yahoo('spy',interval='1d')['Close'].iloc[-1600:],'spy')

  tb_labels=tbl(spy_close,n)

  if negative and not multi_class:

    label_idxs=tb_labels.index[tb_labels['Label']!=-1]
    tb_labels=tb_labels.replace(-1,1)
  
  elif not negative and not multi_class:
    label_idxs=tb_labels.index[tb_labels['Label']!=1]

  else:
    label_idxs=tb_labels.index[tb_labels['Label'].isnull()]
  
  tb_labels.loc[label_idxs]=0

  tb_labels.columns=['targets']
  
  vix_series=pdr.get_data_yahoo('^vix',interval='1d')['Close']

  vix_df=pd.DataFrame(vix_series)

  vix_df.columns=['vix_close']

  common_df=pd.concat([common_df,vix_df,spy_prof],axis=1)
                      
  if timeframe=='1wk':
  
    combined_df=pd.merge_asof(common_df,tb_labels,right_index=True,left_index=True)
  
  else:
    combined_df=pd.concat([common_df,tb_labels],axis=1)

  combined_df=combined_df.astype(float)

  targets=combined_df['targets'].fillna(method='ffill') 

  combined_df=combined_df.drop(['targets'],axis=1).fillna(-1)
  
  return combined_df.loc['2016':],targets.loc['2016':].astype(int)

def get_index(sentiment_df=sentiment_df,timeframe='1wk',tridirectional=False,pos_label=True,index='spy',shift=True):

  common_df=get_common(sentiment_df)

  vix_series=pdr.get_data_yahoo('^vix',interval='1d')['Close']

  vix_df=pd.DataFrame(vix_series)

  spy_df=pdr.get_data_yahoo(index,interval=timeframe)[['Close','Volume']]

  spy_prof=one_step_matprof(pdr.get_data_yahoo(index,interval='1d')['Close'].iloc[-1600:],index)

  vix_df.columns=['vix_close']

  spy_df['labels']=0
  
  if not tridirectional:

    if pos_label:
      spy_df['labels'][spy_df['Close'].pct_change()>0]=1
    else:
      spy_df['labels'][spy_df['Close'].pct_change()<0]=1
    
  else:
    spy_df['labels'][spy_df['Close'].pct_change()>0]=1
    spy_df['labels'][spy_df['Close'].pct_change()<0]=-1

    
  if shift:  
    spy_df['targets']=spy_df['labels'].shift(-1)
    if timeframe=='1d':
      
      to_concat=[vix_df,common_df,spy_prof,spy_df]
      combined_df=pd.concat(to_concat,axis=1)

    else:
     
      to_concat=[vix_df,common_df,spy_prof]
      
      combined_df=pd.concat(to_concat,axis=1)

      combined_df=pd.merge_asof(combined_df,spy_df,right_index=True,left_index=True)
      
  
  else:
     spy_df['targets']=spy_df['labels']
     if timeframe=='1d':
        
        to_concat=[vix_df,common_df,spy_df['targets','labels']]
        combined_df=pd.concat(to_concat,axis=1)
        combined_df=combined_df.drop(['close','close_returns'],axis=1)

     else:
      
        to_concat=[vix_df,common_df]
        
        combined_df=pd.concat(to_concat,axis=1)

        combined_df=pd.merge_asof(combined_df,spy_df[['targets','labels']],right_index=True,left_index=True)

        combined_df['targets']=combined_df['targets'].shift(-2)
        
  combined_df=combined_df.astype(float)

  targets=combined_df['targets'].fillna(method='ffill')

  combined_df=combined_df.drop(['labels','targets'],axis=1).fillna(-1)

  return combined_df.loc['2016':],targets.loc['2016':].astype(int)

def get_sector(sentiment_df=sentiment_df,timeframe='1wk',shift=True):

  common_df=get_common(sentiment_df)

  vix_series=pdr.get_data_yahoo('^vix',interval='1d')['Close']

  vix_df=pd.DataFrame(vix_series)

  vix_df.columns=['vix_close']

  close_diffs=load_diffs()

  returns=pd.concat([pdr.get_data_yahoo(ticker.split('_')[0],interval=timeframe)['Close'].pct_change() for ticker in close_diffs.columns],axis=1).loc['2016':]

  returns.columns=close_diffs.columns

  new_returns=returns.drop(returns.idxmax(axis=1,skipna=True).value_counts().index[(returns.idxmax(axis=1,skipna=True).value_counts()<12)],axis=1)

  coding=new_returns.idxmax(axis=1,skipna=True).factorize()[-1]

  new_returns['labels']=new_returns.idxmax(axis=1,skipna=True).factorize()[0]

  returns=new_returns.copy()

  returns=returns[returns.labels!=-1]

  if shift:
    targets=returns['labels'].shift(-1)
    targets.name='targets'
    if timeframe=='1d':
      
      to_concat=[vix_df,common_df,targets]
      combined_df=pd.concat(to_concat,axis=1)

    else:
      to_concat=[vix_df,common_df]
      
      combined_df=pd.concat(to_concat,axis=1)

      combined_df=pd.merge_asof(combined_df,targets,right_index=True,left_index=True)
  
  else:
    targets=returns['labels']
    targets.name='targets'

    if timeframe=='1d':
      
      to_concat=[vix_df,common_df,targets]
      combined_df=pd.concat(to_concat,axis=1)

    else:
      to_concat=[vix_df,common_df]
      
      combined_df=pd.concat(to_concat,axis=1)

      combined_df=pd.merge_asof(combined_df,targets,right_index=True,left_index=True) #shift without labels

      combined_df['targets']=combined_df['targets'].shift(-3)

  combined_df=combined_df.astype(float)

  targets=combined_df['targets'].fillna(method='ffill')

  combined_df=combined_df.drop(['targets'],axis=1).fillna(-1)

  combined_df['targets']=targets

  combined_df=combined_df.dropna()

  return combined_df.loc['2016':].drop('targets',axis=1),combined_df['targets'].loc['2016':].astype(int)

def get_atr_now(sentiment_df=sentiment_df):

  timeframe='1wk'

  common_df=get_common(sentiment_df)

  vix_series=pdr.get_data_yahoo('^vix',interval='1d')['Close']

  vix_df=pd.DataFrame(vix_series)

  vix_df.columns=['vix_close']

  qqq_df=pdr.get_data_yahoo('qqq',interval=timeframe)#[['Close','Volume']]

  qqq_df['atr_change']=pandas_ta.atr(qqq_df['High'],qqq_df['Low'],qqq_df['Close']).pct_change()

  qqq_df=qqq_df[['Close','Volume','atr_change']]

  #qqq_prof=one_step_matprof(pdr.get_data_yahoo(index,interval='1d')['Close'].iloc[-1600:],index)

  qqq_df['labels']=0

  qqq_df['labels'][qqq_df['atr_change']>0]=1
    
  qqq_df['labels'][qqq_df['atr_change']<=0]=-1

  qqq_df['targets']=qqq_df['labels'].copy()
  
  to_concat=[vix_df,common_df]
  
  combined_df=pd.concat(to_concat,axis=1)

  combined_df=pd.merge_asof(combined_df,qqq_df[['targets','labels']],right_index=True,left_index=True) 

  combined_df['targets']=combined_df['targets'].shift(-3)

  combined_df=combined_df.astype(float)

  targets=combined_df['targets'].fillna(method='ffill')

  combined_df=combined_df.drop(['targets','labels'],axis=1).fillna(-1)

  combined_df['targets']=targets

  combined_df=combined_df.dropna()

  return combined_df.loc['2016':].drop('targets',axis=1),combined_df['targets'].loc['2016':].astype(int)

def get_volatility(sentiment_df=sentiment_df,timeframe='1wk',bidirectional=False,shift=True):

  common_df=get_common(sentiment_df)

  vix_series=pdr.get_data_yahoo('^vix',interval=timeframe)['Close']

  vix_df=pd.DataFrame(vix_series)

  spy_prof=one_step_matprof(pdr.get_data_yahoo('spy',interval='1d')['Close'].iloc[-1600:],'spy')

  vix_df.columns=['vix_close']
                      
  vix_df['labels']=0
  vix_std=abs(vix_df['vix_close'].diff()).std()

  if not bidirectional:

    #vix_df['labels'][abs(vix_df['vix_close'].diff())>vix_std]=1  #abs
    vix_df['labels'][(vix_df['vix_close'].diff())>vix_std]=1

  else:

    vix_df['labels'][(vix_df['vix_close'].diff())>vix_std]=1  
    vix_df['labels'][vix_df['vix_close'].diff()<-vix_std]=-1

  if shift:
    vix_df['targets']=vix_df['labels'].shift(-1)
    
    if timeframe=='1d':

      combined_df=pd.concat([vix_df,common_df,spy_prof],axis=1)

    else:
      combined_df=pd.merge_asof(common_df,vix_df,right_index=True,left_index=True)
  
  else:
    vix_df['targets']=vix_df['labels']

    if timeframe=='1d':

      combined_df=pd.concat([vix_df[['targets','labels']],common_df,spy_prof],axis=1)

    else:
      combined_df=pd.merge_asof(common_df,vix_df[['targets','labels']],right_index=True,left_index=True)
    
  #combined_df=combined_df.fillna(method='pad').dropna()

  combined_df=combined_df.replace({np.inf: np.nan, -np.inf:np.nan}).astype(float)

  targets=combined_df['targets'].fillna(method='ffill')

  print(targets.unique())

  combined_df=combined_df.drop(['labels','targets'],axis=1).fillna(-1)

  return combined_df.loc['2016':],targets.loc['2016':].astype(int)

#create algo to predict from friday

"""####Feature Engineering"""

from sklearn.preprocessing import MinMaxScaler

def scaled_variance_vars(ts_train_data,threshold=0.03):

  scaler=MinMaxScaler()

  scaled_train=scaler.fit_transform(ts_train_data)

  scaled_df=pd.DataFrame(scaled_train,columns=ts_train_data.columns,index=ts_train_data.index)

  return list(scaled_df.var()[scaled_df.var()>=threshold].index)

def match_ts_targets(ts,targets):
  
  ts_copy=ts.copy()
  ts_copy['targets']=targets

  return ts_copy['targets']

def outlier_features(df,ts_train_data):
  if isinstance(df,pd.Series):
      df=pd.DataFrame(df)

  iso=IsolationForest()

  iso.fit(ts_train_data)

  iso_values=iso.predict(df)
  
  return pd.DataFrame(iso_values,index=df.index,columns=['outliers'])

def add_date_features(df):

  date_df=pd.DataFrame(index=df.index)

  if 'date' not in df.columns:
    date_df['date']=df.index

  date_df.loc[:, 'year'] = date_df['date'].dt.year
  date_df.loc[:, 'weekofyear'] = date_df['date'].dt.weekofyear
  date_df.loc[:, 'month'] = date_df['date'].dt.month
  date_df.loc[:, 'dayofweek'] = date_df['date'].dt.dayofweek
  date_df.loc[:, 'weekend'] = (date_df['date'].dt.weekday >=5).astype(int)

  return date_df.drop('date',axis=1)

def agg_features_mean(df,feature):
 # create a bunch of features using the date column

  # create an aggregate dictionary
  aggs = {}

  
  num_cols=[col for col in df.columns if col]
  
  for col in num_cols:
    
    aggs[col] = ['mean']

  # for customer_id, we calculate the total count
  #aggs['feature'] = ['size']
  # again for customer_id, we calculate the total unique

  # we group by customer_id and calculate the aggregates
  agg_df = df.groupby(feature).agg(aggs)
  agg_df = agg_df.reset_index()
  
  return agg_df

def agg_defined_means(df):

  df=df.copy()

  orig_idx=df.index

  agg_columns=shapiro_normality_test(df[df!=-1])

  for col in agg_columns:
    aggs_df=agg_features_mean(df[agg_columns],col)

    aggs_df.columns=[col+'_'+x+'_'+y if y!='' else col for x,y in aggs_df.columns]

    df=pd.merge(df,aggs_df,on=col,how='left')
  
  df.index=orig_idx

  return df
      
class Time2Vec(nn.Module):
  def __init__(self, activation,hiddem_dim,in_features):
      super().__init__()
      self.in_features=in_features
      if activation == "sin":
          self.l1 = SineActivation(self.in_features, hiddem_dim)
      elif activation == "cos":
          self.l1 = CosineActivation(self.in_features, hiddem_dim)
      
      self.fc1 = nn.Linear(hiddem_dim, 2)
  
  def forward(self, x):
      #x = x.unsqueeze(1)
      x = self.l1(x)
      x = self.fc1(x)
      return x

def t2v(tau, f, out_features, w, b, w0, b0, arg=None):
    if arg:
        v1 = f(torch.matmul(tau, w) + b, arg)
    else:
        
        v1 = f(torch.matmul(tau, w) + b)
    v2 = torch.matmul(tau, w0) + b0
   
    return torch.cat([v1, v2], 1)

class SineActivation(nn.Module):
    def __init__(self, in_features, out_features):
        super(SineActivation, self).__init__()
        self.out_features = out_features
        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))
        self.b0 = nn.parameter.Parameter(torch.randn(1, 1))
        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))
        self.b = nn.parameter.Parameter(torch.randn(1, out_features-1))
        self.f = torch.sin

    def forward(self, tau):
        return t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)

def t2v_func(train_data,test_data):

  data=train_data.copy()

  if len(data.shape) >1:
    time2vec=Time2Vec(activation='sin',hiddem_dim=100,in_features=data.shape[-1])

    time2vec.train()
    t2v_train=time2vec(torch.Tensor(data.values))

    time2vec.eval()

    t2v_test=time2vec(torch.Tensor(test_data.values))

  else:
    data=np.reshape(data.values,(data.shape[0],1))
    test_data=np.reshape(test_data.values,(data.shape[0],1))
    
    time2vec=Time2Vec(activation='sin',hiddem_dim=100,in_features=data.shape[-1])
    
    time2vec.train()
    t2v_train=time2vec(torch.Tensor(data))

    time2vec.eval()

    t2v_test=time2vec(torch.Tensor(test_data))

  return pd.DataFrame(t2v_train.detach().numpy(),columns=['t2v_0','t2v_1'],index=data.index),pd.DataFrame(t2v_test.detach().numpy(),columns=['t2v_0','t2v_1'],index=test_data.index)
     
def ts_2_supervised(data, n_in=1, n_out=0, dropnan=True):

  n_vars = 1 if type(data) is list else data.shape[1]
  df = pd.DataFrame(data)
  cols, names = list(), list()
  # input sequence (t-n, ... t-1)
  for i in range(n_in, 0, -1):
    cols.append(df.shift(i))
    names += [col+('(t-%d)' % (i)) for col in data.columns]
  # forecast sequence (t, t+1, ... t+n)
  for i in range(0, n_out):
    cols.append(df.shift(-i))
    if i == 0:
      names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
    else:
      names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
  # put it all together
  agg = pd.concat(cols, axis=1)
  agg.columns = names
  # drop rows with NaN values
  if dropnan:
    agg.dropna(inplace=True)
  return agg

def stationarity_tester(series,threshold=0.01,print_mode=False):
  
  p_value=adfuller(series)[1]

  if print_mode:
    if series.name:
     print(series.name+' is')
    
    if p_value<threshold:
      print('Stationary')
    
    else:
      print('not Stationary')
  
  return p_value

def non_stationary_vars(combined_df):
  col_lst=[]

  for col in combined_df.columns:
    p_value=stationarity_tester(combined_df[col],threshold=0.01)
    if p_value>0.01:
      col_lst.append(col)
  
  return col_lst

#feature engineering

def ts_engineer_splits(combined_df,targets):

  ts_train_val_data,ts_test_data=ts_split_data(combined_df,targets)

  ts_train_data,ts_val_data=ts_split_data(ts_train_val_data,test_size=0.2)

  ts_train_data=ts_train_data.drop('targets',axis=1)

  ts_val_data=ts_val_data.drop('targets',axis=1)

  return ts_train_data,ts_val_data

from sklearn.preprocessing import MinMaxScaler

def scaled_variance_vars(ts_train_data,threshold=0.03):

  scaler=MinMaxScaler()

  scaled_train=scaler.fit_transform(ts_train_data)

  scaled_df=pd.DataFrame(scaled_train,columns=ts_train_data.columns,index=ts_train_data.index)

  return list(scaled_df.var()[scaled_df.var()>=threshold].index)

def ts_single_count(df,col):
  df=df.copy()
  df[col+'_count']=0

  for t in range(len(df)):
    relevant=df[col].iloc[:t+1]
    
    temp = relevant.value_counts()

    temp_item=temp[temp.index==relevant[-1]].item()

    df[col+'_count'].iloc[t]=temp_item
  
  return df[col+'_count']

def engineer_splits(df,targets,ts_train_data):

  if 'targets' in df.columns:
    print('target in df')

  proto=df.dropna().copy()

  #aggs_df=agg_defined_means(proto)

  diff_df=load_diffs()

  super_df=ts_2_supervised(proto,n_in=10,n_out=0,dropnan=False)

  date_df=add_date_features(proto)

  #t2v_df=pd.concat([t2v_func(proto.iloc[:t+1]).iloc[-1,:] for t in range(len(proto))],axis=1).T
  
  iso_df=outlier_features(proto,ts_train_data)

  to_concat=[date_df,iso_df,diff_df,super_df,proto]  #t2v_df

  feat_df=pd.concat(to_concat,axis=1).T.drop_duplicates().T

  targets=match_ts_targets(feat_df,targets)

  _,t2v_df=t2v_func(feat_df.loc[ts_train_data.index],feat_df)

  feat_df=pd.concat([feat_df,t2v_df],axis=1)

  df=feat_df.copy()

  df['targets']=targets.copy()

  targets=df['targets'].fillna(method='ffill').astype('int')
  
  df=df.T.drop_duplicates().T.fillna(-1)
  
  train_val_data,test_data=split_data(df,targets)

  train_data,val_data=split_data(train_val_data)
  
  return train_data,val_data,train_val_data,test_data,df

"""###Feature Selection"""

def feat_experience(model,df,threshold=0.75,target_col='targets',targets=None):

  df=df.copy()

  if targets is not None:
    df['targets']=targets
  
  x=df.drop('targets',axis=1).T.drop_duplicates().T
  y=df.targets

  base_lst=[]
  new_lst=[]
  kf=StratifiedKFold(n_splits=3,shuffle=True,random_state=69)

  print('cv for stat analysis')
  for fold, (t_,v_) in tqdm_notebook(enumerate(kf.split(X=x,y=y))):
    xtrain=x.iloc[t_,:].values
    
    ytrain=y.iloc[t_].values
    
    xval=x.iloc[v_,:].values
    yval=y.iloc[v_].values
  
    model.fit(xtrain,ytrain)

    if len(model.classes_)>2:
      probs=model.predict_proba(xval)
    
    else:

      probs=model.predict_proba(xval)[:,-1]

    baseline_score=sklearn.metrics.roc_auc_score(yval,probs,multi_class='ovr')

    base_lst.append(baseline_score)

    train_data=pd.concat([pd.DataFrame(xtrain),pd.Series(ytrain)],axis=1)#.T.drop_duplicates().T

    cols=list(x.columns)
    cols.append(y.name)

    train_data.columns=cols

    test_data=pd.concat([pd.DataFrame(xval),pd.Series(yval)],axis=1)
    test_data.columns=cols

    trend_df=get_trend_stats(data=train_data,data_test=test_data,target_col=target_col)

    good_feats=list(trend_df['Feature'][abs(trend_df['Trend_correlation'])>threshold])
   
    good_xtrain=x[good_feats].iloc[t_,:]
    good_xval=x[good_feats].iloc[v_,:]

    model.fit(good_xtrain.values,ytrain)

    if len(model.classes_)>2:
      probs=model.predict_proba(good_xval)
      new_score=sklearn.metrics.roc_auc_score(yval,probs,multi_class='ovr')
    
    else:
      probs=model.predict_proba(good_xval)[:,-1]

      new_score=sklearn.metrics.roc_auc_score(yval,probs)

    new_lst.append(new_score)
  
  mean_new_score=np.mean(new_lst)
  mean_base_score=np.mean(base_lst)
  
  if mean_new_score>mean_base_score:
    return list(set(df.columns)-set(good_feats))
  
  else:
    print('futile')
    return []

def nested_feature_selection(df,model,targets=None):
  
  df=df.copy()

  if targets is not None:
    df['targets']=targets
  
  features=df.drop('targets',axis=1).columns
  
  folds_df=create_folds(df)
  
  prime_train_list=[]
  prime_val_list=[]

  print('nested cv for feature selection')
  for fold in tqdm_notebook(folds_df['kfold'].unique()): #outer cv

    df_train_outer = folds_df[folds_df.kfold != fold].reset_index(drop=True)
 
    df_valid_outer = folds_df[folds_df.kfold == fold].reset_index(drop=True)
    
    df_inner= create_folds(df_train_outer)

    val_scores_list=[]
    train_scores_list=[]
    
    print('doing inner folds')
    for inner_fold in tqdm_notebook(df_inner['kfold'].unique()):
      df_train_inner= df_inner[df_inner.kfold != inner_fold].reset_index(drop=True)

      df_valid_inner=df_inner[df_inner.kfold == inner_fold].reset_index(drop=True)
      
      val_series=pd.Series()
      train_series=pd.Series()
      for col in features:
        xtrain_inner=np.expand_dims(df_train_inner[col].values,axis=1)
        ytrain_inner=df_train_inner.targets.values

        xval_inner=np.expand_dims(df_valid_inner[col].values,axis=1)
        yval_inner=df_valid_inner.targets.values
    
        model.fit(xtrain_inner,ytrain_inner)

        if len(model.classes_)>2:
          val_probs=model.predict_proba(xval_inner)

          train_probs=model.predict_proba(xtrain_inner)


          val_score=sklearn.metrics.roc_auc_score(yval_inner,val_probs,multi_class='ovr')

          train_score=sklearn.metrics.roc_auc_score(ytrain_inner,train_probs,multi_class='ovr')
        
        else:

          val_probs=model.predict_proba(xval_inner)[:,-1]

          train_probs=model.predict_proba(xtrain_inner)[:,-1]

          val_score=sklearn.metrics.roc_auc_score(yval_inner,val_probs)

          train_score=sklearn.metrics.roc_auc_score(ytrain_inner,train_probs)
      
        val_series.loc[col]=val_score
        train_series.loc[col]=train_score

      val_scores_list.append(val_series)
      train_scores_list.append(train_series)
  
    prime_train_list.append(pd.concat(train_scores_list,axis=1))
    
    prime_val_list.append(pd.concat(val_scores_list,axis=1))
  
  new_train_list=[i.T.mean() for i in prime_train_list]
  new_val_list=[i.T.mean() for i in prime_val_list]

  train_feat_df=sum(new_train_list)/len(new_train_list)

  val_feat_df=sum(new_val_list)/len(new_val_list)

  comb_df=pd.concat([train_feat_df,val_feat_df],axis=1)

  bad_list=[]

  for var in comb_df.T:
    if comb_df.T[var].iloc[0]<0.5 or comb_df.T[var].iloc[-1]<0.5:
      bad_list.append(var)
  
  return bad_list

def adversarial_validation(train_val_data,model):

  df=train_val_data.copy()

  df['targets']=-1

  x=df.drop('targets',axis=1).reset_index(drop=True)
  y=df.targets.reset_index(drop=True)

  xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.25, random_state=69, stratify=y)

  ytrain=pd.DataFrame(np.zeros(ytrain.shape),columns=['targets'],index=ytrain.index).astype(int)

  ytest=pd.DataFrame(np.ones(ytest.shape),columns=['targets'],index=ytest.index).astype(int)

  new_train=pd.concat([xtrain,ytrain],axis=1)

  new_test=pd.concat([xtest,ytest],axis=1)

  new_data=pd.concat([new_train,new_test])

  new_x=new_data.drop('targets',axis=1).values

  new_y=new_data.targets.values

  kf=StratifiedKFold(n_splits=3,shuffle=True,random_state=69)

  score_lst=[]

  print('cv for adversarial feature selection')
  for fold, (t_,v_) in tqdm_notebook(enumerate(kf.split(X=new_x,y=new_y))):
    xtrain=new_x[t_,:]
    ytrain=new_y[t_]
    
    xval=new_x[v_,:]
    yval=new_y[v_]

    model.fit(xtrain,ytrain)

    if len(model.classes_)>2:
      probs=model.predict_proba(xval)

      score=sklearn.metrics.roc_auc_score(yval,probs,multi_class='ovr')
    
    else:

      probs=model.predict_proba(xval)[:,-1]

      score=sklearn.metrics.roc_auc_score(yval,probs)

    score_lst.append(score)
  
  if np.mean(score_lst)>0.6:
    xtrain,xtest,ytrain,ytest=train_test_split(new_x,new_y,random_state=69,stratify=new_y)
    final_model=model.fit(xtrain,ytrain)

    return final_model,np.mean(score_lst)

  else:
    return None,np.mean(score_lst)

def time_consistency(ts_df,model,targets=None):

  df=ts_df.copy()

  if targets is not None:
    df['targets']=targets

  train_data=df.iloc[:round(0.7*len(df)),:]

  first_data=train_data.iloc[:30]

  last_data=train_data.iloc[-30:]

  bad_lst=[]
  
  print('Time consistency analysis for each feature')
  for col in tqdm_notebook(train_data.columns):
    
    first_x=np.expand_dims(first_data[col].values,axis=1)
    first_y=np.expand_dims(first_data.targets.values,axis=1)

    if len(np.unique(first_y))<2:
      return []

    last_x=np.expand_dims(last_data[col].values,axis=1)
    last_y=np.expand_dims(last_data.targets.values,axis=1)

    if len(np.unique(last_y))<2:
      return []

    model.fit(first_x,first_y)

    if len(model.classes_)>2:
      first_probs=model.predict_proba(first_y)

      last_probs=model.predict_proba(last_x)

      first_score=sklearn.metrics.roc_auc_score(first_y,first_probs,multi_class='ovr')

      last_score=sklearn.metrics.roc_auc_score(last_y,last_probs,multi_class='ovr')

    else:

      first_probs=model.predict_proba(first_y)[:,-1]

      last_probs=model.predict_proba(last_x)[:,-1]

      first_score=sklearn.metrics.roc_auc_score(first_y,first_probs)

      last_score=sklearn.metrics.roc_auc_score(last_y,last_probs)

    if first_score <0.5 or last_score<0.5:
      bad_lst.append(col)

  return bad_lst

"""###Automl"""

def run_nested_flaml(df,targets=None,seconds=300,estimator_list=[]):
  
  df=df.copy()

  if targets is not None:
    df['targets']=targets
  
  folds_df=create_folds(df)

  model_lst=[]

  for fold in tqdm_notebook(folds_df['kfold'].unique()): #outer cv

    df_train_outer = folds_df[folds_df.kfold != fold].reset_index(drop=True)

    df_valid_outer = folds_df[folds_df.kfold == fold].reset_index(drop=True)

    xtrain=df_train_outer.drop(['targets','kfold'],axis=1).values
    ytrain=df_train_outer.targets.values
  
    automl=AutoML()

    if len(targets.unique())>2:
      metric='roc_auc_ovr'
  
    else:
      metric='ap'

    if estimator_list==[]:
      automl_settings = {
          "time_budget": seconds,  # in seconds
          "metric": metric,
          "task": 'classification',
          'eval_method': 'cv'
      }
    
    else:
      automl_settings = {
          "time_budget": seconds,  # in seconds
          "metric": metric,
          "task": 'classification',
          'estimator_list': estimator_list,
          'eval_method': 'cv'
      }
    
    automl.fit(xtrain,ytrain,**automl_settings)

    model_lst.append(automl.model.estimator)
    
  return cv_eval_models(df,model_lst,target_metric)

def run_nested_tpot(df,targets=None):
  
  df=df.copy()

  if targets is not None:
    df['targets']=targets
  
  folds_df=create_folds(df)

  model_lst=[]

  for fold in tqdm_notebook(folds_df['kfold'].unique()): #outer cv

    df_train_outer = folds_df[folds_df.kfold != fold].reset_index(drop=True)

    df_valid_outer = folds_df[folds_df.kfold == fold].reset_index(drop=True)

    xtrain=df_train_outer.drop(['targets','kfold'],axis=1).values
    ytrain=df_train_outer.targets.values
    
    automl=AutoML()

    cv = StratifiedKFold(n_splits=5, random_state=69,shuffle=True)

    tpot = TPOTClassifier(generations=5, population_size=10, verbosity=2, random_state=69,cv=cv,scoring='average_precision')

    tpot.fit(xtrain, ytrain)

    model_lst.append(tpot.fitted_pipeline_)

  return cv_eval_models(model_lst,target_metric)
        
global target_metric
target_metric='average_precision'

def cv_eval_models(df,model_lst,target_metric):

  model_scores=[]
  for model in model_lst:
    fold_lst=run_folds(df,model,pref_metric='precision')
    
    fold_scores=[]
    for fold in fold_lst:
      fold_scores.append(fold_lst[fold]['scores'][target_metric])    

    model_scores.append(np.mean(fold_scores))

    return model_lst[np.argmax(model_scores)]

def end2end(combined_df,targets,seconds=300,estimator_list=[],metric=None):

  ts_train_data,ts_val_data=ts_engineer_splits(combined_df,targets)

  train_data,val_data,train_val_data,test_data,feat_df= engineer_splits(combined_df,targets,ts_train_data)

  ts_targets=match_ts_targets(ts_train_data,targets)

  automl = AutoML()

  if len(targets.unique())>2 and metric is None:
    metric='roc_auc_ovr'
  
  elif metric is None and len(targets.unique())<=2:
    metric='ap'
  
  if estimator_list == []:
    automl_settings = {
        "time_budget": 150,  # in seconds
        "metric": metric, #'',
        'eval_method': 'cv',
        "task": 'classification',
    }
  
  else:
    automl_settings = {
    "time_budget": 150,  # in seconds
    "metric": metric, #'',
    "task": 'classification',
    'eval_method': 'cv',
    'estimator_list': estimator_list}
  
  print(automl_settings)

  automl.fit(
      X_train=train_data.drop('targets',axis=1), 
      y_train=train_data.targets.astype('int'),
      **automl_settings
  )

  model=automl.model.estimator

  ts_targets=match_ts_targets(ts_train_data,targets)

  if len(model.classes_)<=2:
    inconsistent_lst=time_consistency(ts_train_data,model,ts_targets)
    adv_model,adv_score=adversarial_validation(train_val_data,model)

    if adv_score>0.6:
      print('check adv_model')
  
  else:
    inconsistent_lst=[]
  
  bad_trend_feats=feat_experience(model,train_data,threshold=0.5,targets=targets)

  nfs_lst=nested_feature_selection(train_val_data,model)

  feature_combos=[bad_trend_feats,inconsistent_lst+bad_trend_feats,inconsistent_lst,inconsistent_lst+nfs_lst+list(bad_trend_feats),list(bad_trend_feats)+nfs_lst,nfs_lst,inconsistent_lst+nfs_lst]

  score_lst=[]
  
  for combo in feature_combos:
    fold_lst=run_folds(train_val_data.drop(combo,axis=1),model=model,targets=targets,pref_metric='precision')

    prec_lst=[]
  
    for fold in fold_lst:
      prec_lst.append(fold_lst[fold]['scores']['average_precision'])
      
    score_lst.append(np.mean(prec_lst))
  
  to_drop=feature_combos[np.argmax(score_lst)]

  best_model=run_nested_flaml(train_val_data.drop(to_drop,axis=1),targets=targets,seconds=seconds,estimator_list=estimator_list)

  fold_lst=run_folds(train_val_data.drop(to_drop,axis=1),model=best_model,targets=targets,pref_metric='precision')
  
  thresh_lst=[]
  prec_lst=[]
  
  for fold in fold_lst:
    thresh_lst.append(fold_lst[fold]['scores']['precision_thresh'])
    prec_lst.append(fold_lst[fold]['scores']['average_precision'])
  
  return best_model,to_drop,np.mean(thresh_lst),np.mean(prec_lst)

def get_length(sentiment_df=sentiment_df,ticker='tsla',timeframe='1d'):
  
  common_df=get_common(sentiment_df)

  vix_series=pdr.get_data_yahoo('^vix',interval='1d')['Close']

  vix_df=pd.DataFrame(vix_series)

  vix_df.columns=['vix_close']

  hma10=yahooTA(ticker,timeframe)

  hma10.columns=[col+ticker for col in hma10.columns]

  peaks,_ =find_peaks(hma10['hma10'+ticker])
  valleys,_=find_peaks(-hma10['hma10'+ticker])

  locals=insort(peaks,valleys)

  diffs=np.diff(locals)

  hma10['labels']=0

  hma10['labels'][locals]=1

  hma10['targets']=0

  counter=0
  
  for i,j in enumerate(hma10['labels']):
      
    if j==1:
      counter+=1

    if counter<=len(diffs)-1:
      hma10['targets'][i]=diffs[counter]
  
  hma10=hma10[hma10['targets']!=0]      

  to_concat=[vix_df,common_df]

  combined_df=pd.concat(to_concat,axis=1)

  combined_df=pd.merge_asof(combined_df,hma10,right_index=True,left_index=True)
  
  combined_df=combined_df.astype(float)

  targets=combined_df['targets'].fillna(method='ffill')

  combined_df=combined_df.drop(['targets','labels'],axis=1).fillna(-1)

  combined_df['targets']=targets

  combined_df=combined_df.dropna()

  targets=combined_df['targets'].loc['2016':].astype(int)
  combined_df= combined_df.loc['2016':].drop('targets',axis=1)

  while targets.value_counts().min()<3:

    targets[targets==targets.value_counts().index[-1]]=targets.value_counts().index[-2]

  return combined_df,targets

def get_inflections(sentiment_df=sentiment_df,ticker='tsla',timeframe='1d',tridirectional=False):
  
  common_df=get_common(sentiment_df)

  vix_series=pdr.get_data_yahoo('^vix',interval='1d')['Close']

  vix_df=pd.DataFrame(vix_series)

  vix_df.columns=['vix_close']

  hma10=yahooTA(ticker,timeframe)

  hma10.columns=[col+ticker for col in hma10.columns]

  peaks,_ =find_peaks(hma10['hma10'+ticker])
  valleys,_=find_peaks(-hma10['hma10'+ticker])

  hma10['labels']=0

  if not tridirectional:

    locals=insort(peaks,valleys)

    hma10['labels'][locals]=1
  
  else:
    hma10['labels'][peaks]=1
    hma10['labels'][valleys]=-1
    
  to_concat=[vix_df,common_df]

  combined_df=pd.concat(to_concat,axis=1)

  combined_df=pd.merge_asof(combined_df,hma10,right_index=True,left_index=True)
  
  combined_df['targets']=combined_df['labels']

  combined_df=combined_df.astype(float)

  targets=combined_df['targets'].fillna(method='ffill')

  combined_df=combined_df.drop(['targets','labels'],axis=1).fillna(-1)

  combined_df['targets']=targets

  combined_df=combined_df.dropna()

  return combined_df.loc['2016':].drop('targets',axis=1),combined_df['targets'].loc['2016':].astype(int)

#get_atr_now, get_lengths, try binning with get_lengths


#combined_df,targets=get_atr_now()

#combined_df,targets=get_inflections(ticker='spy',timeframe='1d')
#combined_df,targets=get_length(ticker='qqq',timeframe='1wk')

combined_df,targets=get_index(index='qqq',tridirectional=True)

box=end2end(combined_df,targets,seconds=150,estimator_list=['lgbm'])

pickle_jar(box[0],box[1],box[2],label='weekly_spy_trinary')

def pickle_jar(model,columns,threshold,label='something'):

  jar={}

  jar['model']=model

  jar['columns']=columns

  jar['threshold']=threshold

  pickle_dict=pickler(label,jar)

def pickler(path,to_save=None):

  if to_save is not None:
    with open('/content/drive/MyDrive/BinaryStocks/{path}'.format(path=path), 'wb') as handle:
      pickle.dump(to_save, handle)

  else:
    with open('/content/drive/MyDrive/BinaryStocks/{path}'.format(path=path), 'rb') as handle:
      pickle_dict=pickle.load(handle)

    return pickle_dict

def matters(model,test_data):

  probs=model.predict_proba(test_data.drop('targets',axis=1))[:,-1]
  
  preds=model.predict(test_data.drop('targets',axis=1))

  return sklearn.metrics.precision_score(test_data['targets'],preds,average='macro')  
  #return sklearn.metrics.average_precision_score(test_data['targets'],preds)

  #return sklearn.metrics.fbeta_score(test_data['targets'],preds,beta=0.5,average='macro')

"""####Run Cross Validation

"""

def create_folds(df,targets=None,n_folds=5):

  df=df.copy()
  
  if targets is not None:
    df['targets']=targets

  n_folds=max(min(df.targets.value_counts().min()-1,n_folds),2)
    
  df["kfold"] = -1
 
  df = df.sample(frac=1).reset_index(drop=True)
 
  y = df.targets.values
 
  kf=StratifiedKFold(n_splits=n_folds,random_state=
  69,shuffle=True)

  for f, (tr_, v_) in enumerate(kf.split(X=df, y=y)):
    df.loc[v_, 'kfold'] = f
    
  return df

def run_fold(df,model,fold):
  df_train = df[df.kfold != fold].reset_index(drop=True)

  df_valid = df[df.kfold == fold].reset_index(drop=True)

  xtrain=df_train.drop(['targets','kfold'],axis=1).values
  ytrain=df_train.targets.values

  
  xval=df_valid.drop(['targets','kfold'],axis=1).values
  yval=df_valid.targets.values

  model.fit(xtrain,ytrain)

  return model,(xval,yval)

def run_folds(df,model,metric=None,targets=None,task='classification',pref_metric=None):

  print('model:',model)
  print('task:',task.upper())

  df=df.copy()

  if targets is not None:
    df['targets']=targets

  fold_df=create_folds(df)
  
  fold_dict={}

  if task=='classification':
    if pref_metric is None:
      print('Which one is most important? recall,precision,or neither?')
      pref_metric=input()

    for fold in fold_df['kfold'].unique():
      mini_dict={}
      scores_dict={}
      
      print('--'*10)
      
      model,(xval,yval)=run_fold(fold_df,model,fold)
      labels=model.classes_

      if len(df['targets'].unique())>2:
        
        probs= model.predict_proba(xval)

        thresh_df=threshold_matrix(yval,probs,labels)
        
        scores_dict['multi_brier_loss']=multi_brier_loss(yval,probs,labels)

        scores_dict['average_precision']=avg_precision(labels,probs,yval)
     
      elif len(df['targets'].unique())<=2:

        probs = model.predict_proba(xval)[:,-1]

        thresh_df=threshold_matrix(yval,probs,labels)
        
        scores_dict['brier_loss']=sklearn.metrics.brier_score_loss(yval,probs)
        
        scores_dict['average_precision']=sklearn.metrics.average_precision_score(yval,probs,average='macro')


      overall_dict=overall_class_metrics(thresh_df)

      pref_dict=pref_metrics(pref_metric,thresh_df)
      
      scores_dict['roc_auc']=sklearn.metrics.roc_auc_score(yval,probs,average='macro',multi_class='ovr')

      scores_dict={**scores_dict,**pref_dict,**overall_dict}
    
      for score in scores_dict:
        colon_print(score,scores_dict[score])
   
  #elif task=='regression':
    ###
    
      if metric is not None:
        print(str(metric)+':',metric(yval,probs))
      
      mini_dict['val']=(xval,yval,probs)
      mini_dict['scores']=scores_dict
      mini_dict['model']=model
        
      fold_dict[fold]=mini_dict

  return fold_dict

"""####Metrics"""

def multi_brier_loss(targets, probs,labels):
  pad_lst=[]

  for target in targets:
    pad_lst.append((labels==target).astype('int'))
  
  pad_targets=np.array(pad_lst)

  return np.mean(np.sum((probs - pad_targets)**2,axis=1))

def thresholder(probs,threshold,labels):
  
  filter=(probs>=threshold).astype('int')

  if len(filter.shape)>1:
    return labels[np.argmax(filter,axis=1)]
  
  else:
    return filter

def best_thresholds(thresh_df):

  recall_thresh=thresh_df.index[thresh_df['recall'].argmax()]
  precision_thresh=thresh_df.index[thresh_df['precision'].argmax()]
  f1_thresh=thresh_df.index[thresh_df['f1'].argmax()]
  fhalf_thresh=thresh_df.index[thresh_df['f0.5'].argmax()]
  f2_thresh=thresh_df.index[thresh_df['f2'].argmax()]

  return f1_thresh,fhalf_thresh,f2_thresh,recall_thresh,precision_thresh  

def best_class_scores(thresh_df):
  best_recall=thresh_df['recall'].max()
  best_precision=thresh_df['precision'].max()
  best_f1=thresh_df['f1'].max()
  best_f2=thresh_df['f2'].max()
  best_fhalf=thresh_df['f0.5'].max()

  return best_recall,best_precision,best_f1,best_f2,best_fhalf

def threshold_matrix(ytest,probs,labels):
  recall_lst=[] 
  precision_lst=[]
  mcc_lst=[]
  kappa_lst=[]

  for threshold in np.arange(0.5,1,0.05):

    preds=thresholder(probs,threshold,labels)

    cm=sklearn.metrics.confusion_matrix(ytest,preds,labels=labels) 
   
    tp = np.diag(cm)
    fp = np.sum(cm, axis=0) - tp
    fn = np.sum(cm, axis=1) - tp
    tn= cm.sum()-(tp+fp+fn)
    
    if len(labels)==2:
      tp=max(tp)
      fp=max(fp)
      fn=max(fn)
      tn=max(tn)
    
    precision=np.mean(tp/(tp+fp))
  
    recall=np.mean(tp/(tp+fn))
    
    mcc=sklearn.metrics.matthews_corrcoef(ytest,preds)

    try:
      kappa=sklearn.metrics.cohen_kappa_score(ytest,preds)
    except:
      kappa=np.nan
      pass
        
    recall_lst.append(recall)
    precision_lst.append(precision)
    mcc_lst.append(mcc)
    kappa_lst.append(kappa)

  thresh_df=pd.DataFrame(precision_lst,columns=['precision'],index=list(np.arange(0.5,1,0.05)))
  thresh_df['recall']=recall_lst
  thresh_df['mcc']=mcc_lst
  thresh_df['kappa']=kappa_lst

  thresh_df['f1']=(2*thresh_df['recall']*thresh_df['precision'])/(thresh_df['recall']+thresh_df['precision'])

  thresh_df['f0.5']=((1+0.5**2)*thresh_df['recall']*thresh_df['precision'])/((0.5**2)*thresh_df['precision']+thresh_df['recall'])

  thresh_df['f2']=((1+2**2)*thresh_df['recall']*thresh_df['precision'])/((2**2)*thresh_df['precision']+thresh_df['recall'])

  return thresh_df

def avg_precision(labels,probs,yval):

  pad_lst=[]

  for target in yval:
    pad_lst.append((labels==target).astype('int'))

  pad_targets=np.array(pad_lst)
  
  lst=[]
  
  for n in range(len(labels)):
    lst.append(sklearn.metrics.average_precision_score(pad_targets[:,n],probs[:,n]))
  
  return np.mean(lst)

def overall_class_metrics(thresh_df):
  scores_dict={}
  scores_dict['f1_score']=thresh_df['f1'].mean()
  
  scores_dict['mcc']=thresh_df['mcc'].mean()
  scores_dict['cohen_kappa']= thresh_df['kappa'].mean()

  return scores_dict

def pref_metrics(pref_metric,thresh_df):
  f1_thresh,fhalf_thresh,f2_thresh,recall_thresh,precision_thresh=best_thresholds(thresh_df)
  best_recall,best_precision,best_f1,best_f2,best_fhalf=best_class_scores(thresh_df)

  scores_dict={}
  
  if pref_metric.lower()=='recall':
    
    scores_dict['recall_thresh']=recall_thresh
    scores_dict['best_recall']=best_recall

    scores_dict['f2_thresh']= f2_thresh
    scores_dict['best_f2']= best_f2
    
    scores_dict['overall_f2']=thresh_df['f2'].mean()

  elif pref_metric.lower()=='precision':
    
    scores_dict['precision_thresh']=precision_thresh
    scores_dict['best_precision']=best_precision
  
    scores_dict['f0.5_threshold'] =fhalf_thresh
    scores_dict['best_f0.5']=best_fhalf
    
    scores_dict['overall_f0.5']=thresh_df['f0.5'].mean()
    
  else:
    scores_dict['f1_threshold']=f1_thresh
    scores_dict['best_f1']= best_f1

  return scores_dict

