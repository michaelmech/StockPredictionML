{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VolatilityDetection(1)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rRk8BSLk-WX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1432d9b3-e902-4679-e4de-cecb55b34146"
      },
      "source": [
        "!pip install -U yfinance\n",
        "!pip install quandl\n",
        "\n",
        "!pip install -U statsmodels\n",
        "!pip install --upgrade scipy\n",
        "\n",
        "!pip install fracdiff\n",
        "!pip install stumpy\n",
        "\n",
        "!pip install optuna\n",
        "!pip install flaml\n",
        "!pip install tpot\n",
        "\n",
        "!pip install tpot\n",
        "!pip install featexp\n",
        "\n",
        "!pip install sktime\n",
        "!pip install pytrends"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.7/dist-packages (0.1.67)\n",
            "Requirement already satisfied: lxml>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from yfinance) (4.7.1)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.3)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: quandl in /usr/local/lib/python3.7/dist-packages (3.7.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from quandl) (2.8.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from quandl) (8.12.0)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from quandl) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from quandl) (1.19.3)\n",
            "Requirement already satisfied: inflection>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from quandl) (0.5.1)\n",
            "Requirement already satisfied: pandas>=0.14 in /usr/local/lib/python3.7/dist-packages (from quandl) (1.1.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from quandl) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.14->quandl) (2018.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (1.24.3)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "Collecting statsmodels\n",
            "  Using cached statsmodels-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "Requirement already satisfied: scipy>=1.3 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.7.3)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.19.3)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (0.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->statsmodels) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.2->statsmodels) (1.15.0)\n",
            "Installing collected packages: statsmodels\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.12.1\n",
            "    Uninstalling statsmodels-0.12.1:\n",
            "      Successfully uninstalled statsmodels-0.12.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sktime 0.9.0 requires statsmodels<=0.12.1, but you have statsmodels 0.13.1 which is incompatible.\n",
            "fracdiff 0.7.0 requires numpy<2.0.0,>=1.21.3, but you have numpy 1.19.3 which is incompatible.\u001b[0m\n",
            "Successfully installed statsmodels-0.13.1\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.19.3)\n",
            "Requirement already satisfied: fracdiff in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from fracdiff) (1.7.3)\n",
            "Requirement already satisfied: statsmodels<0.14.0,>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from fracdiff) (0.13.1)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from fracdiff) (1.0.1)\n",
            "Collecting numpy<2.0.0,>=1.21.3\n",
            "  Using cached numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<2.0.0,>=1.0.1->fracdiff) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<2.0.0,>=1.0.1->fracdiff) (1.1.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels<0.14.0,>=0.13.0->fracdiff) (0.5.2)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.7/dist-packages (from statsmodels<0.14.0,>=0.13.0->fracdiff) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->statsmodels<0.14.0,>=0.13.0->fracdiff) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->statsmodels<0.14.0,>=0.13.0->fracdiff) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.2->statsmodels<0.14.0,>=0.13.0->fracdiff) (1.15.0)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.3\n",
            "    Uninstalling numpy-1.19.3:\n",
            "      Successfully uninstalled numpy-1.19.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\n",
            "sktime 0.9.0 requires numpy<=1.19.3, but you have numpy 1.21.4 which is incompatible.\n",
            "sktime 0.9.0 requires statsmodels<=0.12.1, but you have statsmodels 0.13.1 which is incompatible.\n",
            "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.4 which is incompatible.\n",
            "flaml 0.9.0 requires xgboost<=1.3.3,>=0.90, but you have xgboost 1.5.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stumpy in /usr/local/lib/python3.7/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numba>=0.48 in /usr/local/lib/python3.7/dist-packages (from stumpy) (0.54.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from stumpy) (1.21.4)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from stumpy) (1.7.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.48->stumpy) (57.4.0)\n",
            "Collecting numpy>=1.15\n",
            "  Downloading numpy-1.20.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /usr/local/lib/python3.7/dist-packages (from numba>=0.48->stumpy) (0.37.0)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.4\n",
            "    Uninstalling numpy-1.21.4:\n",
            "      Successfully uninstalled numpy-1.21.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.20.3 which is incompatible.\n",
            "sktime 0.9.0 requires numpy<=1.19.3, but you have numpy 1.20.3 which is incompatible.\n",
            "sktime 0.9.0 requires statsmodels<=0.12.1, but you have statsmodels 0.13.1 which is incompatible.\n",
            "fracdiff 0.7.0 requires numpy<2.0.0,>=1.21.3, but you have numpy 1.20.3 which is incompatible.\n",
            "flaml 0.9.0 requires xgboost<=1.3.3,>=0.90, but you have xgboost 1.5.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna) (3.10.0)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna) (0.8.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.27)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.20.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.3)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna) (6.6.0)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.8.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.1.6)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.4.0)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.3.3)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (5.8.0)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (0.4.0)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.4.0)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.2.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n",
            "Requirement already satisfied: flaml in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: NumPy>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from flaml) (1.20.3)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from flaml) (1.0.1)\n",
            "Collecting xgboost<=1.3.3,>=0.90\n",
            "  Downloading xgboost-1.3.3-py3-none-manylinux2010_x86_64.whl (157.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 157.5 MB 50 kB/s \n",
            "\u001b[?25hRequirement already satisfied: lightgbm>=2.3.1 in /usr/local/lib/python3.7/dist-packages (from flaml) (3.3.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from flaml) (1.7.3)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from flaml) (1.1.5)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm>=2.3.1->flaml) (0.37.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->flaml) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->flaml) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.4->flaml) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->flaml) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->flaml) (1.1.0)\n",
            "Installing collected packages: xgboost\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 1.5.1\n",
            "    Uninstalling xgboost-1.5.1:\n",
            "      Successfully uninstalled xgboost-1.5.1\n",
            "Successfully installed xgboost-1.3.3\n",
            "Requirement already satisfied: tpot in /usr/local/lib/python3.7/dist-packages (0.11.7)\n",
            "Requirement already satisfied: stopit>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.1.2)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.7/dist-packages (from tpot) (4.62.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.0.1)\n",
            "Requirement already satisfied: update-checker>=0.16 in /usr/local/lib/python3.7/dist-packages (from tpot) (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.20.3)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.1.0)\n",
            "Requirement already satisfied: xgboost>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.3.3)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.7.3)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.1.5)\n",
            "Requirement already satisfied: deap>=1.2 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->tpot) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->tpot) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->tpot) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.0->tpot) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from update-checker>=0.16->tpot) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2021.10.8)\n",
            "Requirement already satisfied: tpot in /usr/local/lib/python3.7/dist-packages (0.11.7)\n",
            "Requirement already satisfied: xgboost>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.3.3)\n",
            "Requirement already satisfied: update-checker>=0.16 in /usr/local/lib/python3.7/dist-packages (from tpot) (0.18.0)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.7.3)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.7/dist-packages (from tpot) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.20.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.0.1)\n",
            "Requirement already satisfied: stopit>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.1.2)\n",
            "Requirement already satisfied: deap>=1.2 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.3.1)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.1.5)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->tpot) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->tpot) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->tpot) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.0->tpot) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from update-checker>=0.16->tpot) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2021.10.8)\n",
            "Requirement already satisfied: featexp in /usr/local/lib/python3.7/dist-packages (0.0.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from featexp) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from featexp) (1.20.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from featexp) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->featexp) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->featexp) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->featexp) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->featexp) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->featexp) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->featexp) (2018.9)\n",
            "Requirement already satisfied: sktime in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Collecting statsmodels<=0.12.1\n",
            "  Using cached statsmodels-0.12.1-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.2.13)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.0.1)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.7/dist-packages (from sktime) (0.54.1)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.1.5)\n",
            "Collecting numpy<=1.19.3\n",
            "  Using cached numpy-1.19.3-cp37-cp37m-manylinux2010_x86_64.whl (14.9 MB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from sktime) (0.37.0)\n",
            "Requirement already satisfied: statsmodels>=0.12.1 in /usr/local/lib/python3.7/dist-packages (from sktime) (0.13.1)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.7/dist-packages (from sktime) (1.20.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.13->sktime) (1.13.3)\n",
            "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->sktime) (0.37.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->sktime) (57.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->sktime) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->sktime) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->sktime) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.0->sktime) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.0->sktime) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.0->sktime) (1.7.3)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.12.1->sktime) (0.5.2)\n",
            "Requirement already satisfied: pytrends in /usr/local/lib/python3.7/dist-packages (4.7.3)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.7/dist-packages (from pytrends) (1.1.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pytrends) (4.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytrends) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (1.20.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25->pytrends) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5BapypPOe2u",
        "outputId": "a0d59eac-81fd-49f0-cea1-3b8140ce51b7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas_datareader.data as pdr\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import manifold\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import quandl\n",
        "import json\n",
        "\n",
        "import tqdm\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import pickle\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import yfinance as yf\n",
        "yf.pdr_override()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "import joblib\n",
        "from joblib import Parallel,delayed\n",
        "\n",
        "import datetime\n",
        "from fracdiff.sklearn import FracdiffStat\n",
        "\n",
        "import sktime\n",
        "\n",
        "from sktime.forecasting.model_selection import temporal_train_test_split as ttts\n",
        "\n",
        "import stumpy\n",
        "#from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('drive',force_remount=True)\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "from flaml import AutoML\n",
        "\n",
        "from tpot import TPOTClassifier\n",
        "\n",
        "import featexp\n",
        "from featexp import get_trend_stats\n",
        "\n",
        "import lightgbm\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "from sklearn import model_selection\n",
        "\n",
        "import sklearn\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "import pytrends\n",
        "from pytrends import dailydata\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBULX71hfMO7"
      },
      "source": [
        "###helper functions\n",
        "\n",
        "def colon_print(word1,word2):\n",
        "  print(word1,':',word2)\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "\n",
        "class StratifiedKFold3(StratifiedKFold):\n",
        "\n",
        "  def split(self, X, y, groups=None):\n",
        "    s = super().split(X, y, groups)\n",
        "    for train_indxs, test_indxs in s:\n",
        "      y_train = y[train_indxs]\n",
        "      train_indxs, cv_indxs = train_test_split(train_indxs,stratify=y_train, test_size=(1 / (self.n_splits - 1)))\n",
        "      yield train_indxs, cv_indxs, test_indxs\n",
        "\n",
        "def split_data(df,targets=None,test_size=0.1):\n",
        "\n",
        "  df=df.copy()\n",
        "  \n",
        "  if targets is not None:\n",
        "    df['targets']=targets\n",
        "\n",
        "  x=df.drop('targets',axis=1)\n",
        "  \n",
        "  y=df['targets']\n",
        "  \n",
        "  xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=test_size, random_state=69, stratify=y)\n",
        "\n",
        "  train_val_data= pd.concat([xtrain,ytrain],axis=1)\n",
        "\n",
        "  test_data=pd.concat([xtest,ytest],axis=1)\n",
        "\n",
        "  return train_val_data,test_data\n",
        "\n",
        "def ts_split_data(df,targets=None,test_size=0.1):\n",
        "  df=df.copy()\n",
        "\n",
        "  if targets is not None:\n",
        "    df['targets']=targets\n",
        "\n",
        "  x=df.drop('targets',axis=1)\n",
        "  \n",
        "  y=df['targets']\n",
        "\n",
        "  ytrain, ytest, xtrain, xtest= ttts(y,x,test_size=test_size)\n",
        "\n",
        "  train_val_data= pd.concat([xtrain,ytrain],axis=1)\n",
        "\n",
        "  test_data=pd.concat([xtest,ytest],axis=1)\n",
        "\n",
        "  return train_val_data,test_data\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Collection"
      ],
      "metadata": {
        "id": "Fw5skJvKVKMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_gtrends():\n",
        "  gtrends_df=pd.read_csv('/content/drive/MyDrive/Nick Fury/External Datasets/gtrends.csv/google_trends.csv')\n",
        "\n",
        "  gtrends_df.index=gtrends_df.date\n",
        "\n",
        "  gtrends_df.index=pd.to_datetime(gtrends_df.index)\n",
        "\n",
        "  gtrends_df=gtrends_df.drop('date',axis=1)\n",
        "\n",
        "  next_date=(gtrends_df.index[-1]+datetime.timedelta(1))\n",
        "\n",
        "  current_date=datetime.datetime.now()\n",
        "\n",
        "  current_month=current_date.month\n",
        "\n",
        "  current_year=current_date.year\n",
        "\n",
        "  month=next_date.month\n",
        "  year=next_date.year\n",
        "\n",
        "  gseries_lst=[]\n",
        "  for term in gtrends_df:\n",
        "    gtrend_series=pytrends.dailydata.get_daily_data(term,start_year=year,start_mon=month,stop_year=current_year,stop_mon=current_month)[term+'_unscaled']\n",
        "    gseries_lst.append(gtrend_series)\n",
        "  \n",
        "  new_df=pd.concat(gseries_lst,axis=1)\n",
        "  new_df.columns=gtrends_df.columns\n",
        "\n",
        "  gtrends_df=pd.concat([gtrends_df,new_df])\n",
        "\n",
        "  gtrends_df=gtrends_df[~gtrends_df.index.duplicated(keep='first')]\n",
        "\n",
        "  gtrends_df.to_csv('/content/drive/MyDrive/Nick Fury/External Datasets/gtrends.csv/google_trends.csv')\n",
        "\n",
        "  return gtrends_df"
      ],
      "metadata": {
        "id": "NLZdyyCE1Vxd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1UjOlVME1Q"
      },
      "source": [
        "def get_close_volume(ticker_list):\n",
        "  returns_lst=[]\n",
        "  volume_lst=[]\n",
        "  for ticker in ticker_list:\n",
        "    data=yf.Ticker(ticker).history(period='max').dropna()\n",
        "    data=data.drop(['Dividends','Stock Splits'],axis=1)\n",
        "\n",
        "    data.columns=[ticker+'_'+col for col in data.columns]\n",
        "\n",
        "    returns_lst.append(data[ticker+'_'+'Close'])\n",
        "    volume_lst.append(data[ticker+'_'+'Volume'])\n",
        "  \n",
        "  close_df=pd.concat(returns_lst,axis=1)\n",
        "  volume_df=pd.concat(volume_lst,axis=1)\n",
        "    \n",
        "  return close_df,volume_df\n",
        "\n",
        "def get_returns(ticker_list):\n",
        "  returns_df=pd.DataFrame()\n",
        "  for ticker in ticker_list:\n",
        "    data=yf.Ticker(ticker).history(period='max').dropna()\n",
        "    returns_df[ticker]=data['Close'].pct_change()\n",
        "  \n",
        "  return returns_df\n",
        "\n",
        "def get_data(timeframe='1d',bidirectional=False):\n",
        "\n",
        "  gtrends_df=pd.read_csv('/content/drive/MyDrive/Nick Fury/External Datasets/gtrends.csv/google_trends.csv')\n",
        "\n",
        "  gtrends_df.index=gtrends_df.date\n",
        "\n",
        "  gtrends_df.index=pd.to_datetime(gtrends_df.index)\n",
        "\n",
        "  gtrends_df=gtrends_df.drop('date',axis=1)\n",
        "\n",
        "  #index_list=['vox','vcr','vdc','vde','vfh','vht','vis','vgt','vaw','vnq','vpu','dbp','vglt','vgit','vgsh','eem','uso','pdbc','gld','vwo','lqd']\n",
        "\n",
        "  #returns_df=get_returns(index_list)\n",
        "\n",
        "  vix_series=pdr.get_data_yahoo('^vix',interval=timeframe)['Close']\n",
        "\n",
        "  vix_df=pd.DataFrame(vix_series)\n",
        "\n",
        "  vix_df.columns=['vix_close']\n",
        "                      \n",
        "  vix_df['labels']=0\n",
        "  vix_std=abs(vix_df['vix_close'].diff()).std()\n",
        "\n",
        "  if not bidirectional:\n",
        "\n",
        "    #vix_df['labels'][abs(vix_df['vix_close'].diff())>vix_std]=1  #abs\n",
        "    vix_df['labels'][(vix_df['vix_close'].diff())>vix_std]=1\n",
        "\n",
        "  else:\n",
        "\n",
        "    vix_df['labels'][(vix_df['vix_close'].diff())>vix_std]=1  \n",
        "    vix_df['labels'][vix_df['vix_close'].diff()<-vix_std]=-1\n",
        "\n",
        "  vix_df['targets']=vix_df['labels'].shift(-1)\n",
        "  \n",
        "  quandl.ApiConfig.api_key = '*****************'\n",
        "\n",
        "  #yale_list=['YALE/US_CONF_INDEX_VAL_INDIV','YALE/US_CONF_INDEX_VAL_INST','YALE/US_CONF_INDEX_CRASH_INDIV','YALE/US_CONF_INDEX_CRASH_INST']\n",
        "\n",
        "  #yale_df_list=[quandl.get(code, start_date='2010-1-1', end_date=str(datetime.datetime.today().date()))['Index Value'] for code in yale_list] \n",
        "\n",
        "  #yale_df=pd.DataFrame(yale_df_list,index=yale_list).T  #yale confidence data\n",
        "\n",
        "  employ_df=quandl.get('FRED/NROUST', start_date='2010-1-1', end_date=str(datetime.datetime.today().date())) #short term unemployment]\n",
        "  product_df=quandl.get('FRED/GDPPOT', start_date='2010-1-1', end_date=str(datetime.datetime.today().date())) #nominal gross domestic product\n",
        "\n",
        "  fed_df=pd.concat([product_df,employ_df],axis=1)\n",
        "  fed_df.columns=['FRED/NROUST','FRED/GDPPOT'] #federal reserve data\n",
        "\n",
        "  infl_df=quandl.get('RATEINF/CPI_USA', start_date='2010-1-1', end_date=str(datetime.datetime.today().date()))\n",
        "  infl_df.columns=['basket_price']\n",
        "  infl_df['inflation_Q']=infl_df['basket_price'].pct_change()\n",
        "\n",
        "  short_df=quandl.get('FINRA/FNSQ_'+'spy'.upper(), start_date='2010-1-1', end_date=str(datetime.datetime.today().date()))['ShortVolume']\n",
        "  short_df.columns=['short_int']\n",
        "\n",
        "  sentiment_df=quandl.get('AAII/AAII_SENTIMENT-AAII-Investor-Sentiment-Data')\n",
        "\n",
        "  sentiment_df=sentiment_df.dropna()\n",
        "  sentiment_df=sentiment_df[['Bullish','Neutral','Bearish']].iloc[1:,:]\n",
        "\n",
        "  if timeframe=='1d':\n",
        "  \n",
        "    to_concat=[short_df,gtrends_df,vix_df] #returns_df,daily\n",
        "    to_merge=[sentiment_df,infl_df,fed_df,employ_df,product_df]  #yale_df,weekly\n",
        "\n",
        "  else:\n",
        "    to_concat=[short_df,gtrends_df] #returns_df,daily\n",
        "    to_merge=[sentiment_df,infl_df,fed_df,employ_df,product_df,vix_df]  #yale_df,weekly\n",
        "\n",
        "  combined_df=pd.concat(to_concat,axis=1)\n",
        "\n",
        "  for df in to_merge:\n",
        "    combined_df=pd.merge_asof(combined_df,df,right_index=True,left_index=True)\n",
        "\n",
        "  combined_df=combined_df.astype(float)\n",
        "\n",
        "  #combined_df=combined_df.fillna(method='pad').dropna()\n",
        "\n",
        "  combined_df=returns_only(combined_df)\n",
        "\n",
        "  targets=combined_df['targets']\n",
        "\n",
        "  combined_df=combined_df.drop(['labels','targets'],axis=1)\n",
        "\n",
        "  return combined_df,targets\n",
        "\n",
        "def returns_only(combined_df):\n",
        "\n",
        "  index_list=['vox','vcr','vdc','vde','vfh','vht','vis','vgt','vaw','vnq','vpu','dbp','vglt','vgit','vgsh','eem','uso','pdbc','gld','vwo','lqd']\n",
        "  returns_df=get_returns(index_list)\n",
        "\n",
        "  combined_df=pd.concat([combined_df,returns_df],axis=1)\n",
        "  \n",
        "  return combined_df.dropna()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jogFT-efN8Tr"
      },
      "source": [
        "gtrends_df=update_gtrends()\n",
        "combined_df,targets=get_data(bidirectional=False,timeframe='1wk')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing"
      ],
      "metadata": {
        "id": "b7HYbHzQP6JU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def frac_diff(x, d=0.7):\n",
        "    \"\"\"\n",
        "    Fractionally difference time series\n",
        "\n",
        "    :param x: numeric vector or univariate time series\n",
        "    :param d: number specifying the fractional difference order.\n",
        "    :return: fractionally differenced series\n",
        "    \"\"\"\n",
        "    if np.isnan(np.sum(x)):\n",
        "        return None\n",
        "\n",
        "    n = len(x)\n",
        "    if n < 2:\n",
        "        return None\n",
        "\n",
        "    x = np.subtract(x, np.mean(x))\n",
        "\n",
        "    # calculate weights\n",
        "    weights = [0] * n\n",
        "    weights[0] = -d\n",
        "    for k in range(2, n):\n",
        "        weights[k - 1] = weights[k - 2] * (k - 1 - d) / k\n",
        "\n",
        "    # difference series\n",
        "    ydiff = list(x)\n",
        "\n",
        "    for i in range(0, n):\n",
        "        dat = x[:i]\n",
        "        w = weights[:i]\n",
        "        ydiff[i] = x[i] + np.dot(w, dat[::-1])\n",
        "\n",
        "    return np.array(ydiff)\n",
        "\n",
        "def one_step_fracdiff(close_df,combined_df):\n",
        "  cutoff_date=combined_df.index[0]\n",
        "\n",
        "  cutoff_index=len(close_df.loc[:cutoff_date])\n",
        "\n",
        "  diff_array=np.zeros((combined_df.shape[0],close_df.shape[-1]))\n",
        "\n",
        "  for t in range(len(diff_array)):\n",
        "    diff_array[t,:]=frac_diff(close_df.iloc[:cutoff_index+t+1].dropna().values)[-1,:]\n",
        "\n",
        "  diff_df=pd.DataFrame(diff_array,columns=close_df.columns,index=combined_df.index)\n",
        "\n",
        "  return diff_df\n",
        "\n",
        "def update_one_fracdiff():\n",
        "  \n",
        "  index_list=['vox','vcr','vdc','vde','vfh','vht','vis','vgt','vaw','vnq','vpu','dbp','vglt','vgit','vgsh','eem','uso','gld','vwo','lqd','spy']\n",
        "  close_df,vol_df=get_close_volume(index_list)\n",
        "  \n",
        "  close_diffs=load_diffs()\n",
        "\n",
        "  next_date=(close_diffs.index[-1]+datetime.timedelta(1))\n",
        "\n",
        "  new_diffs=one_step_fracdiff(close_df,close_df.loc[next_date:])\n",
        "\n",
        "  close_diffs=pd.concat([close_diffs,new_diffs])\n",
        "\n",
        "  close_diffs.to_csv('/content/drive/MyDrive/close_diff.csv')\n",
        "\n",
        "  return close_diffs\n",
        "  \n",
        "def fracdiff_split_transform(combined_df,targets=None):\n",
        "\n",
        "  df=combined_df.copy()\n",
        "\n",
        "  if targets is not None:\n",
        "    df['targets']=targets\n",
        "\n",
        "  train_val_data,test_data=ts_split_data(df,targets)\n",
        "\n",
        "  train_data,val_data=ts_split_data(train_val_data,test_size=0.2)\n",
        "\n",
        "  index_list=['vox','vcr','vdc','vde','vfh','vht','vis','vgt','vaw','vnq','vpu','dbp','vglt','vgit','vgsh','eem','uso','gld','vwo','lqd','spy']\n",
        "\n",
        "  close_df,volume_df=get_close_volume(index_list)\n",
        "\n",
        "  weekly_df=pdr.get_data_yahoo('spy')\n",
        "\n",
        "  weekly_df.columns=['weekly_'+ col for col in weekly_df.columns]\n",
        "\n",
        "  weekly_close=weekly_df['weekly_Close']\n",
        "\n",
        "  weekly_vol=weekly_df['weekly_Volume']\n",
        "\n",
        "  assets_fracdiffs=fit_fracdiffs(close_df,val_data)\n",
        "\n",
        "  weekly_fracdiff=fit_fracdiffs(weekly_close,val_data)\n",
        "\n",
        "  fracdiff_dict={**assets_fracdiffs,**weekly_fracdiff}\n",
        "\n",
        "  combined_df=pd.concat([df,close_df,volume_df],axis=1)\n",
        "\n",
        "  weekly_df=pd.concat([weekly_close,weekly_vol],axis=1)\n",
        "\n",
        "  combined_df=pd.merge_asof(combined_df,weekly_df,right_index=True,left_index=True).dropna()\n",
        "\n",
        "  for asset in fracdiff_dict:\n",
        "    asset_diff=fracdiff_dict[asset].transform(combined_df[asset].values.reshape(-1,1))\n",
        "\n",
        "    combined_df[asset]=asset_diff\n",
        "  \n",
        "  return combined_df.drop('targets',axis=1).dropna()\n",
        "\n",
        "def fit_fracdiffs(close_df,val_data):\n",
        "\n",
        "  cutoff_date=val_data.index[0]\n",
        "\n",
        "  if isinstance(close_df,pd.Series):\n",
        "    close_df=pd.DataFrame(close_df)\n",
        "\n",
        "  fracdiff_dict={}\n",
        "\n",
        "  for asset in close_df.columns:\n",
        "    asset_df=close_df[asset]\n",
        "\n",
        "    fracdev=FracdiffStat(pvalue=0.01)\n",
        "\n",
        "    train_asset_close=fracdev.fit(asset_df.loc[:cutoff_date].dropna().values.reshape(-1,1))\n",
        "\n",
        "    fracdiff_dict[asset]=fracdev\n",
        "\n",
        "  return fracdiff_dict\n",
        "\n",
        "def load_diffs():\n",
        "\n",
        "  close_diffs=pd.read_csv('/content/drive/MyDrive/close_diff.csv')\n",
        "\n",
        "  close_diffs.index=pd.to_datetime(close_diffs['Unnamed: 0'])\n",
        "  close_diffs=close_diffs.drop('Unnamed: 0',axis=1)\n",
        "\n",
        "  return close_diffs\n",
        "\n",
        "def one_step_combine(combined_df):\n",
        "  \n",
        "  close_diffs=load_diffs()\n",
        "  \n",
        "  combined_df=pd.concat([combined_df,close_diffs],axis=1)\n",
        "\n",
        "  return combined_df.dropna()"
      ],
      "metadata": {
        "id": "2HKJ-diZXfBq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9K-UAlKNDp4"
      },
      "source": [
        "####Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def scaled_variance_vars(ts_train_data,threshold=0.03):\n",
        "\n",
        "  scaler=MinMaxScaler()\n",
        "\n",
        "  scaled_train=scaler.fit_transform(ts_train_data)\n",
        "\n",
        "  scaled_df=pd.DataFrame(scaled_train,columns=ts_train_data.columns,index=ts_train_data.index)\n",
        "\n",
        "  return list(scaled_df.var()[scaled_df.var()>=threshold].index)"
      ],
      "metadata": {
        "id": "jvN-bJSQ8qEt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agg_features(df,feature):\n",
        " # create a bunch of features using the date column\n",
        "\n",
        "  # create an aggregate dictionary\n",
        "  aggs = {}\n",
        "  # for aggregation by month, we calculate the\n",
        "  # number of unique month values and also the mean\n",
        "  # we aggregate by num1 and calculate sum, max, min\n",
        "  # and mean values of this column\n",
        "  \n",
        "  num_cols=[col for col in df.columns if col]\n",
        "  \n",
        "  for col in num_cols:\n",
        "    \n",
        "    aggs[col] = ['mean']\n",
        "\n",
        "  # for customer_id, we calculate the total count\n",
        "  #aggs['feature'] = ['size']\n",
        "  # again for customer_id, we calculate the total unique\n",
        "\n",
        "  # we group by customer_id and calculate the aggregates\n",
        "  agg_df = df.groupby(feature).agg(aggs)\n",
        "  agg_df = agg_df.reset_index()\n",
        "  \n",
        "  return agg_df\n",
        "\n",
        "def match_ts_targets(ts,targets):\n",
        "  \n",
        "  ts_copy=ts.copy()\n",
        "  ts_copy['targets']=targets\n",
        "\n",
        "  return ts_copy['targets']\n",
        "\n",
        "def outlier_features(df,ts_train_data):\n",
        "  if isinstance(df,pd.Series):\n",
        "      df=pd.DataFrame(df)\n",
        "\n",
        "  iso=IsolationForest()\n",
        "\n",
        "  iso.fit(ts_train_data)\n",
        "\n",
        "  iso_values=iso.predict(df)\n",
        "  \n",
        "  return pd.DataFrame(iso_values,index=df.index,columns=['outliers'])\n",
        "\n",
        "def add_date_features(df):\n",
        "\n",
        "  date_df=pd.DataFrame(index=df.index)\n",
        "\n",
        "  if 'date' not in df.columns:\n",
        "    date_df['date']=df.index\n",
        "\n",
        "  date_df.loc[:, 'year'] = date_df['date'].dt.year\n",
        "  date_df.loc[:, 'weekofyear'] = date_df['date'].dt.weekofyear\n",
        "  date_df.loc[:, 'month'] = date_df['date'].dt.month\n",
        "  date_df.loc[:, 'dayofweek'] = date_df['date'].dt.dayofweek\n",
        "  date_df.loc[:, 'weekend'] = (date_df['date'].dt.weekday >=5).astype(int)\n",
        "\n",
        "  return date_df.drop('date',axis=1)\n",
        "\n",
        "def poly_featurizer(feat_df,degree=2):\n",
        "\n",
        "  pf=PolynomialFeatures(degree=degree)\n",
        "\n",
        "  poly_values=pf.fit_transform(feat_df)\n",
        "\n",
        "  feat_df=pd.DataFrame(poly_values,columns=['feat_'+ str(i) for i in range(poly_values.shape[-1])],index=feat_df.index)\n",
        "\n",
        "  return feat_df\n",
        "\n",
        "def fracdiff_featurizer(combined_df,col_lst,threshold=0.01):\n",
        "\n",
        "  if isinstance(combined_df,pd.Series):\n",
        "    combined_df=pd.DataFrame(combined_df)\n",
        "  \n",
        "  fracdev=FracdiffStat(pvalue=threshold)\n",
        "\n",
        "  diff_values=fracdev.fit_transform(combined_df[col_lst])\n",
        "\n",
        "  diff_df=pd.DataFrame(diff_values,columns=['diff_'+ col for col in col_lst],index=combined_df.index)\n",
        "\n",
        "  return diff_df\n",
        "\n",
        "def matprof_featurizer(df,window_size=3,discords=False):\n",
        "\n",
        "  if isinstance(df,pd.Series):\n",
        "    df=pd.DataFrame(df)\n",
        "  \n",
        "  if not discords:\n",
        "    pattern='motifs_'\n",
        "\n",
        "  else:\n",
        "    pattern='discords_'\n",
        "  \n",
        "  if df.shape[-1]>1:\n",
        "    matprof_array,_= stumpy.mstump(df,m=window_size,discords=discords,normalize=True)\n",
        "\n",
        "    return pd.DataFrame(matprof_array.T,columns=[pattern+ col for col in df.columns],index=df.index[window_size-1:])\n",
        "  \n",
        "  else:\n",
        "    prof_array,_=stumpy.stump(df,m=window_size,discords=discords,normalize=True)\n",
        "    \n",
        "    return  pd.DataFrame(prof_array.T,columns=[pattern+ col for col in df.columns],index=df.index[window_size-1:])\n",
        "\n",
        "def one_matprof(df,window_size=3,discords=False):\n",
        "  if isinstance(df,pd.Series):\n",
        "    df=pd.DataFrame(df)\n",
        "  \n",
        "  if not discords:\n",
        "    pattern='motifs_'\n",
        "\n",
        "  else:\n",
        "    pattern='discords_'\n",
        "  \n",
        "  \n",
        "  lst=[simply_mstump(df.iloc[:window_size+t],window_size,discords) for t in range(len(df))]\n",
        "\n",
        "  return np.stack(lst)\n",
        "  \n",
        "def simply_mstump(df,window_size=3,discords=False):\n",
        "  prof_array,_=stumpy.mstump(df,m=window_size,discords=discords,normalize=True)\n",
        "  \n",
        "  return prof_array.T[-1,:]\n",
        "    \n",
        "  \n",
        "class Time2Vec(nn.Module):\n",
        "  def __init__(self, activation,hiddem_dim,in_features):\n",
        "      super().__init__()\n",
        "      self.in_features=in_features\n",
        "      if activation == \"sin\":\n",
        "          self.l1 = SineActivation(self.in_features, hiddem_dim)\n",
        "      elif activation == \"cos\":\n",
        "          self.l1 = CosineActivation(self.in_features, hiddem_dim)\n",
        "      \n",
        "      self.fc1 = nn.Linear(hiddem_dim, 2)\n",
        "  \n",
        "  def forward(self, x):\n",
        "      #x = x.unsqueeze(1)\n",
        "      x = self.l1(x)\n",
        "      x = self.fc1(x)\n",
        "      return x\n",
        "\n",
        "def t2v(tau, f, out_features, w, b, w0, b0, arg=None):\n",
        "    if arg:\n",
        "        v1 = f(torch.matmul(tau, w) + b, arg)\n",
        "    else:\n",
        "        \n",
        "        v1 = f(torch.matmul(tau, w) + b)\n",
        "    v2 = torch.matmul(tau, w0) + b0\n",
        "   \n",
        "    return torch.cat([v1, v2], 1)\n",
        "\n",
        "class SineActivation(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(SineActivation, self).__init__()\n",
        "        self.out_features = out_features\n",
        "        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
        "        self.b0 = nn.parameter.Parameter(torch.randn(1, 1))\n",
        "        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
        "        self.b = nn.parameter.Parameter(torch.randn(1, out_features-1))\n",
        "        self.f = torch.sin\n",
        "\n",
        "    def forward(self, tau):\n",
        "        return t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)\n",
        "\n",
        "def t2v_func(data):\n",
        "  if len(data.shape) >1:\n",
        "    time2vec=Time2Vec(activation='sin',hiddem_dim=100,in_features=data.shape[-1])\n",
        "    t2v_data=time2vec(torch.Tensor(data.values))\n",
        "\n",
        "  else:\n",
        "    data=np.reshape(data.values,(data.shape[0],1))\n",
        "    time2vec=Time2Vec(activation='sin',hiddem_dim=100,in_features=data.shape[-1])\n",
        "    t2v_data=time2vec(torch.Tensor(data))\n",
        "\n",
        "  return pd.DataFrame(t2v_data.detach().numpy(),columns=['t2v_0','t2v_1'],index=data.index)\n",
        "     \n",
        "\n",
        "def ts_2_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\n",
        "  n_vars = 1 if type(data) is list else data.shape[1]\n",
        "  df = pd.DataFrame(data)\n",
        "  cols, names = list(), list()\n",
        "  # input sequence (t-n, ... t-1)\n",
        "  for i in range(n_in, 0, -1):\n",
        "    cols.append(df.shift(i))\n",
        "    names += [col+('(t-%d)' % (i)) for col in data.columns]\n",
        "  # forecast sequence (t, t+1, ... t+n)\n",
        "  for i in range(0, n_out):\n",
        "    cols.append(df.shift(-i))\n",
        "    if i == 0:\n",
        "      names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "    else:\n",
        "      names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "  # put it all together\n",
        "  agg = pd.concat(cols, axis=1)\n",
        "  agg.columns = names\n",
        "  # drop rows with NaN values\n",
        "  if dropnan:\n",
        "    agg.dropna(inplace=True)\n",
        "  return agg\n",
        "\n",
        "def stationarity_tester(series,threshold=0.01,print_mode=False):\n",
        "  \n",
        "  p_value=adfuller(series)[1]\n",
        "\n",
        "  if print_mode:\n",
        "    if series.name:\n",
        "     print(series.name+' is')\n",
        "    \n",
        "    if p_value<threshold:\n",
        "      print('Stationary')\n",
        "    \n",
        "    else:\n",
        "      print('not Stationary')\n",
        "  \n",
        "  return p_value\n",
        "\n",
        "def non_stationary_vars(combined_df):\n",
        "  col_lst=[]\n",
        "\n",
        "  for col in combined_df.columns:\n",
        "    p_value=stationarity_tester(combined_df[col],threshold=0.01)\n",
        "    if p_value>0.01:\n",
        "      col_lst.append(col)\n",
        "  \n",
        "  return col_lst"
      ],
      "metadata": {
        "id": "PhwtphBYaiWK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature engineering\n",
        "\n",
        "def ts_engineer_splits(combined_df):\n",
        "\n",
        "  ts_train_val_data,ts_test_data=ts_split_data(combined_df,targets)\n",
        "\n",
        "  ts_train_data,ts_val_data=ts_split_data(ts_train_val_data,test_size=0.2)\n",
        "\n",
        "  ts_train_data=ts_train_data.drop('targets',axis=1)\n",
        "\n",
        "  ts_val_data=ts_val_data.drop('targets',axis=1)\n",
        "\n",
        "  return ts_train_data,ts_val_data\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def scaled_variance_vars(ts_train_data,threshold=0.03):\n",
        "\n",
        "  scaler=MinMaxScaler()\n",
        "\n",
        "  scaled_train=scaler.fit_transform(ts_train_data)\n",
        "\n",
        "  scaled_df=pd.DataFrame(scaled_train,columns=ts_train_data.columns,index=ts_train_data.index)\n",
        "\n",
        "  return list(scaled_df.var()[scaled_df.var()>=threshold].index)\n",
        "\n",
        "def ts_single_count(df,col):\n",
        "  df=df.copy()\n",
        "  df[col+'_count']=0\n",
        "\n",
        "  for t in range(len(df)):\n",
        "    relevant=df[col].iloc[:t+1]\n",
        "    \n",
        "    temp = relevant.value_counts()\n",
        "\n",
        "    temp_item=temp[temp.index==relevant[-1]].item()\n",
        "\n",
        "    df[col+'_count'].iloc[t]=temp_item\n",
        "  \n",
        "  return df[col+'_count']\n",
        "\n",
        "def engineer_splits(df,targets,ts_train_data):\n",
        "\n",
        "  if 'targets' in df.columns:\n",
        "    print('target in df')\n",
        "\n",
        "  proto=df.dropna().copy()\n",
        "\n",
        "  close_diffs=load_diffs()\n",
        "\n",
        "  diff_df=one_step_combine(proto)\n",
        "\n",
        "  scaled_var_vars= scaled_variance_vars(ts_train_data)\n",
        "\n",
        "  super_df=ts_2_supervised(proto,n_in=2)\n",
        "\n",
        "  date_df=add_date_features(proto)\n",
        "\n",
        "  t2v_df=pd.concat([t2v_func(combined_df.iloc[:t+1]).iloc[-1,:] for t in range(len(combined_df))],axis=1).T\n",
        "  \n",
        "  iso_df=outlier_features(proto,ts_train_data)\n",
        "\n",
        "  to_concat=[date_df,iso_df,t2v_df,diff_df,super_df,proto]\n",
        "\n",
        "  feat_df=pd.concat(to_concat,axis=1).T.drop_duplicates().T\n",
        "\n",
        "  targets=match_ts_targets(feat_df,targets)\n",
        "\n",
        "  df=feat_df.copy().dropna()\n",
        "\n",
        "  df['targets']=targets.dropna().copy()\n",
        "\n",
        "  x=df.drop('targets',axis=1).T.drop_duplicates().T\n",
        "\n",
        "  y=df['targets']\n",
        "\n",
        "  xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=69, stratify=y)\n",
        "\n",
        "  train_val_data,test_data=split_data(df,targets)\n",
        "\n",
        "  train_data,val_data=split_data(train_val_data)\n",
        "\n",
        "  return train_data.T.drop_duplicates().T,val_data.T.drop_duplicates().T,train_val_data.T.drop_duplicates().T,test_data.T.drop_duplicates().T,df"
      ],
      "metadata": {
        "id": "dcf9T8f4cMuv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts_train_data,ts_val_data=ts_engineer_splits(combined_df)\n",
        "\n",
        "train_data,val_data,train_val_data,test_data,feat_df= engineer_splits(combined_df,targets,ts_train_data)"
      ],
      "metadata": {
        "id": "bolcpv63peFn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this base model was derived using flaml's automl on the train_val data, and serves as the base for feature selection\n",
        "\n",
        "model=LGBMClassifier(colsample_bytree=0.7339786814213308,\n",
        "               learning_rate=0.0637270125392533, max_bin=1023,\n",
        "               min_child_samples=7, n_estimators=663, num_leaves=18,\n",
        "               reg_alpha=0.0009765625, reg_lambda=0.023076931736850263,\n",
        "               verbose=-1)"
      ],
      "metadata": {
        "id": "lCmmPsujc135"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Feature Selection"
      ],
      "metadata": {
        "id": "OohJUHZIe_ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def permutation_importance(model,features_valid,labels_valid):\n",
        "  base_score=roc_auc_score(labels_valid,model.predict(features_valid))\n",
        "\n",
        "  list_fimportance=[]\n",
        "\n",
        "  for col in features_valid.columns:\n",
        "    save=features_valid[col].copy()\n",
        "    features_valid[col]=np.random.permutation(features_valid[col])\n",
        "    col_score=roc_auc_score(labels_valid,model.predict(features_valid))\n",
        "\n",
        "    features_valid[col]=save\n",
        "    \n",
        "    list_fimportance.append([col,base_score-col_score])\n",
        "  \n",
        "  return pd.DataFrame(list_fimportance,columns=['feature','importance'])\n",
        "\n",
        "def adversarial_single_selection(df_train, df_test,model, N_val=50000):\n",
        "    df_train=df_train.copy()\n",
        "    df_test=df_test.copy()\n",
        "    \n",
        "    df_train['targets'] = 0\n",
        "    df_test['targets'] = 1\n",
        "    target = 'targets'\n",
        "\n",
        "    df_master = pd.concat([df_train, df_test], axis=0)\n",
        "    adversarial_test = df_master.sample(N_val, replace=False)\n",
        "    adversarial_train = df_master[~df_master.index.isin(adversarial_test.index)]\n",
        "\n",
        "    model.fit(adversarial_train.drop('targets',axis=1),adversarial_train.targets)\n",
        "\n",
        "    probs=model.predict_proba(adversarial_test.drop('targets',axis=1))[:,-1]\n",
        "\n",
        "    return sklearn.metrics.roc_auc_score(adversarial_test.targets,probs),model\n",
        "\n",
        "def feat_experience(model,df,threshold=0.75,target_col='targets'):\n",
        "\n",
        "  df=df.copy()\n",
        "\n",
        "  if targets is not None:\n",
        "    df['targets']=targets\n",
        "  \n",
        "  x=df.drop('targets',axis=1).T.drop_duplicates().T\n",
        "  y=df.targets\n",
        "\n",
        "  base_lst=[]\n",
        "  new_lst=[]\n",
        "  kf=StratifiedKFold(n_splits=3,shuffle=True,random_state=69)\n",
        "\n",
        "  print('cv for stat analysis')\n",
        "  for fold, (t_,v_) in tqdm_notebook(enumerate(kf.split(X=x,y=y))):\n",
        "    xtrain=x.iloc[t_,:].values\n",
        "    \n",
        "    ytrain=y.iloc[t_].values\n",
        "    \n",
        "    xval=x.iloc[v_,:].values\n",
        "    yval=y.iloc[v_].values\n",
        "  \n",
        "    model.fit(xtrain,ytrain)\n",
        "\n",
        "    probs=model.predict_proba(xval)[:,-1]\n",
        "\n",
        "    baseline_score=sklearn.metrics.roc_auc_score(yval,probs)\n",
        "\n",
        "    base_lst.append(baseline_score)\n",
        "\n",
        "    train_data=pd.concat([pd.DataFrame(xtrain),pd.Series(ytrain)],axis=1).T.drop_duplicates().T\n",
        "\n",
        "    cols=list(x.columns)\n",
        "    cols.append(y.name)\n",
        "\n",
        "    train_data.columns=cols\n",
        "\n",
        "    test_data=pd.concat([pd.DataFrame(xval),pd.Series(yval)],axis=1)\n",
        "    test_data.columns=cols\n",
        "\n",
        "    trend_df=get_trend_stats(data=train_data,data_test=test_data,target_col=target_col)\n",
        "\n",
        "    good_feats=list(trend_df['Feature'][abs(trend_df['Trend_correlation'])>threshold])\n",
        "   \n",
        "    good_xtrain=x[good_feats].iloc[t_,:]\n",
        "    good_xval=x[good_feats].iloc[v_,:]\n",
        "\n",
        "    model.fit(good_xtrain.values,ytrain)\n",
        "\n",
        "    probs=model.predict_proba(good_xval)[:,-1]\n",
        "\n",
        "    new_score=sklearn.metrics.roc_auc_score(yval,probs)\n",
        "\n",
        "    new_lst.append(new_score)\n",
        "  \n",
        "  mean_new_score=np.mean(new_lst)\n",
        "  mean_base_score=np.mean(base_lst)\n",
        "  \n",
        "  if mean_new_score>mean_base_score:\n",
        "    return set(df.columns)-set(good_feats)\n",
        "  \n",
        "  else:\n",
        "    print('futile')\n",
        "    return None\n",
        "\n",
        "def nested_feature_selection(df,model,targets=None):\n",
        "  \n",
        "  df=df.copy()\n",
        "\n",
        "  if targets is not None:\n",
        "    df['targets']=targets\n",
        "  \n",
        "  features=df.drop('targets',axis=1).columns\n",
        "  \n",
        "  folds_df=create_folds(df)\n",
        "\n",
        "  prime_train_list=[]\n",
        "  prime_val_list=[]\n",
        "\n",
        "  print('nested cv for feature selection')\n",
        "  for fold in tqdm_notebook(folds_df['kfold'].unique()): #outer cv\n",
        "\n",
        "    df_train_outer = folds_df[folds_df.kfold != fold].reset_index(drop=True)\n",
        " \n",
        "    df_valid_outer = folds_df[folds_df.kfold == fold].reset_index(drop=True)\n",
        "    \n",
        "    df_inner= create_folds(df_train_outer)\n",
        "\n",
        "    val_scores_list=[]\n",
        "    train_scores_list=[]\n",
        "    \n",
        "    print('doing inner folds')\n",
        "    for inner_fold in tqdm_notebook(df_inner['kfold'].unique()):\n",
        "      df_train_inner= df_inner[df_inner.kfold != inner_fold].reset_index(drop=True)\n",
        "\n",
        "      df_valid_inner=df_inner[df_inner.kfold == inner_fold].reset_index(drop=True)\n",
        "      \n",
        "      val_series=pd.Series()\n",
        "      train_series=pd.Series()\n",
        "      for col in features:\n",
        "        xtrain_inner=np.expand_dims(df_train_inner[col].values,axis=1)\n",
        "        ytrain_inner=df_train_inner.targets.values\n",
        "\n",
        "        xval_inner=np.expand_dims(df_valid_inner[col].values,axis=1)\n",
        "        yval_inner=df_valid_inner.targets.values\n",
        "    \n",
        "        model.fit(xtrain_inner,ytrain_inner)\n",
        "\n",
        "        val_probs=model.predict_proba(xval_inner)[:,-1]\n",
        "\n",
        "        train_probs=model.predict_proba(xtrain_inner)[:,-1]\n",
        "\n",
        "        val_score=sklearn.metrics.roc_auc_score(yval_inner,val_probs)\n",
        "\n",
        "        train_score=sklearn.metrics.roc_auc_score(ytrain_inner,train_probs)\n",
        "      \n",
        "        val_series.loc[col]=val_score\n",
        "        train_series.loc[col]=train_score\n",
        "\n",
        "      val_scores_list.append(val_series)\n",
        "      train_scores_list.append(train_series)\n",
        "  \n",
        "    prime_train_list.append(pd.concat(train_scores_list,axis=1))\n",
        "    \n",
        "    prime_val_list.append(pd.concat(val_scores_list,axis=1))\n",
        "  \n",
        "  new_train_list=[i.T.mean() for i in prime_train_list]\n",
        "  new_val_list=[i.T.mean() for i in prime_val_list]\n",
        "\n",
        "  train_feat_df=sum(new_train_list)/len(new_train_list)\n",
        "\n",
        "  val_feat_df=sum(new_val_list)/len(new_val_list)\n",
        "\n",
        "  comb_df=pd.concat([train_feat_df,val_feat_df],axis=1)\n",
        "\n",
        "  bad_list=[]\n",
        "\n",
        "  for var in comb_df.T:\n",
        "    if comb_df.T[var].iloc[0]<0.5 or comb_df.T[var].iloc[-1]<0.5:\n",
        "      bad_list.append(var)\n",
        "  \n",
        "  return bad_list\n",
        "\n",
        "def adversarial_validation(train_val_data,model):\n",
        "\n",
        "  df=train_val_data.copy()\n",
        "\n",
        "  df['targets']=-1\n",
        "\n",
        "  x=df.drop('targets',axis=1).reset_index(drop=True)\n",
        "  y=df.targets.reset_index(drop=True)\n",
        "\n",
        "  xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.25, random_state=69, stratify=y)\n",
        "\n",
        "  ytrain=pd.DataFrame(np.zeros(ytrain.shape),columns=['targets'],index=ytrain.index).astype(int)\n",
        "\n",
        "  ytest=pd.DataFrame(np.ones(ytest.shape),columns=['targets'],index=ytest.index).astype(int)\n",
        "\n",
        "  new_train=pd.concat([xtrain,ytrain],axis=1)\n",
        "\n",
        "  new_test=pd.concat([xtest,ytest],axis=1)\n",
        "\n",
        "  new_data=pd.concat([new_train,new_test])\n",
        "\n",
        "  new_x=new_data.drop('targets',axis=1).values\n",
        "\n",
        "  new_y=new_data.targets.values\n",
        "\n",
        "  kf=StratifiedKFold(n_splits=3,shuffle=True,random_state=69)\n",
        "\n",
        "  score_lst=[]\n",
        "\n",
        "  print('cv for adversarial feature selection')\n",
        "  for fold, (t_,v_) in tqdm_notebook(enumerate(kf.split(X=new_x,y=new_y))):\n",
        "    xtrain=new_x[t_,:]\n",
        "    ytrain=new_y[t_]\n",
        "    \n",
        "    xval=new_x[v_,:]\n",
        "    yval=new_y[v_]\n",
        "\n",
        "    model.fit(xtrain,ytrain)\n",
        "\n",
        "    probs=model.predict_proba(xval)[:,-1]\n",
        "\n",
        "    score=sklearn.metrics.roc_auc_score(yval,probs)\n",
        "\n",
        "    score_lst.append(score)\n",
        "  \n",
        "  if np.mean(score_lst)>0.6:\n",
        "    xtrain,xtest,ytrain,ytest=train_test_split(new_x,new_y,random_state=69,stratify=new_y)\n",
        "    final_model=model.fit(xtrain,ytrain)\n",
        "\n",
        "    return final_model,np.mean(score_lst)\n",
        "\n",
        "  else:\n",
        "    return None,np.mean(score_lst)\n",
        "\n",
        "def time_consistency(ts_df,model,targets=None):\n",
        "\n",
        "  df=ts_df.copy()\n",
        "\n",
        "  if targets is not None:\n",
        "    df['targets']=targets\n",
        "\n",
        "  train_data=df.iloc[:round(0.7*len(df)),:]\n",
        "\n",
        "  first_data=train_data.iloc[:30]\n",
        "\n",
        "  last_data=train_data.iloc[-30:]\n",
        "\n",
        "  bad_lst=[]\n",
        "  \n",
        "  print('Time consistency analysis for each feature')\n",
        "  for col in tqdm_notebook(train_data.columns):\n",
        "    \n",
        "    first_x=np.expand_dims(first_data[col].values,axis=1)\n",
        "    first_y=np.expand_dims(first_data.targets.values,axis=1)\n",
        "\n",
        "    last_x=np.expand_dims(last_data[col].values,axis=1)\n",
        "    last_y=np.expand_dims(last_data.targets.values,axis=1)\n",
        "\n",
        "    model.fit(first_x,first_y)\n",
        "\n",
        "    first_probs=model.predict_proba(first_y)[:,-1]\n",
        "\n",
        "    last_probs=model.predict_proba(last_x)[:,-1]\n",
        "\n",
        "    first_score=sklearn.metrics.roc_auc_score(first_y,first_probs)\n",
        "\n",
        "    last_score=sklearn.metrics.roc_auc_score(last_y,last_probs)\n",
        "\n",
        "    if first_score <0.5 or last_score<0.5:\n",
        "      bad_lst.append(col)\n",
        "\n",
        "  return bad_lst                "
      ],
      "metadata": {
        "id": "NJu9_AgZWNwb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inconsistent_lst=time_consistency(train_data,model)\n",
        "\n",
        "bad_trend_feats=feat_experience(model,train_data,threshold=0.5)\n",
        "\n",
        "adv_model,adv_score=adversarial_validation(train_val_data,model)\n",
        "\n",
        "nfs_lst=nested_feature_selection(train_val_data,model)"
      ],
      "metadata": {
        "id": "EIsZl4O_R45g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_combos=[bad_trend_feats,inconsistent_lst+list(bad_trend_feats),inconsistent_lst,inconsistent_lst+nfs_lst+list(bad_trend_feats),list(bad_trend_feats)+nfs_lst,\n",
        "nfs_lst,inconsistent_lst+nfs_lst]\n",
        "\n",
        "score_lst=[]\n",
        "f_lst2=[]\n",
        "for combo in feature_combos:\n",
        "  fold_lst=run_folds(train_val_data.drop(combo,axis=1),model=model,targets=targets,pref_metric='precision')\n",
        "\n",
        "  prec_lst=[]\n",
        "  f_lst=[]\n",
        "  for fold in fold_lst:\n",
        "    prec_lst.append(fold_lst[fold]['scores']['average_precision'])\n",
        "    f_lst.append(fold_lst[fold]['scores']['overall_f0.5'])\n",
        "\n",
        "  score_lst.append(np.mean(prec_lst))\n",
        "  f_lst2.append(np.mean(f_lst))\n",
        "\n",
        "\n",
        "#just take the argmax(index) of either lists to see which combination of features to drop maximizes the validation score of the preferred metric"
      ],
      "metadata": {
        "id": "Q8txLopLSM0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_weekly_to_drop(to_save=None):\n",
        "\n",
        "  if to_save is not None:\n",
        "    with open('/content/drive/MyDrive/drop_columns.txt', 'w') as fp:\n",
        "      json.dump(to_save,fp)\n",
        "\n",
        "  with open('/content/drive/MyDrive/drop_columns.txt', 'r') as fp:\n",
        "    to_drop=json.load(fp)\n",
        "\n",
        "  return to_drop"
      ],
      "metadata": {
        "id": "1DqTb7ANSOoD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_drop=load_weekly_to_drop()"
      ],
      "metadata": {
        "id": "R0XLXP1DSw06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Automl"
      ],
      "metadata": {
        "id": "WBectS_1fCBj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZl5xePGuCD3"
      },
      "source": [
        "def run_nested_flaml(df,targets=None,seconds=300):\n",
        "  \n",
        "  df=df.copy()\n",
        "\n",
        "  if targets is not None:\n",
        "    df['targets']=targets\n",
        "  \n",
        "  folds_df=create_folds(df)\n",
        "\n",
        "  model_lst=[]\n",
        "\n",
        "  for fold in tqdm_notebook(folds_df['kfold'].unique()): #outer cv\n",
        "\n",
        "    df_train_outer = folds_df[folds_df.kfold != fold].reset_index(drop=True)\n",
        "\n",
        "    df_valid_outer = folds_df[folds_df.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "    xtrain=df_train_outer.drop(['targets','kfold'],axis=1).values\n",
        "    ytrain=df_train_outer.targets.values\n",
        "    \n",
        "    automl=AutoML()\n",
        "\n",
        "    automl_settings = {\n",
        "        \"time_budget\": seconds,  # in seconds\n",
        "        \"metric\": 'ap',\n",
        "        \"task\": 'classification',\n",
        "    }\n",
        "\n",
        "    automl.fit(xtrain,ytrain,**automl_settings)\n",
        "\n",
        "    model_lst.append(automl.model.estimator)\n",
        "    \n",
        "  return cv_eval_models(df,model_lst,target_metric)\n",
        "\n",
        "def run_nested_tpot(df,targets=None):\n",
        "  \n",
        "  df=df.copy()\n",
        "\n",
        "  if targets is not None:\n",
        "    df['targets']=targets\n",
        "  \n",
        "  folds_df=create_folds(df)\n",
        "\n",
        "  model_lst=[]\n",
        "\n",
        "  for fold in tqdm_notebook(folds_df['kfold'].unique()): #outer cv\n",
        "\n",
        "    df_train_outer = folds_df[folds_df.kfold != fold].reset_index(drop=True)\n",
        "\n",
        "    df_valid_outer = folds_df[folds_df.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "    xtrain=df_train_outer.drop(['targets','kfold'],axis=1).values\n",
        "    ytrain=df_train_outer.targets.values\n",
        "    \n",
        "    automl=AutoML()\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, random_state=69,shuffle=True)\n",
        "\n",
        "    tpot = TPOTClassifier(generations=5, population_size=10, verbosity=2, random_state=69,cv=cv,scoring='average_precision')\n",
        "\n",
        "    tpot.fit(xtrain, ytrain)\n",
        "\n",
        "    model_lst.append(tpot.fitted_pipeline_)\n",
        "\n",
        "  return model_lst\n",
        "  #return cv_eval_models(model_lst,target_metric=target_metric)\n",
        "        \n",
        "global target_metric\n",
        "target_metric='average_precision'\n",
        "\n",
        "def cv_eval_models(df,model_lst,target_metric):\n",
        "  \n",
        "  model_scores=[]\n",
        "  for model in model_lst:\n",
        "    try:\n",
        "      fold_lst=run_folds(df,model,pref_metric='precision')\n",
        "      \n",
        "      fold_scores=[]\n",
        "      for fold in fold_lst:\n",
        "        fold_scores.append(fold_lst[fold]['scores'][target_metric])    \n",
        "\n",
        "      model_scores.append(np.mean(fold_scores))\n",
        "    \n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      pass\n",
        "\n",
        "  return model_lst[np.argmax(model_scores)]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model=LGBMClassifier(colsample_bytree=0.3794507019586446,\n",
        "               learning_rate=0.019289262657833043, max_bin=255,\n",
        "               min_child_samples=2, n_estimators=132, num_leaves=389,\n",
        "               reg_alpha=0.0009765625, reg_lambda=0.011000563062299913,\n",
        "               verbose=-1)"
      ],
      "metadata": {
        "id": "hvNWm7EO8AYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model=run_nested_flaml(train_val_data.drop(to_drop,axis=1),targets=targets,seconds=300)\n",
        "tpot_model=run_nested_tpot(train_val_data.drop(to_drop,axis=1),targets=targets)"
      ],
      "metadata": {
        "id": "UyoQ-6P-SFCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fold_lst=run_folds(train_val_data.drop(to_drop,axis=1),model=best_model,targets=targets,pref_metric='precision')"
      ],
      "metadata": {
        "id": "IGYn6pLs3G8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.fit(train_val_data.drop(to_drop,axis=1),train_val_data.targets)\n",
        "probs=best_model.predict_proba(test_data.drop(to_drop,axis=1))[:,-1]\n",
        "\n",
        "print(sklearn.metrics.average_precision_score(test_data.targets,probs))"
      ],
      "metadata": {
        "id": "yNifo2BlCHxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb0ke4AVK2vD"
      },
      "source": [
        "def matters(model,test_data):\n",
        "  probs=model.predict_proba(test_data.drop('targets',axis=1))[:,-1]\n",
        "  \n",
        "  preds=model.predict(test_data.drop('targets',axis=1))\n",
        "  \n",
        "  return sklearn.metrics.average_precision_score(test_data['targets'],probs),sklearn.metrics.fbeta_score(test_data['targets'],preds,beta=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_week(feat_df):\n",
        "\n",
        "  best_threshold=0.5\n",
        "\n",
        "  best_model=pickle_jar()\n",
        "\n",
        "  to_drop=load_weekly_to_drop()\n",
        "\n",
        "  cols_to_use=train_val_data.drop(to_drop,axis=1).columns\n",
        "\n",
        "  dropped_df=feat_df[cols_to_use]\n",
        "\n",
        "  best_model.fit(dropped_df.drop('targets',axis=1).iloc[:-1],feat_df.targets.iloc[:-1])\n",
        "\n",
        "  prob=best_model.predict_proba(dropped_df.drop('targets',axis=1).iloc[-1].values.reshape(1,-1))[:,-1]\n",
        "\n",
        "  if prob>best_threshold:\n",
        "    print('Positive increase in volatility next week')\n",
        "\n",
        "  elif prob<1-best_threshold:\n",
        "    print('No positive increase in volatility next week')"
      ],
      "metadata": {
        "id": "jTa_moJRDrkw"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_next_week(feat_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw0ZvZ8C6_20",
        "outputId": "443566f9-13d0-49e3-d6c4-385118f34993"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No positive increase in volatility next week\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pickler(model):\n",
        "  with open('/content/drive/MyDrive/weekly_pos_model.pkl','wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "def pickle_jar():\n",
        "  with open('/content/drive/MyDrive/weekly_pos_model.pkl','rb') as file:\n",
        "     model= pickle.load(file)\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "ftn1prG6HPNa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8cwH-gfWJyY"
      },
      "source": [
        "####Run Cross Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IflE9RCGZ56-"
      },
      "source": [
        "def create_folds(df,targets=None,n_folds=5):\n",
        "\n",
        "  df=df.copy()\n",
        "  \n",
        "  if targets is not None:\n",
        "    df['targets']=targets\n",
        "  \n",
        "  \n",
        "  df[\"kfold\"] = -1\n",
        " \n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        " \n",
        "  y = df.targets.values\n",
        " \n",
        "  kf=StratifiedKFold(n_splits=n_folds,random_state=69,shuffle=True)\n",
        "\n",
        "  for f, (tr_, v_) in enumerate(kf.split(X=df, y=y)):\n",
        "    df.loc[v_, 'kfold'] = f\n",
        "    \n",
        "  return df\n",
        "\n",
        "def run_fold(df,model,fold):\n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True)\n",
        "\n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "  xtrain=df_train.drop(['targets','kfold'],axis=1).values\n",
        "  ytrain=df_train.targets.values\n",
        "\n",
        "  \n",
        "  xval=df_valid.drop(['targets','kfold'],axis=1).values\n",
        "  yval=df_valid.targets.values\n",
        "\n",
        "  model.fit(xtrain,ytrain)\n",
        "\n",
        "  return model,(xval,yval)\n",
        "\n",
        "def run_folds(df,model,metric=None,targets=None,n_folds=5,task='classification',pref_metric=None,param_grid=None):\n",
        "\n",
        "  print('model:',model)\n",
        "  print('task:',task.upper())\n",
        "\n",
        "  df=df.copy()\n",
        "\n",
        "  if targets is not None:\n",
        "    df['targets']=targets\n",
        "  \n",
        "  fold_df=create_folds(df,n_folds=n_folds)\n",
        "  \n",
        "  fold_dict={}\n",
        "\n",
        "  if task=='classification':\n",
        "    if pref_metric is None:\n",
        "      print('Which one is most important? recall,precision,or neither?')\n",
        "      pref_metric=input()\n",
        "\n",
        "    for fold in range(n_folds):\n",
        "      mini_dict={}\n",
        "      scores_dict={}\n",
        "      \n",
        "      print('--'*10)\n",
        "      \n",
        "      model,(xval,yval)=run_fold(fold_df,model,fold)\n",
        "      labels=model.classes_\n",
        "\n",
        "      if param_grid is not None:\n",
        "        model,best_params,(xval,yval)=run_nested_fold(fold_df,model,fold,param_grid)\n",
        "      \n",
        "      if len(df['targets'].unique())>2:\n",
        "        \n",
        "        probs= model.predict_proba(xval)\n",
        "        \n",
        "        scores_dict['multi_brier_loss']=multi_brier_loss(yval,probs,labels)\n",
        "        scores_dict['average_precision']=avg_precision(thresh_df)\n",
        "     \n",
        "      elif len(df['targets'].unique())<=2:\n",
        "\n",
        "        probs = model.predict_proba(xval)[:,-1]\n",
        "        \n",
        "        scores_dict['brier_loss']=sklearn.metrics.brier_score_loss(yval,probs)\n",
        "        scores_dict['average_precision']=sklearn.metrics.average_precision_score(yval,probs,average='macro')\n",
        "\n",
        "      thresh_df=threshold_matrix(yval,probs,labels)\n",
        "\n",
        "      overall_dict=overall_class_metrics(thresh_df)\n",
        "\n",
        "      pref_dict=pref_metrics(pref_metric,thresh_df)\n",
        "      \n",
        "      scores_dict['roc_auc']=sklearn.metrics.roc_auc_score(yval,probs,average='macro',multi_class='ovr')\n",
        "\n",
        "      scores_dict={**scores_dict,**pref_dict,**overall_dict}\n",
        "    \n",
        "      for score in scores_dict:\n",
        "        colon_print(score,scores_dict[score])\n",
        "   \n",
        "  #elif task=='regression':\n",
        "    ###\n",
        "    \n",
        "      if metric is not None:\n",
        "        print(str(metric)+':',metric(yval,probs))\n",
        "      \n",
        "      mini_dict['val']=(xval,yval,probs)\n",
        "      mini_dict['scores']=scores_dict\n",
        "      mini_dict['model']=model\n",
        "\n",
        "      if param_grid is not None:\n",
        "        mini_dict['best_params']=best_params\n",
        "        \n",
        "      fold_dict[fold]=mini_dict\n",
        "\n",
        "  return fold_dict"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffvScrfkWQtV"
      },
      "source": [
        "####Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj_aWKNxlQov"
      },
      "source": [
        "def multi_brier_loss(targets, probs,labels):\n",
        "  pad_lst=[]\n",
        "\n",
        "  for target in targets:\n",
        "    pad_lst.append((labels==target).astype('int'))\n",
        "  \n",
        "  pad_targets=np.array(pad_lst)\n",
        "\n",
        "  return np.mean(np.sum((probs - pad_targets)**2,axis=1))\n",
        "\n",
        "def thresholder(probs,threshold,labels):\n",
        "  \n",
        "  filter=(probs>=threshold).astype('int')\n",
        "\n",
        "  if len(filter.shape)>1:\n",
        "    return labels[np.argmax(filter,axis=1)]\n",
        "  \n",
        "  else:\n",
        "    return filter\n",
        "\n",
        "def best_thresholds(thresh_df):\n",
        "\n",
        "  recall_thresh=thresh_df.index[thresh_df['recall'].argmax()]\n",
        "  precision_thresh=thresh_df.index[thresh_df['precision'].argmax()]\n",
        "  f1_thresh=thresh_df.index[thresh_df['f1'].argmax()]\n",
        "  fhalf_thresh=thresh_df.index[thresh_df['f0.5'].argmax()]\n",
        "  f2_thresh=thresh_df.index[thresh_df['f2'].argmax()]\n",
        "\n",
        "  return f1_thresh,fhalf_thresh,f2_thresh,recall_thresh,precision_thresh  \n",
        "\n",
        "def best_class_scores(thresh_df):\n",
        "  best_recall=thresh_df['recall'].max()\n",
        "  best_precision=thresh_df['precision'].max()\n",
        "  best_f1=thresh_df['f1'].max()\n",
        "  best_f2=thresh_df['f2'].max()\n",
        "  best_fhalf=thresh_df['f0.5'].max()\n",
        "\n",
        "  return best_recall,best_precision,best_f1,best_f2,best_fhalf\n",
        "\n",
        "def threshold_matrix(ytest,probs,labels):\n",
        "  recall_lst=[] \n",
        "  precision_lst=[]\n",
        "  mcc_lst=[]\n",
        "  kappa_lst=[]\n",
        "\n",
        "  for threshold in np.arange(0.5,1,0.05):\n",
        "\n",
        "    preds=thresholder(probs,threshold,labels)\n",
        "\n",
        "    cm=sklearn.metrics.confusion_matrix(ytest,preds,labels=labels) \n",
        "   \n",
        "    tp = np.diag(cm)\n",
        "    fp = np.sum(cm, axis=0) - tp\n",
        "    fn = np.sum(cm, axis=1) - tp\n",
        "    tn= cm.sum()-(tp+fp+fn)\n",
        "    \n",
        "    if len(labels)==2:\n",
        "      tp=max(tp)\n",
        "      fp=max(fp)\n",
        "      fn=max(fn)\n",
        "      tn=max(tn)\n",
        "    \n",
        "    precision=np.mean(tp/(tp+fp))\n",
        "  \n",
        "    recall=np.mean(tp/(tp+fn))\n",
        "    \n",
        "    mcc=sklearn.metrics.matthews_corrcoef(ytest,preds)\n",
        "\n",
        "    try:\n",
        "      kappa=sklearn.metrics.cohen_kappa_score(ytest,preds)\n",
        "    except:\n",
        "      kappa=np.nan\n",
        "      pass\n",
        "        \n",
        "    recall_lst.append(recall)\n",
        "    precision_lst.append(precision)\n",
        "    mcc_lst.append(mcc)\n",
        "    kappa_lst.append(kappa)\n",
        "\n",
        "  thresh_df=pd.DataFrame(precision_lst,columns=['precision'],index=list(np.arange(0.5,1,0.05)))\n",
        "  thresh_df['recall']=recall_lst\n",
        "  thresh_df['mcc']=mcc_lst\n",
        "  thresh_df['kappa']=kappa_lst\n",
        "\n",
        "  thresh_df['f1']=(2*thresh_df['recall']*thresh_df['precision'])/(thresh_df['recall']+thresh_df['precision'])\n",
        "\n",
        "  thresh_df['f0.5']=((1+0.5**2)*thresh_df['recall']*thresh_df['precision'])/((0.5**2)*thresh_df['precision']+thresh_df['recall'])\n",
        "\n",
        "  thresh_df['f2']=((1+2**2)*thresh_df['recall']*thresh_df['precision'])/((2**2)*thresh_df['precision']+thresh_df['recall'])\n",
        "\n",
        "  return thresh_df\n",
        "\n",
        "def avg_precision(thresh_df):\n",
        "  \n",
        "  try:\n",
        "    avg_precision=sklearn.metrics.auc(thresh_df['recall'],thresh_df['precision'])\n",
        "      \n",
        "  except:\n",
        "  \n",
        "    avg_precision=np.nan\n",
        "    pass\n",
        "  \n",
        "  return avg_precision\n",
        "\n",
        "def overall_class_metrics(thresh_df):\n",
        "  scores_dict={}\n",
        "  scores_dict['f1_score']=thresh_df['f1'].mean()\n",
        "  \n",
        "  scores_dict['mcc']=thresh_df['mcc'].mean()\n",
        "  scores_dict['cohen_kappa']= thresh_df['kappa'].mean()\n",
        "\n",
        "  return scores_dict\n",
        "\n",
        "def pref_metrics(pref_metric,thresh_df):\n",
        "  f1_thresh,fhalf_thresh,f2_thresh,recall_thresh,precision_thresh=best_thresholds(thresh_df)\n",
        "  best_recall,best_precision,best_f1,best_f2,best_fhalf=best_class_scores(thresh_df)\n",
        "\n",
        "  scores_dict={}\n",
        "  \n",
        "  if pref_metric.lower()=='recall':\n",
        "    \n",
        "    scores_dict['recall_thresh']=recall_thresh\n",
        "    scores_dict['best_recall']=best_recall\n",
        "\n",
        "    scores_dict['f2_thresh']= f2_thresh\n",
        "    scores_dict['best_f2']= best_f2\n",
        "    \n",
        "    scores_dict['overall_f2']=thresh_df['f2'].mean()\n",
        "\n",
        "  elif pref_metric.lower()=='precision':\n",
        "    \n",
        "    scores_dict['precision_thresh']=precision_thresh\n",
        "    scores_dict['best_precision']=best_precision\n",
        "  \n",
        "    scores_dict['f0.5_threshold'] =fhalf_thresh\n",
        "    scores_dict['best_f0.5']=best_fhalf\n",
        "    \n",
        "    scores_dict['overall_f0.5']=thresh_df['f0.5'].mean()\n",
        "    \n",
        "  else:\n",
        "    scores_dict['f1_threshold']=f1_thresh\n",
        "    scores_dict['best_f1']= best_f1\n",
        "\n",
        "  return scores_dict"
      ],
      "execution_count": 37,
      "outputs": []
    }
  ]
}